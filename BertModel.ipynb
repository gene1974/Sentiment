{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332f779e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T09:02:10.400639Z",
     "start_time": "2022-04-19T09:02:10.393871Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f885cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T09:02:11.300180Z",
     "start_time": "2022-04-19T09:02:11.291692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "\n",
    "from crf import CRF\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0fa60c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T09:02:53.564502Z",
     "start_time": "2022-04-19T09:02:53.557744Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "def logger(content):\n",
    "    logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "    log_format = '[%(asctime)s] %(message)s'\n",
    "    date_format = '%Y%m%d %H:%M:%S'\n",
    "    logging.basicConfig(level = logging.DEBUG, format = log_format, datefmt = date_format)\n",
    "    logging.info(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9e21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'path': '/data/pretrained/bert-base-chinese/',\n",
    "    'bert_out_dim': 768,\n",
    "    'n_class': 9, \n",
    "    'dropout': 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d23ad724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T09:02:54.615788Z",
     "start_time": "2022-04-19T09:02:54.241527Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20240321 11:24:26] Load dataset: 8670\n",
      "[20240321 11:24:26] words: 2250, variety: 26, category: 9\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "words = set()\n",
    "varietys = set()\n",
    "categorys = set()\n",
    "with open('./Dataset/comment_labeled.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        comment_id = data['comment_id']\n",
    "        comment_variety = data['comment_variety'] # 花生\n",
    "        user_star = data['user_star'] # 2\n",
    "        comment_text = data['comment_text']\n",
    "        comment_units = data['comment_units'] # 多个四元组\n",
    "        dataset.append({\n",
    "            'id': comment_id, \n",
    "            'variety': comment_variety, \n",
    "            'user_star': user_star, \n",
    "            'text': comment_text, \n",
    "            'comment': comment_units, \n",
    "        })\n",
    "        words |= set(comment_text)\n",
    "        varietys.add(comment_variety)\n",
    "        categorys |= {i['aspect'] for i in comment_units}\n",
    "\n",
    "logger('Load dataset: {}'.format(len(dataset)))\n",
    "logger('words: {}, variety: {}, category: {}'.format(len(words), len(varietys), len(categorys)))\n",
    "\n",
    "word_list = list(words)\n",
    "word_list.insert(0, '[PAD]')\n",
    "word_list.insert(1, '[UNK]')\n",
    "word_dict = {word_list[i]: i for i in range(len(word_list))}\n",
    "\n",
    "\n",
    "variety_list = list(varietys)\n",
    "variety_list.insert(0, '<PAD>')\n",
    "variety_list.insert(1, '<OOV>')\n",
    "variety_dict = {variety_list[i]: i for i in range(len(variety_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225bec5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T09:02:57.193365Z",
     "start_time": "2022-04-19T09:02:57.183644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20240321 09:47:48] Load Glove Word embedding: torch.Size([2252, 300])\n"
     ]
    }
   ],
   "source": [
    "def load_glove(word_to_ix, dim = 100):\n",
    "    if dim == 100:\n",
    "        path = '/data/pretrained/Glove/glove.6B.100d.txt'\n",
    "    elif dim == 300:\n",
    "        path = '/data/pretrained/Glove/glove.840B.300d.txt'\n",
    "    word_emb = []\n",
    "    word_emb = torch.zeros((len(word_to_ix), dim), dtype = torch.float)\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split(' ') # [word emb1 emb2 ... emb n]\n",
    "            word = data[0]\n",
    "            if word in word_to_ix:\n",
    "                word_emb[word_to_ix[word]] = torch.tensor([float(i) for i in data[1:]])\n",
    "    return word_emb\n",
    "word_emb = load_glove(word_dict, 300)\n",
    "\n",
    "logger('Load Glove Word embedding: {}'.format(word_emb.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29ddf199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T09:20:40.588648Z",
     "start_time": "2022-04-19T09:20:40.430610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-A': 1, 'I-A': 2, 'E-A': 3, 'S-A': 4, 'B-O': 5, 'I-O': 6, 'E-O': 7, 'S-O': 8}\n",
      "{'text': '这次买的没有之前买的品质好，以前每一个颗粒饱满，这次的质量参差不齐。', 'word_ids': [1478, 1803, 1781, 565, 1915, 2050, 1674, 127, 1781, 565, 38, 1522, 587, 890, 448, 127, 1479, 2135, 140, 2195, 506, 941, 1630, 890, 1478, 1803, 565, 1522, 19, 66, 1324, 1234, 1236, 1641, 0, 0, 0, 0, 0, 0], 'word_masks': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': [5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 7, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# tag BIO label\n",
    "tag_list = ['O'] + [i + '-A' for i in ['B', 'I', 'E', 'S']] + [i + '-O' for i in ['B', 'I', 'E', 'S']]\n",
    "tag_dict = {tag_list[i]: i for i in range(len(tag_list))}\n",
    "print(tag_dict)\n",
    "\n",
    "def tagging(text, aspects, opinions):\n",
    "    tags = ['O'] * len(text)\n",
    "    for t in sorted(aspects, key = lambda x: x['tail'] - x['head']):\n",
    "        if t['tail'] - t['head'] == 1:\n",
    "            tags[t['head']] = 'S-A'\n",
    "        else:\n",
    "            tags[t['head']] = 'B-A'\n",
    "            for i in range(t['head'] + 1, t['tail'] - 1):\n",
    "                tags[i] = 'I-A'\n",
    "            tags[t['tail'] - 1] = 'E-A'\n",
    "    for o in sorted(opinions, key = lambda x: x['tail'] - x['head']):\n",
    "        if o['tail'] - o['head'] == 1:\n",
    "            tags[o['head']] = 'S-O'\n",
    "        else:\n",
    "            tags[o['head']] = 'B-O'\n",
    "            for i in range(o['head'] + 1, o['tail'] - 1):\n",
    "                tags[i] = 'I-O'\n",
    "            tags[o['tail'] - 1] = 'E-O'\n",
    "    return tags\n",
    "\n",
    "sen_len = 40\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "tagged_dataset = []\n",
    "for item in dataset:\n",
    "    aspects = []\n",
    "    opinions = []\n",
    "    for comment in item['comment']: # 每个 comment 是一个四元组，描述一个方面\n",
    "        aspects += comment['target']\n",
    "        opinions += comment['opinion']\n",
    "    text = item['text']\n",
    "    tags = tagging(text, aspects, opinions) + ['O'] * (sen_len - len(text))\n",
    "    word_ids = [word_dict[word] if word in word_dict else word_dict['[UNK]'] for word in item['text']] + \\\n",
    "               [word_dict['[PAD]']] * (sen_len - len(text))\n",
    "    word_masks = [1] * len(text) + [0] * (sen_len - len(text))\n",
    "    tag_ids = [tag_dict[i] for i in tags]\n",
    "    tagged_dataset.append({\n",
    "        'text': item['text'],\n",
    "        'word_ids': word_ids, \n",
    "        'word_masks': word_masks,\n",
    "        'labels': tag_ids\n",
    "    })\n",
    "print(tagged_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9367573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentence_entity(text, tags, tag_list):\n",
    "    tags = [tag_list[i] for i in tags]\n",
    "    entity = []\n",
    "    count = len(text)\n",
    "    i = 0\n",
    "    while i < count:\n",
    "        if tags[i][0] == 'B':\n",
    "            j = i + 1\n",
    "            while j < count:\n",
    "                if tags[j][0] == 'E':\n",
    "                    break\n",
    "                else:\n",
    "                    j += 1\n",
    "            entity.append({\n",
    "                \"text\": ''.join(text[i: j]),\n",
    "                \"start_index\": i,\n",
    "                \"end_index\": j,\n",
    "                \"label\": tags[i][2:]\n",
    "            })\n",
    "            i = j + 1\n",
    "        elif tags[i][0] == 'S':\n",
    "            entity.append({\n",
    "                \"text\": text[i],\n",
    "                \"start_index\": i,\n",
    "                \"end_index\": i,\n",
    "                \"label\": tags[i][2:]\n",
    "            })\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return entity\n",
    "\n",
    "# print(tokens[0], labels[0])\n",
    "# label_sentence_entity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906ff6d",
   "metadata": {},
   "source": [
    "### LSTM + Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c25833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T09:20:42.789276Z",
     "start_time": "2022-04-19T09:20:42.777169Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_lstm(batch):\n",
    "    tokens = torch.tensor([item['word_ids'] for item in batch], dtype = torch.long, device = device)\n",
    "    masks = torch.tensor([item['word_masks'] for item in batch], dtype = torch.bool, device = device)\n",
    "    labels = torch.tensor([item['labels'] for item in batch], dtype = torch.long, device = device)\n",
    "    return tokens, masks, labels\n",
    "\n",
    "n_train, n_dev = int(0.6 * len(tagged_dataset)), int(0.2 * len(tagged_dataset))\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(tagged_dataset[:n_train], batch_size = batch_size, collate_fn = collate_lstm)\n",
    "valid_loader = DataLoader(tagged_dataset[n_train: n_train + n_dev], batch_size = batch_size, collate_fn = collate_lstm)\n",
    "test_loader = DataLoader(tagged_dataset[n_train + n_dev:], batch_size = batch_size, collate_fn = collate_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c854c177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T09:21:53.064154Z",
     "start_time": "2022-04-19T09:21:53.049400Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMLinear(nn.Module):\n",
    "    def __init__(self, args, word_emb, tag_dict):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(word_emb)\n",
    "        self.dropout1 = nn.Dropout(p = args['dropout'])\n",
    "        self.dropout2 = nn.Dropout(p = args['dropout'])\n",
    "        self.lstm = nn.LSTM(args['emb_dim'], args['hidden_dim'] // 2,\n",
    "                            num_layers = args['num_layers'], bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(args['hidden_dim'], args['n_class'])\n",
    "        \n",
    "    def forward(self, tokens, masks):\n",
    "        embeds = self.word_embedding(tokens)\n",
    "        embeds = self.dropout1(embeds) # (batch_size, sen_len, 256)\n",
    "        sen_len = torch.sum(masks, dim = 1, dtype = torch.int64).to('cpu') # (batch_size)\n",
    "        pack_seq = pack_padded_sequence(embeds, sen_len, batch_first = True, enforce_sorted = False)\n",
    "        lstm_out, _ = self.lstm(pack_seq)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first = True) # (batch_size, seq_len, hidden_size)\n",
    "        lstm_feats = self.hidden2tag(lstm_out) # （batch_size, seq_len, tagset_size)\n",
    "        lstm_feats = self.dropout2(lstm_feats)\n",
    "\n",
    "        return lstm_feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df52e80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T00:32:18.296503Z",
     "start_time": "2022-04-20T00:32:18.288298Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_result(avg_train_losses, avg_valid_losses):\n",
    "    model_time = '{}'.format(time.strftime('%m%d%H%M', time.localtime()))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(avg_train_losses)\n",
    "    plt.plot(avg_valid_losses)\n",
    "    plt.legend(['train_loss', 'valid_loss'])\n",
    "    # plt.savefig('./train_loss/' + model_time + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9809c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T06:17:32.517335Z",
     "start_time": "2022-04-20T06:07:58.697086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20240320 21:51:16] [epoch 1] TLoss: 1.674 VLoss: 1.455 TAcc: 0.492 VAcc: 0.574\n",
      "[20240320 21:51:27] [epoch 2] TLoss: 1.496 VLoss: 1.341 TAcc: 0.515 VAcc: 0.562\n",
      "[20240320 21:51:39] [epoch 3] TLoss: 1.387 VLoss: 1.233 TAcc: 0.520 VAcc: 0.569\n",
      "[20240320 21:51:50] [epoch 4] TLoss: 1.300 VLoss: 1.138 TAcc: 0.521 VAcc: 0.572\n",
      "[20240320 21:52:01] [epoch 5] TLoss: 1.219 VLoss: 1.056 TAcc: 0.527 VAcc: 0.576\n",
      "[20240320 21:52:12] [epoch 6] TLoss: 1.150 VLoss: 0.984 TAcc: 0.532 VAcc: 0.582\n",
      "[20240320 21:52:23] [epoch 7] TLoss: 1.098 VLoss: 0.925 TAcc: 0.535 VAcc: 0.584\n",
      "[20240320 21:52:33] [epoch 8] TLoss: 1.054 VLoss: 0.880 TAcc: 0.535 VAcc: 0.574\n",
      "[20240320 21:52:44] [epoch 9] TLoss: 1.020 VLoss: 0.843 TAcc: 0.536 VAcc: 0.578\n",
      "[20240320 21:52:54] [epoch 10] TLoss: 0.991 VLoss: 0.811 TAcc: 0.539 VAcc: 0.575\n",
      "[20240320 21:53:05] [epoch 11] TLoss: 0.971 VLoss: 0.785 TAcc: 0.537 VAcc: 0.578\n",
      "[20240320 21:53:17] [epoch 12] TLoss: 0.952 VLoss: 0.762 TAcc: 0.538 VAcc: 0.581\n",
      "[20240320 21:53:29] [epoch 13] TLoss: 0.935 VLoss: 0.748 TAcc: 0.539 VAcc: 0.580\n",
      "[20240320 21:53:40] [epoch 14] TLoss: 0.923 VLoss: 0.739 TAcc: 0.539 VAcc: 0.570\n",
      "[20240320 21:53:52] [epoch 15] TLoss: 0.913 VLoss: 0.726 TAcc: 0.538 VAcc: 0.576\n",
      "[20240320 21:54:03] [epoch 16] TLoss: 0.906 VLoss: 0.714 TAcc: 0.538 VAcc: 0.578\n",
      "[20240320 21:54:14] [epoch 17] TLoss: 0.899 VLoss: 0.708 TAcc: 0.539 VAcc: 0.578\n",
      "[20240320 21:54:26] [epoch 18] TLoss: 0.893 VLoss: 0.703 TAcc: 0.539 VAcc: 0.577\n",
      "[20240320 21:54:37] [epoch 19] TLoss: 0.887 VLoss: 0.699 TAcc: 0.539 VAcc: 0.577\n",
      "[20240320 21:54:48] [epoch 20] TLoss: 0.884 VLoss: 0.696 TAcc: 0.538 VAcc: 0.575\n",
      "[20240320 21:55:00] [epoch 21] TLoss: 0.880 VLoss: 0.697 TAcc: 0.537 VAcc: 0.570\n",
      "[20240320 21:55:11] [epoch 22] TLoss: 0.877 VLoss: 0.694 TAcc: 0.539 VAcc: 0.570\n",
      "[20240320 21:55:21] [epoch 23] TLoss: 0.873 VLoss: 0.687 TAcc: 0.539 VAcc: 0.576\n",
      "[20240320 21:55:32] [epoch 24] TLoss: 0.871 VLoss: 0.686 TAcc: 0.541 VAcc: 0.574\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels[:, :sen_len]\n\u001b[1;32m     49\u001b[0m output \u001b[38;5;241m=\u001b[39m model(tokens, masks)\n\u001b[0;32m---> 50\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mentrophy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m valid_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     52\u001b[0m predict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mindices \u001b[38;5;66;03m# (n_batch, n_tokens)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py:961\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:2468\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2467\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:2288\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2286\u001b[0m reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   2287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 2288\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2291\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mnll_loss2d(\n\u001b[1;32m   2292\u001b[0m         \u001b[38;5;28minput\u001b[39m, target, weight, reduction_enum, ignore_index)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'epochs': 100, \n",
    "    'path': '/data/pretrained/bert-base-chinese/',\n",
    "    'emb_dim': 300,\n",
    "    'hidden_dim': 200,\n",
    "    'num_layers': 1,\n",
    "    'n_class': 9, \n",
    "    'dropout': 0.2\n",
    "}\n",
    "# from sklearn.metrics import accuracy_score\n",
    "model = LSTMLinear(args, word_emb, tag_dict).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 3e-4)\n",
    "early_stopping = EarlyStopping(patience = 10, verbose = False)\n",
    "entrophy = nn.CrossEntropyLoss()\n",
    "\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "for epoch in range(args['epochs']):\n",
    "    train_correct, train_total, valid_correct, valid_total = 0, 0, 0, 0\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    model.train()\n",
    "    for _, batch in enumerate(train_loader):\n",
    "        tokens, masks, labels = batch\n",
    "        sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "        tokens = tokens[:, :sen_len]\n",
    "        masks = masks[:, :sen_len]\n",
    "        labels = labels[:, :sen_len]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(tokens, masks) # (n_batch, n_token, n_class)\n",
    "        loss = entrophy(output.permute(0, 2, 1), labels)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predict = torch.max(output, dim = 2).indices # (n_batch, n_tokens)\n",
    "        train_correct += torch.sum(predict[masks == 1] == labels[masks == 1]).item()\n",
    "        train_total += torch.sum(masks == 1).item()\n",
    "    avg_train_loss = np.average(train_losses)\n",
    "    avg_train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gold_num = 0\n",
    "        predict_num = 0\n",
    "        correct_num = 0\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            tokens, masks, labels = batch\n",
    "            sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "            tokens = tokens[:, :sen_len]\n",
    "            masks = masks[:, :sen_len]\n",
    "            labels = labels[:, :sen_len]\n",
    "            output = model(tokens, masks)\n",
    "            loss = entrophy(output.permute(0, 2, 1), labels)\n",
    "            valid_losses.append(loss.item())\n",
    "            predict = torch.max(output, dim = 2).indices # (n_batch, n_tokens)\n",
    "            valid_correct += torch.sum(predict[masks == 1] == labels[masks == 1]).item()\n",
    "            valid_total += torch.sum(masks == 1).item()\n",
    "            for j in range(labels.shape[0]):\n",
    "                gold_entity = label_sentence_entity(text[j], labels[j].tolist(), tag_list)\n",
    "                pred_entity = label_sentence_entity(text[j], predict[j], tag_list)\n",
    "                gold_num += len(gold_entity)\n",
    "                predict_num += len(pred_entity)\n",
    "                for entity in gold_entity:\n",
    "                    if entity in pred_entity:\n",
    "                        correct_num += 1\n",
    "        avg_valid_loss = np.average(valid_losses)\n",
    "        avg_valid_losses.append(avg_valid_loss)\n",
    "    precision = correct_num / (predict_num + 0.000000001)\n",
    "    recall = correct_num / (gold_num + 0.000000001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.000000001)\n",
    "    logger('[Test] Precision: {:.8f} Recall: {:.8f} F1: {:.8f}'.format(precision, recall, f1))\n",
    "    \n",
    "    logger('[epoch {:d}] TLoss: {:.3f} VLoss: {:.3f} TAcc: {:.3f} VAcc: {:.3f}'.format(\n",
    "            epoch + 1, avg_train_loss, avg_valid_loss, train_correct / train_total, valid_correct / valid_total))\n",
    "    early_stopping(avg_valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        logger(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "\n",
    "# plot_result(avg_train_losses, avg_valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de90e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T06:06:13.653608Z",
     "start_time": "2022-04-20T06:06:13.640689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 37]) torch.Size([8, 37]) torch.Size([8, 37]) torch.Size([8, 37, 9]) torch.Size([8, 37])\n"
     ]
    }
   ],
   "source": [
    "output = model(tokens, masks)\n",
    "predict = torch.max(output, dim = 2).indices\n",
    "print(tokens.shape, masks.shape, labels.shape, output.shape, predict.shape)\n",
    "print(torch.sum(predict[masks == 1] == labels[masks == 1]).item())\n",
    "print(torch.sum(masks == 1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b20bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T07:17:06.748230Z",
     "start_time": "2022-04-20T07:17:06.220030Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20240320 21:57:19] [Test] Tagging accuracy: 0.56391567\n",
      "[20240320 21:57:19] [Test] Precision: 0.30051813 Recall: 0.32293987 F1: 0.31132582\n",
      "[20240320 21:57:19] Tagging accuracy: 0.5639156673720511\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "gold_num = 0\n",
    "predict_num = 0\n",
    "correct_num = 0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        tokens, masks, labels = batch\n",
    "        sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "        tokens = tokens[:, :sen_len]\n",
    "        masks = masks[:, :sen_len]\n",
    "        labels = labels[:, :sen_len]\n",
    "        output = model(tokens, masks)\n",
    "        predict = torch.max(output, dim = 2).indices # (n_batch, n_tokens)\n",
    "        correct += torch.sum(predict[masks == 1] == labels[masks == 1]).item()\n",
    "        total += torch.sum(masks).item()\n",
    "        for j in range(labels.shape[0]):\n",
    "            gold_entity = label_sentence_entity(text[j], labels[j].tolist(), tag_list)\n",
    "            pred_entity = label_sentence_entity(text[j], predict[j], tag_list)\n",
    "            gold_num += len(gold_entity)\n",
    "            predict_num += len(pred_entity)\n",
    "            for entity in gold_entity:\n",
    "                if entity in pred_entity:\n",
    "                    correct_num += 1\n",
    "            # print(gold_entity)\n",
    "            # print(pred_entity)\n",
    "            # return\n",
    "    precision = correct_num / (predict_num + 0.000000001)\n",
    "    recall = correct_num / (gold_num + 0.000000001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.000000001)\n",
    "    logger('[Test] Tagging accuracy: {:.8f}'.format(correct / total))\n",
    "    logger('[Test] Precision: {:.8f} Recall: {:.8f} F1: {:.8f}'.format(precision, recall, f1))\n",
    "logger('Tagging accuracy: {}'.format(correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c35d3e",
   "metadata": {},
   "source": [
    "### LSTM + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf0a00f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T07:21:55.690615Z",
     "start_time": "2022-04-20T07:21:55.673454Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMCRF(nn.Module):\n",
    "    def __init__(self, args, word_emb, tag_dict):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(word_emb)\n",
    "        self.dropout1 = nn.Dropout(p = args['dropout'])\n",
    "        self.dropout2 = nn.Dropout(p = args['dropout'])\n",
    "        self.lstm = nn.LSTM(args['emb_dim'], args['hidden_dim'] // 2,\n",
    "                            num_layers = args['num_layers'], bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(args['hidden_dim'], args['n_class'])\n",
    "        self.crf = CRF({tag_dict[tag]: tag for tag in tag_dict})\n",
    "        \n",
    "    def _get_lstm_out(self, tokens, masks):\n",
    "        embeds = self.word_embedding(tokens)\n",
    "        embeds = self.dropout1(embeds) # (batch_size, sen_len, 256)\n",
    "        sen_len = torch.sum(masks, dim = 1, dtype = torch.int64).to('cpu') # (batch_size)\n",
    "        pack_seq = pack_padded_sequence(embeds, sen_len, batch_first = True, enforce_sorted = False)\n",
    "        lstm_out, _ = self.lstm(pack_seq)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first = True) # (batch_size, seq_len, hidden_size)\n",
    "        lstm_feats = self.hidden2tag(lstm_out) # （batch_size, seq_len, tagset_size)\n",
    "        lstm_feats = self.dropout2(lstm_feats)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def forward(self, tokens, masks, labels):\n",
    "        lstm_feats = self._get_lstm_out(tokens, masks)\n",
    "        log_likelihood = self.crf(lstm_feats, labels, masks)\n",
    "        n_batch = labels.shape[0]\n",
    "        loss = -log_likelihood / n_batch\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, tokens, masks):\n",
    "        lstm_feats = self._get_lstm_out(tokens, masks)\n",
    "        pred = self.crf.viterbi_tags(lstm_feats, masks)\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a753809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:12:39.694012Z",
     "start_time": "2022-04-20T07:23:43.539739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20240321 10:25:55] [Test] Precision: 0.41616766 Recall: 0.15092291 F1: 0.22151394\n",
      "[20240321 10:25:55] [epoch 1] TLoss: 33.741 VLoss: 26.994 TAcc: 0.425 VAcc: 0.553\n",
      "[20240321 10:26:48] [Test] Precision: 0.44983819 Recall: 0.15092291 F1: 0.22601626\n",
      "[20240321 10:26:48] [epoch 2] TLoss: 28.735 VLoss: 25.181 TAcc: 0.493 VAcc: 0.547\n",
      "[20240321 10:27:41] [Test] Precision: 0.50177936 Recall: 0.15309446 F1: 0.23460898\n",
      "[20240321 10:27:41] [epoch 3] TLoss: 27.059 VLoss: 23.799 TAcc: 0.501 VAcc: 0.541\n",
      "[20240321 10:28:34] [Test] Precision: 0.50719424 Recall: 0.15309446 F1: 0.23519600\n",
      "[20240321 10:28:34] [epoch 4] TLoss: 25.694 VLoss: 22.575 TAcc: 0.503 VAcc: 0.536\n",
      "[20240321 10:29:27] [Test] Precision: 0.43734015 Recall: 0.18566775 F1: 0.26067073\n",
      "[20240321 10:29:27] [epoch 5] TLoss: 24.368 VLoss: 21.368 TAcc: 0.508 VAcc: 0.540\n",
      "[20240321 10:30:18] [Test] Precision: 0.33585477 Recall: 0.24104235 F1: 0.28065740\n",
      "[20240321 10:30:18] [epoch 6] TLoss: 23.239 VLoss: 20.288 TAcc: 0.514 VAcc: 0.540\n",
      "[20240321 10:31:10] [Test] Precision: 0.36723164 Recall: 0.35287731 F1: 0.35991141\n",
      "[20240321 10:31:10] [epoch 7] TLoss: 22.187 VLoss: 19.502 TAcc: 0.520 VAcc: 0.543\n",
      "[20240321 10:32:02] [Test] Precision: 0.36836403 Recall: 0.36916395 F1: 0.36876356\n",
      "[20240321 10:32:02] [epoch 8] TLoss: 21.260 VLoss: 18.626 TAcc: 0.522 VAcc: 0.551\n",
      "[20240321 10:32:54] [Test] Precision: 0.36038647 Recall: 0.40499457 F1: 0.38139059\n",
      "[20240321 10:32:54] [epoch 9] TLoss: 20.389 VLoss: 17.867 TAcc: 0.529 VAcc: 0.559\n",
      "[20240321 10:33:46] [Test] Precision: 0.35721018 Recall: 0.41150923 F1: 0.38244198\n",
      "[20240321 10:33:46] [epoch 10] TLoss: 19.587 VLoss: 17.185 TAcc: 0.533 VAcc: 0.565\n",
      "[20240321 10:34:38] [Test] Precision: 0.35430157 Recall: 0.41585233 F1: 0.38261738\n",
      "[20240321 10:34:38] [epoch 11] TLoss: 18.796 VLoss: 16.476 TAcc: 0.536 VAcc: 0.572\n",
      "[20240321 10:35:31] [Test] Precision: 0.34743474 Recall: 0.41910966 F1: 0.37992126\n",
      "[20240321 10:35:31] [epoch 12] TLoss: 18.099 VLoss: 15.867 TAcc: 0.538 VAcc: 0.575\n",
      "[20240321 10:36:24] [Test] Precision: 0.34353147 Recall: 0.42671010 F1: 0.38062954\n",
      "[20240321 10:36:24] [epoch 13] TLoss: 17.474 VLoss: 15.364 TAcc: 0.538 VAcc: 0.574\n",
      "[20240321 10:37:18] [Test] Precision: 0.34027778 Recall: 0.42562432 F1: 0.37819585\n",
      "[20240321 10:37:18] [epoch 14] TLoss: 16.840 VLoss: 14.782 TAcc: 0.545 VAcc: 0.581\n",
      "[20240321 10:38:07] [Test] Precision: 0.34011874 Recall: 0.43539631 F1: 0.38190476\n",
      "[20240321 10:38:07] [epoch 15] TLoss: 16.248 VLoss: 14.378 TAcc: 0.546 VAcc: 0.580\n",
      "[20240321 10:38:51] [Test] Precision: 0.33333333 Recall: 0.44842562 F1: 0.38240741\n",
      "[20240321 10:38:51] [epoch 16] TLoss: 15.748 VLoss: 13.960 TAcc: 0.548 VAcc: 0.578\n",
      "[20240321 10:39:44] [Test] Precision: 0.32902183 Recall: 0.44191097 F1: 0.37720111\n",
      "[20240321 10:39:44] [epoch 17] TLoss: 15.253 VLoss: 13.559 TAcc: 0.549 VAcc: 0.580\n",
      "[20240321 10:40:37] [Test] Precision: 0.35194175 Recall: 0.47231270 F1: 0.40333797\n",
      "[20240321 10:40:37] [epoch 18] TLoss: 14.848 VLoss: 13.226 TAcc: 0.548 VAcc: 0.580\n",
      "[20240321 10:41:29] [Test] Precision: 0.35499594 Recall: 0.47448426 F1: 0.40613383\n",
      "[20240321 10:41:29] [epoch 19] TLoss: 14.410 VLoss: 12.858 TAcc: 0.551 VAcc: 0.586\n",
      "[20240321 10:42:25] [Test] Precision: 0.35576923 Recall: 0.48208469 F1: 0.40940526\n",
      "[20240321 10:42:25] [epoch 20] TLoss: 14.014 VLoss: 12.653 TAcc: 0.547 VAcc: 0.577\n",
      "[20240321 10:43:22] [Test] Precision: 0.35652174 Recall: 0.48968512 F1: 0.41262580\n",
      "[20240321 10:43:22] [epoch 21] TLoss: 13.659 VLoss: 12.328 TAcc: 0.548 VAcc: 0.580\n",
      "[20240321 10:44:15] [Test] Precision: 0.37732794 Recall: 0.50597177 F1: 0.43228200\n",
      "[20240321 10:44:15] [epoch 22] TLoss: 13.347 VLoss: 12.014 TAcc: 0.550 VAcc: 0.588\n",
      "[20240321 10:45:08] [Test] Precision: 0.36721828 Recall: 0.50597177 F1: 0.42557078\n",
      "[20240321 10:45:08] [epoch 23] TLoss: 13.024 VLoss: 11.811 TAcc: 0.552 VAcc: 0.586\n",
      "[20240321 10:46:00] [Test] Precision: 0.37628458 Recall: 0.51682953 F1: 0.43549863\n",
      "[20240321 10:46:00] [epoch 24] TLoss: 12.717 VLoss: 11.615 TAcc: 0.552 VAcc: 0.586\n",
      "[20240321 10:46:53] [Test] Precision: 0.35742653 Recall: 0.48859935 F1: 0.41284404\n",
      "[20240321 10:46:53] [epoch 25] TLoss: 12.452 VLoss: 11.418 TAcc: 0.550 VAcc: 0.585\n",
      "[20240321 10:47:46] [Test] Precision: 0.38302934 Recall: 0.52442997 F1: 0.44271311\n",
      "[20240321 10:47:46] [epoch 26] TLoss: 12.246 VLoss: 11.246 TAcc: 0.550 VAcc: 0.588\n",
      "[20240321 10:48:40] [Test] Precision: 0.37620579 Recall: 0.50814332 F1: 0.43233256\n",
      "[20240321 10:48:40] [epoch 27] TLoss: 11.995 VLoss: 11.043 TAcc: 0.550 VAcc: 0.592\n",
      "[20240321 10:49:34] [Test] Precision: 0.40437601 Recall: 0.54180239 F1: 0.46310905\n",
      "[20240321 10:49:34] [epoch 28] TLoss: 11.789 VLoss: 10.875 TAcc: 0.549 VAcc: 0.594\n",
      "[20240321 10:50:25] [Test] Precision: 0.39467312 Recall: 0.53094463 F1: 0.45277778\n",
      "[20240321 10:50:25] [epoch 29] TLoss: 11.587 VLoss: 10.739 TAcc: 0.552 VAcc: 0.594\n",
      "[20240321 10:51:17] [Test] Precision: 0.38075990 Recall: 0.51140065 F1: 0.43651529\n",
      "[20240321 10:51:17] [epoch 30] TLoss: 11.402 VLoss: 10.619 TAcc: 0.552 VAcc: 0.593\n",
      "[20240321 10:52:08] [Test] Precision: 0.39578264 Recall: 0.52985885 F1: 0.45311049\n",
      "[20240321 10:52:08] [epoch 31] TLoss: 11.247 VLoss: 10.486 TAcc: 0.555 VAcc: 0.597\n",
      "[20240321 10:53:01] [Test] Precision: 0.39659367 Recall: 0.53094463 F1: 0.45403900\n",
      "[20240321 10:53:01] [epoch 32] TLoss: 11.086 VLoss: 10.354 TAcc: 0.553 VAcc: 0.599\n",
      "[20240321 10:53:52] [Test] Precision: 0.39352227 Recall: 0.52768730 F1: 0.45083488\n",
      "[20240321 10:53:52] [epoch 33] TLoss: 10.942 VLoss: 10.235 TAcc: 0.553 VAcc: 0.600\n",
      "[20240321 10:54:44] [Test] Precision: 0.38862559 Recall: 0.53420195 F1: 0.44993141\n",
      "[20240321 10:54:44] [epoch 34] TLoss: 10.810 VLoss: 10.166 TAcc: 0.554 VAcc: 0.593\n",
      "[20240321 10:55:36] [Test] Precision: 0.37991968 Recall: 0.51357220 F1: 0.43674977\n",
      "[20240321 10:55:36] [epoch 35] TLoss: 10.642 VLoss: 10.054 TAcc: 0.551 VAcc: 0.595\n",
      "[20240321 10:56:28] [Test] Precision: 0.37953263 Recall: 0.51140065 F1: 0.43570768\n",
      "[20240321 10:56:28] [epoch 36] TLoss: 10.549 VLoss: 9.959 TAcc: 0.553 VAcc: 0.595\n",
      "[20240321 10:57:18] [Test] Precision: 0.37763371 Recall: 0.50597177 F1: 0.43248260\n",
      "[20240321 10:57:18] [epoch 37] TLoss: 10.442 VLoss: 9.888 TAcc: 0.556 VAcc: 0.598\n",
      "[20240321 10:58:09] [Test] Precision: 0.35618597 Recall: 0.49077090 F1: 0.41278539\n",
      "[20240321 10:58:09] [epoch 38] TLoss: 10.321 VLoss: 9.828 TAcc: 0.553 VAcc: 0.591\n",
      "[20240321 10:59:01] [Test] Precision: 0.35048232 Recall: 0.47339848 F1: 0.40277136\n",
      "[20240321 10:59:01] [epoch 39] TLoss: 10.234 VLoss: 9.727 TAcc: 0.552 VAcc: 0.598\n",
      "[20240321 10:59:52] [Test] Precision: 0.37025316 Recall: 0.50814332 F1: 0.42837529\n",
      "[20240321 10:59:52] [epoch 40] TLoss: 10.142 VLoss: 9.697 TAcc: 0.555 VAcc: 0.591\n",
      "[20240321 11:00:44] [Test] Precision: 0.36334661 Recall: 0.49511401 F1: 0.41911765\n",
      "[20240321 11:00:44] [epoch 41] TLoss: 10.038 VLoss: 9.599 TAcc: 0.556 VAcc: 0.597\n",
      "[20240321 11:01:36] [Test] Precision: 0.36027714 Recall: 0.50814332 F1: 0.42162162\n",
      "[20240321 11:01:36] [epoch 42] TLoss: 9.959 VLoss: 9.596 TAcc: 0.557 VAcc: 0.588\n",
      "[20240321 11:02:27] [Test] Precision: 0.35090480 Recall: 0.48425624 F1: 0.40693431\n",
      "[20240321 11:02:27] [epoch 43] TLoss: 9.895 VLoss: 9.517 TAcc: 0.559 VAcc: 0.591\n",
      "[20240321 11:03:19] [Test] Precision: 0.34330709 Recall: 0.47339848 F1: 0.39799178\n",
      "[20240321 11:03:19] [epoch 44] TLoss: 9.810 VLoss: 9.461 TAcc: 0.550 VAcc: 0.590\n",
      "[20240321 11:04:11] [Test] Precision: 0.32734375 Recall: 0.45494028 F1: 0.38073603\n",
      "[20240321 11:04:11] [epoch 45] TLoss: 9.737 VLoss: 9.433 TAcc: 0.557 VAcc: 0.586\n",
      "[20240321 11:05:04] [Test] Precision: 0.33981337 Recall: 0.47448426 F1: 0.39601269\n",
      "[20240321 11:05:04] [epoch 46] TLoss: 9.678 VLoss: 9.390 TAcc: 0.555 VAcc: 0.584\n",
      "[20240321 11:05:57] [Test] Precision: 0.32327920 Recall: 0.45385451 F1: 0.37759711\n",
      "[20240321 11:05:57] [epoch 47] TLoss: 9.602 VLoss: 9.346 TAcc: 0.558 VAcc: 0.586\n",
      "[20240321 11:06:51] [Test] Precision: 0.33075136 Recall: 0.46362649 F1: 0.38607595\n",
      "[20240321 11:06:51] [epoch 48] TLoss: 9.568 VLoss: 9.297 TAcc: 0.559 VAcc: 0.587\n",
      "[20240321 11:07:45] [Test] Precision: 0.30007446 Recall: 0.43756786 F1: 0.35600707\n",
      "[20240321 11:07:45] [epoch 49] TLoss: 9.475 VLoss: 9.298 TAcc: 0.557 VAcc: 0.576\n",
      "[20240321 11:08:38] [Test] Precision: 0.32804233 Recall: 0.47122693 F1: 0.38680927\n",
      "[20240321 11:08:38] [epoch 50] TLoss: 9.460 VLoss: 9.222 TAcc: 0.553 VAcc: 0.583\n",
      "[20240321 11:09:29] [Test] Precision: 0.29618321 Recall: 0.42128122 F1: 0.34782609\n",
      "[20240321 11:09:29] [epoch 51] TLoss: 9.404 VLoss: 9.212 TAcc: 0.554 VAcc: 0.582\n",
      "[20240321 11:10:20] [Test] Precision: 0.29843750 Recall: 0.41476656 F1: 0.34711495\n",
      "[20240321 11:10:20] [epoch 52] TLoss: 9.358 VLoss: 9.166 TAcc: 0.558 VAcc: 0.585\n",
      "[20240321 11:11:11] [Test] Precision: 0.29204204 Recall: 0.42236699 F1: 0.34531735\n",
      "[20240321 11:11:11] [epoch 53] TLoss: 9.323 VLoss: 9.170 TAcc: 0.556 VAcc: 0.573\n",
      "[20240321 11:12:02] [Test] Precision: 0.29495862 Recall: 0.42562432 F1: 0.34844444\n",
      "[20240321 11:12:02] [epoch 54] TLoss: 9.279 VLoss: 9.127 TAcc: 0.555 VAcc: 0.578\n",
      "[20240321 11:12:55] [Test] Precision: 0.27653214 Recall: 0.40173724 F1: 0.32757857\n",
      "[20240321 11:12:55] [epoch 55] TLoss: 9.212 VLoss: 9.104 TAcc: 0.562 VAcc: 0.574\n",
      "[20240321 11:13:48] [Test] Precision: 0.26692015 Recall: 0.38110749 F1: 0.31395349\n",
      "[20240321 11:13:48] [epoch 56] TLoss: 9.181 VLoss: 9.082 TAcc: 0.557 VAcc: 0.574\n",
      "[20240321 11:14:44] [Test] Precision: 0.26834862 Recall: 0.38110749 F1: 0.31493943\n",
      "[20240321 11:14:44] [epoch 57] TLoss: 9.156 VLoss: 9.069 TAcc: 0.557 VAcc: 0.575\n",
      "[20240321 11:15:40] [Test] Precision: 0.28463285 Recall: 0.40825190 F1: 0.33541481\n",
      "[20240321 11:15:40] [epoch 58] TLoss: 9.120 VLoss: 9.041 TAcc: 0.559 VAcc: 0.575\n",
      "[20240321 11:16:33] [Test] Precision: 0.26106527 Recall: 0.37785016 F1: 0.30878438\n",
      "[20240321 11:16:33] [epoch 59] TLoss: 9.103 VLoss: 9.015 TAcc: 0.556 VAcc: 0.574\n",
      "[20240321 11:17:25] [Test] Precision: 0.27947598 Recall: 0.41693811 F1: 0.33464052\n",
      "[20240321 11:17:25] [epoch 60] TLoss: 9.054 VLoss: 9.021 TAcc: 0.558 VAcc: 0.561\n",
      "[20240321 11:18:18] [Test] Precision: 0.25167535 Recall: 0.36699240 F1: 0.29858657\n",
      "[20240321 11:18:18] [epoch 61] TLoss: 9.025 VLoss: 9.006 TAcc: 0.559 VAcc: 0.566\n",
      "[20240321 11:19:13] [Test] Precision: 0.24593640 Recall: 0.37785016 F1: 0.29794521\n",
      "[20240321 11:19:13] [epoch 62] TLoss: 9.000 VLoss: 9.020 TAcc: 0.559 VAcc: 0.549\n",
      "[20240321 11:20:05] [Test] Precision: 0.23361823 Recall: 0.35613464 F1: 0.28215054\n",
      "[20240321 11:20:05] [epoch 63] TLoss: 8.983 VLoss: 9.007 TAcc: 0.556 VAcc: 0.548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 35\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (n_batch, n_tokens)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m train_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(predict[masks \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m labels[masks \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m train_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(masks \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mLSTMCRF.predict\u001b[0;34m(self, tokens, masks)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, masks):\n\u001b[1;32m     31\u001b[0m     lstm_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lstm_out(tokens, masks)\n\u001b[0;32m---> 32\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviterbi_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m~/Documents/Sentiment/crf.py:214\u001b[0m, in \u001b[0;36mCRF.viterbi_tags\u001b[0;34m(self, logits, mask)\u001b[0m\n\u001b[1;32m    212\u001b[0m tag_sequence[sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, end_tag] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# We pass the tags and the transitions to ``viterbi_decode``.\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m viterbi_path, viterbi_score \u001b[38;5;241m=\u001b[39m \u001b[43mviterbi_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Get rid of START and END sentinels and append.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m viterbi_path \u001b[38;5;241m=\u001b[39m viterbi_path[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Sentiment/crf.py:34\u001b[0m, in \u001b[0;36mviterbi_decode\u001b[0;34m(tag_sequence, transition_matrix, tag_observations)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the scores for all possible paths.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, sequence_length):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Add pairwise potentials to current scores.\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     summed_potentials \u001b[38;5;241m=\u001b[39m \u001b[43mpath_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m transition_matrix\n\u001b[1;32m     35\u001b[0m     scores, paths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(summed_potentials, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# If we have an observation for this timestep, use it\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# instead of the distribution over tags.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'epochs': 100, \n",
    "    'path': '/data/pretrained/bert-base-chinese/',\n",
    "    'emb_dim': 300,\n",
    "    'hidden_dim': 200,\n",
    "    'num_layers': 1,\n",
    "    'n_class': 9, \n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "model = LSTMCRF(args, word_emb, tag_dict).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "early_stopping = EarlyStopping(patience = 10, verbose = False)\n",
    "entrophy = nn.CrossEntropyLoss()\n",
    "\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "for epoch in range(args['epochs']):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for _, batch in enumerate(train_loader):\n",
    "        tokens, masks, labels = batch\n",
    "        sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "        tokens = tokens[:, :sen_len]\n",
    "        masks = masks[:, :sen_len]\n",
    "        labels = labels[:, :sen_len]\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(tokens, masks, labels) # (n_batch, n_token, n_class)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predict = model.predict(tokens, masks) # (n_batch, n_tokens)\n",
    "        train_correct += torch.sum(predict[masks == 1] == labels[masks == 1]).item()\n",
    "        train_total += torch.sum(masks == 1).item()\n",
    "    avg_train_loss = np.average(train_losses)\n",
    "    avg_train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gold_num = 0\n",
    "        predict_num = 0\n",
    "        correct_num = 0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            tokens, masks, labels = batch\n",
    "            sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "            tokens = tokens[:, :sen_len]\n",
    "            masks = masks[:, :sen_len]\n",
    "            labels = labels[:, :sen_len]\n",
    "            loss = model(tokens, masks, labels)\n",
    "            valid_losses.append(loss.item())\n",
    "            predict = model.predict(tokens, masks)\n",
    "            valid_correct += torch.sum(predict[masks == 1] == labels[masks == 1]).item()\n",
    "            valid_total += torch.sum(masks == 1).item()\n",
    "            for j in range(labels.shape[0]):\n",
    "                gold_entity = label_sentence_entity(text[j], labels[j].tolist(), tag_list)\n",
    "                pred_entity = label_sentence_entity(text[j], predict[j], tag_list)\n",
    "                gold_num += len(gold_entity)\n",
    "                predict_num += len(pred_entity)\n",
    "                for entity in gold_entity:\n",
    "                    if entity in pred_entity:\n",
    "                        correct_num += 1\n",
    "        avg_valid_loss = np.average(valid_losses)\n",
    "        avg_valid_losses.append(avg_valid_loss)\n",
    "    precision = correct_num / (predict_num + 0.000000001)\n",
    "    recall = correct_num / (gold_num + 0.000000001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.000000001)\n",
    "    logger('[Test] Precision: {:.8f} Recall: {:.8f} F1: {:.8f}'.format(precision, recall, f1))\n",
    "    \n",
    "    logger('[epoch {:d}] TLoss: {:.3f} VLoss: {:.3f} TAcc: {:.3f} VAcc: {:.3f}'.format(\n",
    "            epoch + 1, avg_train_loss, avg_valid_loss, train_correct / train_total, valid_correct / valid_total))\n",
    "    early_stopping(avg_valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        logger(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "plot_result(avg_train_losses, avg_valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f58f4e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20240321 12:27:42] [Test] Precision: 0.01298701 Recall: 0.00108578 F1: 0.00200401\n",
      "[20240321 12:27:42] [epoch 1] TLoss: 34.938 VLoss: 26.756 TAcc: 0.440 VAcc: 0.614\n",
      "[20240321 12:28:35] [Test] Precision: 0.09523810 Recall: 0.00217155 F1: 0.00424628\n",
      "[20240321 12:28:35] [epoch 2] TLoss: 29.057 VLoss: 24.470 TAcc: 0.496 VAcc: 0.599\n",
      "[20240321 12:29:27] [Test] Precision: 0.50000000 Recall: 0.00217155 F1: 0.00432432\n",
      "[20240321 12:29:27] [epoch 3] TLoss: 27.294 VLoss: 23.000 TAcc: 0.510 VAcc: 0.591\n",
      "[20240321 12:30:19] [Test] Precision: 0.44642857 Recall: 0.10857763 F1: 0.17467249\n",
      "[20240321 12:30:19] [epoch 4] TLoss: 25.718 VLoss: 21.759 TAcc: 0.511 VAcc: 0.584\n",
      "[20240321 12:31:10] [Test] Precision: 0.46067416 Recall: 0.13355049 F1: 0.20707071\n",
      "[20240321 12:31:10] [epoch 5] TLoss: 24.377 VLoss: 20.712 TAcc: 0.515 VAcc: 0.587\n",
      "[20240321 12:32:01] [Test] Precision: 0.45864662 Recall: 0.13246471 F1: 0.20556024\n",
      "[20240321 12:32:01] [epoch 6] TLoss: 23.217 VLoss: 19.852 TAcc: 0.522 VAcc: 0.582\n",
      "[20240321 12:32:54] [Test] Precision: 0.42154567 Recall: 0.19543974 F1: 0.26706231\n",
      "[20240321 12:32:54] [epoch 7] TLoss: 22.207 VLoss: 18.944 TAcc: 0.523 VAcc: 0.584\n",
      "[20240321 12:33:47] [Test] Precision: 0.45574388 Recall: 0.26275787 F1: 0.33333333\n",
      "[20240321 12:33:47] [epoch 8] TLoss: 21.249 VLoss: 18.270 TAcc: 0.526 VAcc: 0.582\n",
      "[20240321 12:34:40] [Test] Precision: 0.46708464 Recall: 0.32356135 F1: 0.38229634\n",
      "[20240321 12:34:40] [epoch 9] TLoss: 20.305 VLoss: 17.524 TAcc: 0.525 VAcc: 0.594\n",
      "[20240321 12:35:32] [Test] Precision: 0.47808765 Recall: 0.39087948 F1: 0.43010753\n",
      "[20240321 12:35:32] [epoch 10] TLoss: 19.468 VLoss: 16.822 TAcc: 0.533 VAcc: 0.600\n",
      "[20240321 12:36:24] [Test] Precision: 0.45454545 Recall: 0.42888165 F1: 0.44134078\n",
      "[20240321 12:36:24] [epoch 11] TLoss: 18.677 VLoss: 16.162 TAcc: 0.538 VAcc: 0.602\n",
      "[20240321 12:37:16] [Test] Precision: 0.42176166 Recall: 0.44191097 F1: 0.43160127\n",
      "[20240321 12:37:16] [epoch 12] TLoss: 17.963 VLoss: 15.648 TAcc: 0.537 VAcc: 0.599\n",
      "[20240321 12:38:08] [Test] Precision: 0.44332999 Recall: 0.47991314 F1: 0.46089677\n",
      "[20240321 12:38:08] [epoch 13] TLoss: 17.261 VLoss: 15.075 TAcc: 0.542 VAcc: 0.605\n",
      "[20240321 12:39:00] [Test] Precision: 0.44856348 Recall: 0.52551574 F1: 0.48400000\n",
      "[20240321 12:39:00] [epoch 14] TLoss: 16.616 VLoss: 14.573 TAcc: 0.545 VAcc: 0.608\n",
      "[20240321 12:39:53] [Test] Precision: 0.45743686 Recall: 0.53094463 F1: 0.49145729\n",
      "[20240321 12:39:53] [epoch 15] TLoss: 16.028 VLoss: 14.056 TAcc: 0.545 VAcc: 0.611\n",
      "[20240321 12:40:45] [Test] Precision: 0.45261293 Recall: 0.55483170 F1: 0.49853658\n",
      "[20240321 12:40:45] [epoch 16] TLoss: 15.489 VLoss: 13.672 TAcc: 0.546 VAcc: 0.609\n",
      "[20240321 12:41:36] [Test] Precision: 0.45914742 Recall: 0.56134636 F1: 0.50512946\n",
      "[20240321 12:41:36] [epoch 17] TLoss: 14.988 VLoss: 13.282 TAcc: 0.546 VAcc: 0.610\n",
      "[20240321 12:42:28] [Test] Precision: 0.45526316 Recall: 0.56351792 F1: 0.50363901\n",
      "[20240321 12:42:28] [epoch 18] TLoss: 14.541 VLoss: 12.990 TAcc: 0.548 VAcc: 0.607\n",
      "[20240321 12:43:20] [Test] Precision: 0.44955752 Recall: 0.55157438 F1: 0.49536811\n",
      "[20240321 12:43:20] [epoch 19] TLoss: 14.146 VLoss: 12.694 TAcc: 0.552 VAcc: 0.606\n",
      "[20240321 12:44:14] [Test] Precision: 0.44082333 Recall: 0.55808903 F1: 0.49257307\n",
      "[20240321 12:44:14] [epoch 20] TLoss: 13.795 VLoss: 12.488 TAcc: 0.547 VAcc: 0.596\n",
      "[20240321 12:45:08] [Test] Precision: 0.44337979 Recall: 0.55266015 F1: 0.49202513\n",
      "[20240321 12:45:08] [epoch 21] TLoss: 13.423 VLoss: 12.134 TAcc: 0.550 VAcc: 0.606\n",
      "[20240321 12:46:01] [Test] Precision: 0.44573991 Recall: 0.53963084 F1: 0.48821218\n",
      "[20240321 12:46:01] [epoch 22] TLoss: 13.077 VLoss: 11.886 TAcc: 0.551 VAcc: 0.608\n",
      "[20240321 12:46:52] [Test] Precision: 0.44843049 Recall: 0.54288817 F1: 0.49115914\n",
      "[20240321 12:46:52] [epoch 23] TLoss: 12.833 VLoss: 11.623 TAcc: 0.550 VAcc: 0.612\n",
      "[20240321 12:47:45] [Test] Precision: 0.44580420 Recall: 0.55374593 F1: 0.49394673\n",
      "[20240321 12:47:45] [epoch 24] TLoss: 12.534 VLoss: 11.485 TAcc: 0.552 VAcc: 0.607\n",
      "[20240321 12:48:36] [Test] Precision: 0.43722564 Recall: 0.54071661 F1: 0.48349515\n",
      "[20240321 12:48:36] [epoch 25] TLoss: 12.311 VLoss: 11.296 TAcc: 0.551 VAcc: 0.606\n",
      "[20240321 12:49:27] [Test] Precision: 0.43876652 Recall: 0.54071661 F1: 0.48443580\n",
      "[20240321 12:49:27] [epoch 26] TLoss: 12.079 VLoss: 11.124 TAcc: 0.550 VAcc: 0.605\n",
      "[20240321 12:50:20] [Test] Precision: 0.43451327 Recall: 0.53311618 F1: 0.47879083\n",
      "[20240321 12:50:20] [epoch 27] TLoss: 11.843 VLoss: 10.951 TAcc: 0.552 VAcc: 0.608\n",
      "[20240321 12:51:16] [Test] Precision: 0.42478632 Recall: 0.53963084 F1: 0.47537064\n",
      "[20240321 12:51:16] [epoch 28] TLoss: 11.668 VLoss: 10.853 TAcc: 0.552 VAcc: 0.599\n",
      "[20240321 12:52:10] [Test] Precision: 0.43532560 Recall: 0.52985885 F1: 0.47796278\n",
      "[20240321 12:52:10] [epoch 29] TLoss: 11.460 VLoss: 10.635 TAcc: 0.555 VAcc: 0.609\n",
      "[20240321 12:53:04] [Test] Precision: 0.40932642 Recall: 0.51465798 F1: 0.45598846\n",
      "[20240321 12:53:04] [epoch 30] TLoss: 11.319 VLoss: 10.589 TAcc: 0.551 VAcc: 0.597\n",
      "[20240321 12:53:57] [Test] Precision: 0.40881590 Recall: 0.51357220 F1: 0.45524543\n",
      "[20240321 12:53:57] [epoch 31] TLoss: 11.130 VLoss: 10.426 TAcc: 0.553 VAcc: 0.601\n",
      "[20240321 12:54:48] [Test] Precision: 0.41421144 Recall: 0.51900109 F1: 0.46072289\n",
      "[20240321 12:54:48] [epoch 32] TLoss: 10.970 VLoss: 10.304 TAcc: 0.554 VAcc: 0.600\n",
      "[20240321 12:55:40] [Test] Precision: 0.41538462 Recall: 0.52768730 F1: 0.46484935\n",
      "[20240321 12:55:40] [epoch 33] TLoss: 10.821 VLoss: 10.214 TAcc: 0.555 VAcc: 0.598\n",
      "[20240321 12:56:32] [Test] Precision: 0.40153453 Recall: 0.51140065 F1: 0.44985673\n",
      "[20240321 12:56:32] [epoch 34] TLoss: 10.688 VLoss: 10.118 TAcc: 0.557 VAcc: 0.597\n",
      "[20240321 12:57:25] [Test] Precision: 0.39500861 Recall: 0.49837134 F1: 0.44071051\n",
      "[20240321 12:57:25] [epoch 35] TLoss: 10.579 VLoss: 10.032 TAcc: 0.554 VAcc: 0.597\n",
      "[20240321 12:58:17] [Test] Precision: 0.39932031 Recall: 0.51031488 F1: 0.44804576\n",
      "[20240321 12:58:17] [epoch 36] TLoss: 10.470 VLoss: 9.926 TAcc: 0.556 VAcc: 0.600\n",
      "[20240321 12:59:11] [Test] Precision: 0.40418118 Recall: 0.50380022 F1: 0.44852586\n",
      "[20240321 12:59:11] [epoch 37] TLoss: 10.364 VLoss: 9.823 TAcc: 0.551 VAcc: 0.602\n",
      "[20240321 13:00:03] [Test] Precision: 0.40611725 Recall: 0.51900109 F1: 0.45567207\n",
      "[20240321 13:00:03] [epoch 38] TLoss: 10.257 VLoss: 9.785 TAcc: 0.554 VAcc: 0.599\n",
      "[20240321 13:00:55] [Test] Precision: 0.36355932 Recall: 0.46579805 F1: 0.40837696\n",
      "[20240321 13:00:55] [epoch 39] TLoss: 10.139 VLoss: 9.740 TAcc: 0.556 VAcc: 0.592\n",
      "[20240321 13:01:48] [Test] Precision: 0.38921651 Recall: 0.50162866 F1: 0.43833017\n",
      "[20240321 13:01:48] [epoch 40] TLoss: 10.102 VLoss: 9.625 TAcc: 0.558 VAcc: 0.600\n",
      "[20240321 13:02:40] [Test] Precision: 0.37759336 Recall: 0.49402823 F1: 0.42803387\n",
      "[20240321 13:02:40] [epoch 41] TLoss: 9.981 VLoss: 9.610 TAcc: 0.555 VAcc: 0.595\n",
      "[20240321 13:03:32] [Test] Precision: 0.38945578 Recall: 0.49728556 F1: 0.43681450\n",
      "[20240321 13:03:32] [epoch 42] TLoss: 9.916 VLoss: 9.519 TAcc: 0.556 VAcc: 0.598\n",
      "[20240321 13:04:24] [Test] Precision: 0.39273649 Recall: 0.50488599 F1: 0.44180523\n",
      "[20240321 13:04:24] [epoch 43] TLoss: 9.838 VLoss: 9.471 TAcc: 0.556 VAcc: 0.598\n",
      "[20240321 13:05:16] [Test] Precision: 0.37683665 Recall: 0.47339848 F1: 0.41963426\n",
      "[20240321 13:05:16] [epoch 44] TLoss: 9.750 VLoss: 9.415 TAcc: 0.557 VAcc: 0.598\n",
      "[20240321 13:06:08] [Test] Precision: 0.35270049 Recall: 0.46796960 F1: 0.40223985\n",
      "[20240321 13:06:08] [epoch 45] TLoss: 9.712 VLoss: 9.400 TAcc: 0.556 VAcc: 0.592\n",
      "[20240321 13:07:00] [Test] Precision: 0.35473251 Recall: 0.46796960 F1: 0.40355805\n",
      "[20240321 13:07:00] [epoch 46] TLoss: 9.626 VLoss: 9.352 TAcc: 0.558 VAcc: 0.590\n",
      "[20240321 13:07:52] [Test] Precision: 0.34089132 Recall: 0.47339848 F1: 0.39636364\n",
      "[20240321 13:07:52] [epoch 47] TLoss: 9.571 VLoss: 9.359 TAcc: 0.556 VAcc: 0.577\n",
      "[20240321 13:08:43] [Test] Precision: 0.34000000 Recall: 0.44299674 F1: 0.38472419\n",
      "[20240321 13:08:43] [epoch 48] TLoss: 9.562 VLoss: 9.255 TAcc: 0.559 VAcc: 0.592\n",
      "[20240321 13:09:37] [Test] Precision: 0.34111760 Recall: 0.44408252 F1: 0.38584906\n",
      "[20240321 13:09:37] [epoch 49] TLoss: 9.472 VLoss: 9.246 TAcc: 0.562 VAcc: 0.588\n",
      "[20240321 13:10:28] [Test] Precision: 0.36159601 Recall: 0.47231270 F1: 0.40960452\n",
      "[20240321 13:10:28] [epoch 50] TLoss: 9.420 VLoss: 9.195 TAcc: 0.556 VAcc: 0.591\n",
      "[20240321 13:11:20] [Test] Precision: 0.33465190 Recall: 0.45928339 F1: 0.38718535\n",
      "[20240321 13:11:20] [epoch 51] TLoss: 9.380 VLoss: 9.195 TAcc: 0.556 VAcc: 0.580\n",
      "[20240321 13:12:13] [Test] Precision: 0.34257749 Recall: 0.45602606 F1: 0.39124360\n",
      "[20240321 13:12:13] [epoch 52] TLoss: 9.344 VLoss: 9.146 TAcc: 0.556 VAcc: 0.587\n",
      "[20240321 13:13:05] [Test] Precision: 0.36090835 Recall: 0.48317047 F1: 0.41318477\n",
      "[20240321 13:13:05] [epoch 53] TLoss: 9.276 VLoss: 9.092 TAcc: 0.559 VAcc: 0.590\n",
      "[20240321 13:13:57] [Test] Precision: 0.34704370 Recall: 0.43973941 F1: 0.38793103\n",
      "[20240321 13:13:57] [epoch 54] TLoss: 9.225 VLoss: 9.050 TAcc: 0.560 VAcc: 0.594\n",
      "[20240321 13:14:48] [Test] Precision: 0.30762712 Recall: 0.39413681 F1: 0.34554974\n",
      "[20240321 13:14:48] [epoch 55] TLoss: 9.199 VLoss: 9.063 TAcc: 0.559 VAcc: 0.581\n",
      "[20240321 13:15:41] [Test] Precision: 0.29354047 Recall: 0.38979370 F1: 0.33488806\n",
      "[20240321 13:15:41] [epoch 56] TLoss: 9.178 VLoss: 9.062 TAcc: 0.559 VAcc: 0.576\n",
      "[20240321 13:16:35] [Test] Precision: 0.30456026 Recall: 0.40608035 F1: 0.34806887\n",
      "[20240321 13:16:35] [epoch 57] TLoss: 9.148 VLoss: 9.007 TAcc: 0.561 VAcc: 0.583\n",
      "[20240321 13:17:27] [Test] Precision: 0.30664395 Recall: 0.39087948 F1: 0.34367542\n",
      "[20240321 13:17:27] [epoch 58] TLoss: 9.124 VLoss: 8.962 TAcc: 0.558 VAcc: 0.587\n",
      "[20240321 13:18:20] [Test] Precision: 0.30846605 Recall: 0.39956569 F1: 0.34815516\n",
      "[20240321 13:18:20] [epoch 59] TLoss: 9.073 VLoss: 8.961 TAcc: 0.559 VAcc: 0.581\n",
      "[20240321 13:19:13] [Test] Precision: 0.31583404 Recall: 0.40499457 F1: 0.35490009\n",
      "[20240321 13:19:13] [epoch 60] TLoss: 9.056 VLoss: 8.951 TAcc: 0.560 VAcc: 0.579\n",
      "[20240321 13:20:07] [Test] Precision: 0.29360695 Recall: 0.40390879 F1: 0.34003656\n",
      "[20240321 13:20:07] [epoch 61] TLoss: 9.034 VLoss: 8.977 TAcc: 0.560 VAcc: 0.566\n",
      "[20240321 13:21:01] [Test] Precision: 0.29504132 Recall: 0.38762215 F1: 0.33505396\n",
      "[20240321 13:21:01] [epoch 62] TLoss: 9.003 VLoss: 8.947 TAcc: 0.560 VAcc: 0.575\n",
      "[20240321 13:21:56] [Test] Precision: 0.28442623 Recall: 0.37676439 F1: 0.32414759\n",
      "[20240321 13:21:56] [epoch 63] TLoss: 8.957 VLoss: 8.924 TAcc: 0.560 VAcc: 0.575\n",
      "[20240321 13:22:51] [Test] Precision: 0.29207120 Recall: 0.39196526 F1: 0.33472415\n",
      "[20240321 13:22:51] [epoch 64] TLoss: 8.952 VLoss: 8.912 TAcc: 0.563 VAcc: 0.571\n",
      "[20240321 13:23:47] [Test] Precision: 0.30625508 Recall: 0.40933768 F1: 0.35037175\n",
      "[20240321 13:23:47] [epoch 65] TLoss: 8.936 VLoss: 8.901 TAcc: 0.559 VAcc: 0.571\n",
      "[20240321 13:24:39] [Test] Precision: 0.30032733 Recall: 0.39847991 F1: 0.34251050\n",
      "[20240321 13:24:39] [epoch 66] TLoss: 8.884 VLoss: 8.870 TAcc: 0.562 VAcc: 0.577\n",
      "[20240321 13:25:32] [Test] Precision: 0.29270315 Recall: 0.38327904 F1: 0.33192290\n",
      "[20240321 13:25:32] [epoch 67] TLoss: 8.849 VLoss: 8.853 TAcc: 0.563 VAcc: 0.578\n",
      "[20240321 13:26:25] [Test] Precision: 0.29356358 Recall: 0.40608035 F1: 0.34077449\n",
      "[20240321 13:26:25] [epoch 68] TLoss: 8.866 VLoss: 8.884 TAcc: 0.563 VAcc: 0.565\n",
      "[20240321 13:27:17] [Test] Precision: 0.28662930 Recall: 0.38870793 F1: 0.32995392\n",
      "[20240321 13:27:17] [epoch 69] TLoss: 8.821 VLoss: 8.857 TAcc: 0.561 VAcc: 0.566\n",
      "[20240321 13:28:11] [Test] Precision: 0.27476038 Recall: 0.37350706 F1: 0.31661298\n",
      "[20240321 13:28:11] [epoch 70] TLoss: 8.830 VLoss: 8.859 TAcc: 0.563 VAcc: 0.568\n",
      "[20240321 13:29:02] [Test] Precision: 0.28176796 Recall: 0.38762215 F1: 0.32632541\n",
      "[20240321 13:29:02] [epoch 71] TLoss: 8.802 VLoss: 8.838 TAcc: 0.561 VAcc: 0.571\n",
      "[20240321 13:29:53] [Test] Precision: 0.27656250 Recall: 0.38436482 F1: 0.32167197\n",
      "[20240321 13:29:53] [epoch 72] TLoss: 8.772 VLoss: 8.829 TAcc: 0.560 VAcc: 0.571\n",
      "[20240321 13:30:45] [Test] Precision: 0.28870968 Recall: 0.38870793 F1: 0.33132809\n",
      "[20240321 13:30:45] [epoch 73] TLoss: 8.779 VLoss: 8.831 TAcc: 0.565 VAcc: 0.566\n",
      "[20240321 13:31:37] [Test] Precision: 0.27159533 Recall: 0.37893594 F1: 0.31640979\n",
      "[20240321 13:31:37] [epoch 74] TLoss: 8.740 VLoss: 8.815 TAcc: 0.562 VAcc: 0.566\n",
      "[20240321 13:32:28] [Test] Precision: 0.27764326 Recall: 0.37350706 F1: 0.31851852\n",
      "[20240321 13:32:28] [epoch 75] TLoss: 8.736 VLoss: 8.801 TAcc: 0.560 VAcc: 0.571\n",
      "[20240321 13:33:18] [Test] Precision: 0.27634660 Recall: 0.38436482 F1: 0.32152589\n",
      "[20240321 13:33:18] [epoch 76] TLoss: 8.714 VLoss: 8.830 TAcc: 0.564 VAcc: 0.559\n",
      "[20240321 13:34:10] [Test] Precision: 0.26524614 Recall: 0.39196526 F1: 0.31638913\n",
      "[20240321 13:34:10] [epoch 77] TLoss: 8.729 VLoss: 8.844 TAcc: 0.566 VAcc: 0.553\n",
      "[20240321 13:35:03] [Test] Precision: 0.26063418 Recall: 0.36590662 F1: 0.30442638\n",
      "[20240321 13:35:03] [epoch 78] TLoss: 8.697 VLoss: 8.799 TAcc: 0.563 VAcc: 0.562\n",
      "[20240321 13:35:58] [Test] Precision: 0.27159533 Recall: 0.37893594 F1: 0.31640979\n",
      "[20240321 13:35:58] [epoch 79] TLoss: 8.680 VLoss: 8.824 TAcc: 0.563 VAcc: 0.557\n",
      "[20240321 13:36:52] [Test] Precision: 0.26452599 Recall: 0.37567861 F1: 0.31045312\n",
      "[20240321 13:36:52] [epoch 80] TLoss: 8.675 VLoss: 8.797 TAcc: 0.564 VAcc: 0.558\n",
      "[20240321 13:37:45] [Test] Precision: 0.26888708 Recall: 0.35939197 F1: 0.30762082\n",
      "[20240321 13:37:45] [epoch 81] TLoss: 8.667 VLoss: 8.783 TAcc: 0.566 VAcc: 0.568\n",
      "[20240321 13:38:36] [Test] Precision: 0.27872861 Recall: 0.37133550 F1: 0.31843575\n",
      "[20240321 13:38:36] [epoch 82] TLoss: 8.649 VLoss: 8.777 TAcc: 0.566 VAcc: 0.570\n",
      "[20240321 13:39:27] [Test] Precision: 0.26235741 Recall: 0.37459283 F1: 0.30858676\n",
      "[20240321 13:39:27] [epoch 83] TLoss: 8.608 VLoss: 8.811 TAcc: 0.564 VAcc: 0.557\n",
      "[20240321 13:40:17] [Test] Precision: 0.26425270 Recall: 0.37242128 F1: 0.30914826\n",
      "[20240321 13:40:17] [epoch 84] TLoss: 8.614 VLoss: 8.801 TAcc: 0.566 VAcc: 0.555\n",
      "[20240321 13:41:09] [Test] Precision: 0.26566217 Recall: 0.36373507 F1: 0.30705774\n",
      "[20240321 13:41:09] [epoch 85] TLoss: 8.630 VLoss: 8.770 TAcc: 0.565 VAcc: 0.563\n",
      "[20240321 13:42:01] [Test] Precision: 0.26419558 Recall: 0.36373507 F1: 0.30607583\n",
      "[20240321 13:42:01] [epoch 86] TLoss: 8.624 VLoss: 8.773 TAcc: 0.563 VAcc: 0.564\n",
      "[20240321 13:42:54] [Test] Precision: 0.25517241 Recall: 0.36156352 F1: 0.29919137\n",
      "[20240321 13:42:54] [epoch 87] TLoss: 8.591 VLoss: 8.814 TAcc: 0.563 VAcc: 0.552\n",
      "[20240321 13:43:46] [Test] Precision: 0.26376147 Recall: 0.37459283 F1: 0.30955585\n",
      "[20240321 13:43:46] [epoch 88] TLoss: 8.580 VLoss: 8.787 TAcc: 0.565 VAcc: 0.556\n",
      "[20240321 13:44:40] [Test] Precision: 0.26067073 Recall: 0.37133550 F1: 0.30631437\n",
      "[20240321 13:44:40] [epoch 89] TLoss: 8.574 VLoss: 8.776 TAcc: 0.568 VAcc: 0.558\n",
      "[20240321 13:45:33] [Test] Precision: 0.25417299 Recall: 0.36373507 F1: 0.29924073\n",
      "[20240321 13:45:33] [epoch 90] TLoss: 8.561 VLoss: 8.804 TAcc: 0.566 VAcc: 0.555\n",
      "[20240321 13:46:25] [Test] Precision: 0.26598264 Recall: 0.36590662 F1: 0.30804388\n",
      "[20240321 13:46:25] [epoch 91] TLoss: 8.571 VLoss: 8.769 TAcc: 0.566 VAcc: 0.565\n",
      "[20240321 13:47:18] [Test] Precision: 0.25509434 Recall: 0.36699240 F1: 0.30097952\n",
      "[20240321 13:47:18] [epoch 92] TLoss: 8.556 VLoss: 8.808 TAcc: 0.564 VAcc: 0.549\n",
      "[20240321 13:48:11] [Test] Precision: 0.26219512 Recall: 0.37350706 F1: 0.30810569\n",
      "[20240321 13:48:11] [epoch 93] TLoss: 8.534 VLoss: 8.773 TAcc: 0.569 VAcc: 0.554\n",
      "[20240321 13:49:03] [Test] Precision: 0.26619273 Recall: 0.36590662 F1: 0.30818473\n",
      "[20240321 13:49:03] [epoch 94] TLoss: 8.543 VLoss: 8.768 TAcc: 0.571 VAcc: 0.562\n",
      "[20240321 13:49:56] [Test] Precision: 0.25983038 Recall: 0.36590662 F1: 0.30387737\n",
      "[20240321 13:49:56] [epoch 95] TLoss: 8.506 VLoss: 8.767 TAcc: 0.566 VAcc: 0.560\n",
      "[20240321 13:50:50] [Test] Precision: 0.25442478 Recall: 0.37459283 F1: 0.30303030\n",
      "[20240321 13:50:50] [epoch 96] TLoss: 8.522 VLoss: 8.797 TAcc: 0.568 VAcc: 0.545\n",
      "[20240321 13:51:43] [Test] Precision: 0.25716440 Recall: 0.37024973 F1: 0.30351580\n",
      "[20240321 13:51:43] [epoch 97] TLoss: 8.513 VLoss: 8.761 TAcc: 0.573 VAcc: 0.559\n",
      "[20240321 13:52:37] [Test] Precision: 0.25611621 Recall: 0.36373507 F1: 0.30058322\n",
      "[20240321 13:52:37] [epoch 98] TLoss: 8.498 VLoss: 8.800 TAcc: 0.566 VAcc: 0.552\n",
      "[20240321 13:53:32] [Test] Precision: 0.25995492 Recall: 0.37567861 F1: 0.30728242\n",
      "[20240321 13:53:32] [epoch 99] TLoss: 8.453 VLoss: 8.795 TAcc: 0.571 VAcc: 0.551\n",
      "[20240321 13:54:29] [Test] Precision: 0.25868726 Recall: 0.36373507 F1: 0.30234657\n",
      "[20240321 13:54:29] [epoch 100] TLoss: 8.474 VLoss: 8.743 TAcc: 0.569 VAcc: 0.560\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'epochs': 100, \n",
    "    'path': '/data/pretrained/bert-base-chinese/',\n",
    "    'emb_dim': 300,\n",
    "    'hidden_dim': 200,\n",
    "    'num_layers': 1,\n",
    "    'n_class': 9, \n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "model = LSTMCRF(args, word_emb, tag_dict).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "early_stopping = EarlyStopping(patience = 10, verbose = False)\n",
    "entrophy = nn.CrossEntropyLoss()\n",
    "\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "for epoch in range(args['epochs']):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for _, batch in enumerate(train_loader):\n",
    "        tokens, masks, labels = batch\n",
    "        sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "        tokens = tokens[:, :sen_len]\n",
    "        masks = masks[:, :sen_len]\n",
    "        labels = labels[:, :sen_len]\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(tokens, masks, labels) # (n_batch, n_token, n_class)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predict = model.predict(tokens, masks) # (n_batch, n_tokens)\n",
    "        train_correct += torch.sum(predict[masks == 1] == labels[masks == 1]).item()\n",
    "        train_total += torch.sum(masks == 1).item()\n",
    "    avg_train_loss = np.average(train_losses)\n",
    "    avg_train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gold_num = 0\n",
    "        predict_num = 0\n",
    "        correct_num = 0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        for i, batch in enumerate(valid_loader):\n",
    "            tokens, masks, labels = batch\n",
    "            sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "            tokens = tokens[:, :sen_len]\n",
    "            masks = masks[:, :sen_len]\n",
    "            labels = labels[:, :sen_len]\n",
    "            loss = model(tokens, masks, labels)\n",
    "            valid_losses.append(loss.item())\n",
    "            predict = model.predict(tokens, masks)\n",
    "            valid_correct += torch.sum(predict[masks == 1] == labels[masks == 1]).item()\n",
    "            valid_total += torch.sum(masks == 1).item()\n",
    "            for j in range(labels.shape[0]):\n",
    "                gold_entity = label_sentence_entity(text[j], labels[j].tolist(), tag_list)\n",
    "                pred_entity = label_sentence_entity(text[j], predict[j], tag_list)\n",
    "                gold_num += len(gold_entity)\n",
    "                predict_num += len(pred_entity)\n",
    "                for entity in gold_entity:\n",
    "                    if entity in pred_entity:\n",
    "                        correct_num += 1\n",
    "        avg_valid_loss = np.average(valid_losses)\n",
    "        avg_valid_losses.append(avg_valid_loss)\n",
    "    precision = correct_num / (predict_num + 0.000000001)\n",
    "    recall = correct_num / (gold_num + 0.000000001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.000000001)\n",
    "    logger('[Test] Precision: {:.8f} Recall: {:.8f} F1: {:.8f}'.format(precision, recall, f1))\n",
    "    \n",
    "    logger('[epoch {:d}] TLoss: {:.3f} VLoss: {:.3f} TAcc: {:.3f} VAcc: {:.3f}'.format(\n",
    "            epoch + 1, avg_train_loss, avg_valid_loss, train_correct / train_total, valid_correct / valid_total))\n",
    "    early_stopping(avg_valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        logger(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# plot_result(avg_train_losses, avg_valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56b35b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:59:32.420276Z",
     "start_time": "2022-04-20T08:59:32.408043Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d775f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T08:59:50.737389Z",
     "start_time": "2022-04-20T08:59:46.475685Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-0420 16:59:50] [Test] Tagging accuracy: 0.52943021\n",
      "[2022-0420 16:59:50] [Test] Precision: 0.21648288 Recall: 0.12275794 F1: 0.15667332\n",
      "[2022-0420 16:59:50] Tagging accuracy: 0.5294302061290069\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "correct_num, gold_num, predict_num = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        tokens, masks, labels = batch\n",
    "        sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "        tokens = tokens[:, :sen_len]\n",
    "        masks = masks[:, :sen_len]\n",
    "        labels = labels[:, :sen_len]\n",
    "        predict = model.predict(tokens, masks)\n",
    "#         predict = torch.max(output, dim = 2).indices # (n_batch, n_tokens)\n",
    "        correct += torch.sum(predict[masks == 1] == labels[masks == 1]).item()\n",
    "        total += torch.sum(masks).item()\n",
    "        for j in range(labels.shape[0]):\n",
    "            text = [word_list[int(w)] for w in tokens[j][masks[j] == 1]]\n",
    "            gold_entity = label_sentence_entity(text, labels[j].tolist(), tag_list)\n",
    "            pred_entity = label_sentence_entity(text, predict[j], tag_list)\n",
    "            gold_num += len(gold_entity)\n",
    "            predict_num += len(pred_entity)\n",
    "            for entity in gold_entity:\n",
    "                if entity in pred_entity:\n",
    "                    correct_num += 1\n",
    "#             print(gold_entity)\n",
    "#             print(pred_entity)\n",
    "#             gold_entity()\n",
    "    precision = correct_num / (predict_num + 0.000000001)\n",
    "    recall = correct_num / (gold_num + 0.000000001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.000000001)\n",
    "    logger('[Test] Tagging accuracy: {:.8f}'.format(correct / total))\n",
    "    logger('[Test] Precision: {:.8f} Recall: {:.8f} F1: {:.8f}'.format(precision, recall, f1))\n",
    "logger('Tagging accuracy: {}'.format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedc00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eacf43d0",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b619b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, tag_list):\n",
    "    model.eval()\n",
    "    correct_num, predict_num, gold_num = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            # tokens, masks, labels, texts = batch\n",
    "            # print(tokens.shape, masks.shape, labels.shape, len(texts))\n",
    "            # sen_len = torch.max(torch.sum(masks, dim = 1, dtype = torch.int64)).item()\n",
    "            # output = model(**batch) # (n_batch, n_token, n_class)\n",
    "            tokens, masks, labels, texts = batch['tokens'], batch['masks'], batch['labels'], batch['texts']\n",
    "            sen_len = max(len(text) for text in texts)\n",
    "            # tokens = tokens[:, :sen_len]\n",
    "            # masks = masks[:, :sen_len]\n",
    "            labels = labels[:, :sen_len]\n",
    "            pred = model.predict(**batch)[:, :sen_len]\n",
    "            # print(pred)\n",
    "            for j in range(labels.shape[0]):\n",
    "                gold_entity = label_sentence_entity(texts[j], labels[j].tolist(), tag_list)\n",
    "                pred_entity = label_sentence_entity(texts[j], pred[j], tag_list)\n",
    "                gold_num += len(gold_entity)\n",
    "                predict_num += len(pred_entity)\n",
    "                for entity in gold_entity:\n",
    "                    if entity in pred_entity:\n",
    "                        correct_num += 1\n",
    "                # print(gold_entity)\n",
    "                # print(pred_entity)\n",
    "                # return\n",
    "    precision = correct_num / (predict_num + 0.000000001)\n",
    "    recall = correct_num / (gold_num + 0.000000001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.000000001)\n",
    "    logger('[Test] Precision: {:.6f} Recall: {:.6f} F1: {:.6f}'.format(precision, recall, f1))\n",
    "    return precision, recall, f1\n",
    "\n",
    "# evaluate(model, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "362818fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def save_model(model, path):\n",
    "    ts = time.strftime('%m%d%H%M', time.localtime())\n",
    "    torch.save(model.state_dict(), '{}.{}'.format(path, ts))\n",
    "    print('Save model to', '{}.{}'.format(path, ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c0822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "def cls_metrics(model, data_loader):\n",
    "    all_label = []\n",
    "    all_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels, texts = batch\n",
    "            logits = model(input_ids, attention_mask)['logits']\n",
    "            pred = torch.argmax(logits, dim = 1)\n",
    "            all_label.extend(labels.tolist())\n",
    "            all_pred.extend(pred.detach().cpu().numpy().tolist())\n",
    "    acc = accuracy_score(all_label, all_pred)\n",
    "    p = precision_score(all_label, all_pred, average = 'macro')\n",
    "    r = recall_score(all_label, all_pred, average = 'macro')\n",
    "    f1 = f1_score(all_label, all_pred, average = 'macro')\n",
    "    # logger('[Eval] {} Acc: {:.4f}, P: {:.4f}, R: {:.4f}, F1: {:.4f}'.format(data_loader.__str__, acc, p, r, f1))\n",
    "    return acc, p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8347f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, epochs = 100, lr = 1e-5):\n",
    "    print('Train {} model with lr = {}'.format(model.__class__.__name__, lr))\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    early_stopping = EarlyStopping(patience = 5, verbose = False)\n",
    "    entrophy = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        model.train()\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # tokens, masks, labels, texts = batch\n",
    "            # output = model(tokens, masks, labels) # (n_batch, n_token, n_class)\n",
    "            output = model(**batch) # (n_batch, n_token, n_class)\n",
    "            masks, labels = batch['masks'], batch['labels']\n",
    "            loss = entrophy(output.permute(0, 2, 1), labels[:, :output.shape[1]])\n",
    "            # loss = entrophy(output.permute(0, 2, 1), labels[:])\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predict = torch.max(output, dim = 2).indices # (n_batch, n_tokens)\n",
    "            # train_correct += torch.sum(predict == labels[:, :output.shape[1]]).item()\n",
    "            train_correct += torch.sum(torch.logical_and(predict == labels[:, :output.shape[1]], masks == 1)).item()\n",
    "            train_total += torch.sum(masks == 1).item()\n",
    "        avg_train_loss = np.average(train_losses)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(valid_loader):\n",
    "                tokens, masks, labels, texts = batch\n",
    "                # output = model(tokens, masks, labels)\n",
    "                output = model(**batch)\n",
    "                masks, labels = batch['masks'], batch['labels']\n",
    "                loss = entrophy(output.permute(0, 2, 1), labels[:, :output.shape[1]])\n",
    "                valid_losses.append(loss.item())\n",
    "                predict = torch.max(output, dim = 2).indices # (n_batch, n_tokens)\n",
    "                # valid_correct += torch.sum(predict == labels[:, :output.shape[1]]).item()\n",
    "                valid_correct += torch.sum(torch.logical_and(predict == labels[:, :output.shape[1]], masks == 1)).item()\n",
    "                valid_total += torch.sum(masks == 1).item()\n",
    "            avg_valid_loss = np.average(valid_losses)\n",
    "        precision, recall, f1 = evaluate(model, valid_loader, tag_list)\n",
    "        \n",
    "        # logger('[epoch {:d}] TLoss: {:.3f} VLoss: {:.3f} TAcc: {:.3f} VAcc: {:.3f}'.format(\n",
    "        #     epoch + 1, avg_train_loss, avg_valid_loss, train_correct / train_total, valid_correct / valid_total))\n",
    "        logger('[epoch {:d}] TLoss: {:.3f} VLoss: {:.3f} TAcc: {:.3f} VAcc: {:.3f}'.format(\n",
    "            epoch + 1, avg_train_loss, avg_valid_loss, train_correct / train_total, valid_correct / valid_total))\n",
    "        # logger('Precision: {:.3f} Recall: {:.3f} F1: {:.3f}'.format(precision, recall, f1))\n",
    "        # early_stopping(f1, model)\n",
    "        early_stopping(-valid_correct / valid_total, model)\n",
    "        if early_stopping.early_stop:\n",
    "            logger(\"Early stopping\")\n",
    "            break\n",
    "    save_model(model, './results/{}.{}'.format(model.__class__.__name__, lr))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438612cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crf(model, epochs = 100, lr = 1e-5):\n",
    "    print('Train {} model with lr = {}'.format(model.__class__.__name__, lr))\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    early_stopping = EarlyStopping(patience = 5, verbose = False)\n",
    "    entrophy = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        model.train()\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            # tokens, masks, labels, texts = batch\n",
    "            optimizer.zero_grad()\n",
    "            # loss = model(tokens, masks, labels) # (n_batch, n_token, n_class)\n",
    "            masks, labels = batch['masks'], batch['labels']\n",
    "            loss = model(**batch) # (n_batch, n_token, n_class)\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predict = model.predict(**batch) # (n_batch, n_tokens)\n",
    "            # train_correct += torch.sum(predict == labels[:, :output.shape[1]]).item()\n",
    "            train_correct += torch.sum(torch.logical_and(predict == labels[:, :predict.shape[1]], masks == 1)).item()\n",
    "            train_total += torch.sum(masks == 1).item()\n",
    "        avg_train_loss = np.average(train_losses)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(valid_loader):\n",
    "                # tokens, masks, labels, texts = batch\n",
    "                # loss = model(tokens, masks, labels)\n",
    "                masks, labels = batch['masks'], batch['labels']\n",
    "                loss = model(**batch)\n",
    "                valid_losses.append(loss.item())\n",
    "                predict = model.predict(**batch)\n",
    "                # predict = model.predict(tokens, masks) # (n_batch, n_tokens)\n",
    "                # valid_correct += torch.sum(predict == labels[:, :predict.shape[1]]).item()\n",
    "                valid_correct += torch.sum(torch.logical_and(predict == labels[:, :predict.shape[1]], masks == 1)).item()\n",
    "                valid_total += torch.sum(masks == 1).item()\n",
    "            avg_valid_loss = np.average(valid_losses)\n",
    "        precision, recall, f1 = evaluate(model, valid_loader, tag_list)\n",
    "        \n",
    "        # logger('[epoch {:d}] TLoss: {:.3f} VLoss: {:.3f} TAcc: {:.3f} VAcc: {:.3f}'.format(\n",
    "        #     epoch + 1, avg_train_loss, avg_valid_loss, train_correct / train_total, valid_correct / valid_total))\n",
    "        logger('[epoch {:d}] TLoss: {:.3f} VLoss: {:.3f} TAcc: {:.3f} VAcc: {:.3f}'.format(\n",
    "            epoch + 1, avg_train_loss, avg_valid_loss, train_correct / train_total, valid_correct / valid_total))\n",
    "        # logger('Precision: {:.3f} Recall: {:.3f} F1: {:.3f}'.format(precision, recall, f1))\n",
    "        # early_stopping(f1, model)\n",
    "        early_stopping(-valid_correct / valid_total, model)\n",
    "        if early_stopping.early_stop:\n",
    "            logger(\"Early stopping\")\n",
    "            break\n",
    "    save_model(model, './results/{}.{}'.format(model.__class__.__name__, lr))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b6b0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "args = {\n",
    "    'path': '/data/pretrained/bert-base-chinese/',\n",
    "    'bert_out_dim': 768,\n",
    "    'n_class': 9, \n",
    "    'dropout': 0.2\n",
    "}\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "def collate(batch):\n",
    "    tokens = tokenizer([item['text'] for item in batch], \n",
    "                       padding = 'max_length', truncation = True, max_length = 40, return_tensors = 'pt')\n",
    "    labels = torch.tensor([item['labels'] for item in batch], dtype = torch.long, device = device)\n",
    "    # entity_embeds = torch.tensor([item['entity_embeds'] for item in batch], dtype = torch.float, device = device)\n",
    "    # return tokens['input_ids'].to(device), tokens['attention_mask'].to(device), labels, [item['text'] for item in batch]\n",
    "    return {\n",
    "        'tokens': tokens['input_ids'].to(device),\n",
    "        'masks': tokens['attention_mask'].to(device),\n",
    "        'labels': labels,\n",
    "        'texts': [item['text'] for item in batch],\n",
    "        # 'entity_embeds': entity_embeds,\n",
    "    }\n",
    "\n",
    "n_train, n_dev = int(0.6 * len(tagged_dataset)), int(0.2 * len(tagged_dataset))\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(tagged_dataset[:n_train], batch_size = batch_size, collate_fn = collate)\n",
    "valid_loader = DataLoader(tagged_dataset[n_train: n_train + n_dev], batch_size = batch_size, collate_fn = collate)\n",
    "test_loader = DataLoader(tagged_dataset[n_train + n_dev:], batch_size = batch_size, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2212b7b",
   "metadata": {},
   "source": [
    "### BertLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLinear(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.cls = nn.Linear(args['bert_out_dim'], args['n_class'])\n",
    "        \n",
    "    def forward(self, tokens, masks, labels = None):\n",
    "        bert_out = self.bert(tokens, masks)['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        return cls_out\n",
    "\n",
    "    def predict(self, tokens, masks):\n",
    "        cls_out = self.forward(tokens, masks)\n",
    "        pred = torch.max(cls_out, dim = 2).indices\n",
    "        return pred\n",
    "\n",
    "# model = BertLinear(args, tokenizer, tag_dict).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab2c69d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[20231212 10:18:27] [Test] Precision: 0.090909 Recall: 0.023110 F1: 0.036852\n",
      "[20231212 10:18:27] [epoch 1] TLoss: 1.018 VLoss: 0.617 TAcc: 0.507 VAcc: 0.630\n",
      "[20231212 10:19:39] [Test] Precision: 0.344449 Recall: 0.297405 F1: 0.319203\n",
      "[20231212 10:19:39] [epoch 2] TLoss: 0.583 VLoss: 0.514 TAcc: 0.637 VAcc: 0.678\n",
      "[20231212 10:20:54] [Test] Precision: 0.414243 Recall: 0.391173 F1: 0.402377\n",
      "[20231212 10:20:54] [epoch 3] TLoss: 0.495 VLoss: 0.488 TAcc: 0.681 VAcc: 0.692\n",
      "[20231212 10:22:08] [Test] Precision: 0.470727 Recall: 0.453874 F1: 0.462147\n",
      "[20231212 10:22:08] [epoch 4] TLoss: 0.455 VLoss: 0.475 TAcc: 0.702 VAcc: 0.705\n",
      "[20231212 10:23:20] [Test] Precision: 0.501184 Recall: 0.481152 F1: 0.490964\n",
      "[20231212 10:23:20] [epoch 5] TLoss: 0.429 VLoss: 0.465 TAcc: 0.720 VAcc: 0.711\n",
      "[20231212 10:24:34] [Test] Precision: 0.508745 Recall: 0.495927 F1: 0.502254\n",
      "[20231212 10:24:34] [epoch 6] TLoss: 0.410 VLoss: 0.462 TAcc: 0.732 VAcc: 0.713\n",
      "[20231212 10:25:48] [Test] Precision: 0.519486 Recall: 0.505020 F1: 0.512151\n",
      "[20231212 10:25:48] [epoch 7] TLoss: 0.398 VLoss: 0.466 TAcc: 0.738 VAcc: 0.710\n",
      "[20231212 10:26:53] [Test] Precision: 0.515464 Recall: 0.501989 F1: 0.508637\n",
      "[20231212 10:26:53] [epoch 8] TLoss: 0.388 VLoss: 0.463 TAcc: 0.746 VAcc: 0.710\n",
      "[20231212 10:28:00] [Test] Precision: 0.518861 Recall: 0.510703 F1: 0.514749\n",
      "[20231212 10:28:00] [epoch 9] TLoss: 0.379 VLoss: 0.465 TAcc: 0.751 VAcc: 0.709\n",
      "[20231212 10:29:11] [Test] Precision: 0.525793 Recall: 0.511650 F1: 0.518625\n",
      "[20231212 10:29:11] [epoch 10] TLoss: 0.371 VLoss: 0.460 TAcc: 0.755 VAcc: 0.713\n",
      "[20231212 10:30:23] [Test] Precision: 0.523660 Recall: 0.505209 F1: 0.514269\n",
      "[20231212 10:30:23] [epoch 11] TLoss: 0.364 VLoss: 0.473 TAcc: 0.761 VAcc: 0.706\n",
      "[20231212 10:31:34] [Test] Precision: 0.527016 Recall: 0.502557 F1: 0.514496\n",
      "[20231212 10:31:34] [epoch 12] TLoss: 0.359 VLoss: 0.465 TAcc: 0.765 VAcc: 0.710\n",
      "[20231212 10:32:45] [Test] Precision: 0.528620 Recall: 0.505588 F1: 0.516847\n",
      "[20231212 10:32:45] [epoch 13] TLoss: 0.352 VLoss: 0.474 TAcc: 0.769 VAcc: 0.706\n",
      "[20231212 10:33:56] [Test] Precision: 0.529668 Recall: 0.507293 F1: 0.518239\n",
      "[20231212 10:33:56] [epoch 14] TLoss: 0.345 VLoss: 0.472 TAcc: 0.774 VAcc: 0.709\n",
      "[20231212 10:35:06] [Test] Precision: 0.528633 Recall: 0.507104 F1: 0.517645\n",
      "[20231212 10:35:06] [epoch 15] TLoss: 0.340 VLoss: 0.471 TAcc: 0.777 VAcc: 0.708\n",
      "[20231212 10:35:06] Early stopping\n",
      "[20231212 10:35:11] [Test] Precision: 0.555088 Recall: 0.434425 F1: 0.487400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5550883095036679, 0.4344248806976412, 0.48739961180310787)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BertLinear\n",
    "model = BertLinear(args).to(device)\n",
    "train(model, epochs = 100, lr = 1e-6)\n",
    "evaluate(model, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee08e24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/bert_linear.1e-06.12121051\n"
     ]
    }
   ],
   "source": [
    "save_model(model, './results/bert_linear.{}'.format(1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff73bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertLinear\n",
    "model = BertLinear(args).to(device)\n",
    "train(model, epochs = 100, lr = 1e-6)\n",
    "evaluate(model, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae84e7",
   "metadata": {},
   "source": [
    "### BertMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f5c95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'path': '/data/pretrained/bert-base-chinese/',\n",
    "    'bert_out_dim': 768,\n",
    "    'n_class': 9, \n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(args['bert_out_dim'], 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, args['n_class']),\n",
    "        )\n",
    "        \n",
    "    def forward(self, tokens, masks, labels = None):\n",
    "        bert_out = self.bert(tokens, masks)['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        return cls_out\n",
    "\n",
    "    def predict(self, tokens, masks):\n",
    "        cls_out = self.forward(tokens, masks)\n",
    "        pred = torch.max(cls_out, dim = 2).indices\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "333deb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertMLP model with lr = 1e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231216 23:08:42] [Test] Precision: 0.005334 Recall: 0.003220 F1: 0.004016\n",
      "[20231216 23:08:42] [epoch 1] TLoss: 2.208 VLoss: 2.095 TAcc: 0.093 VAcc: 0.469\n",
      "[20231216 23:09:41] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:09:41] [epoch 2] TLoss: 1.995 VLoss: 1.756 TAcc: 0.438 VAcc: 0.646\n",
      "[20231216 23:10:42] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:10:42] [epoch 3] TLoss: 1.686 VLoss: 1.423 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:11:43] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:11:43] [epoch 4] TLoss: 1.476 VLoss: 1.241 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:12:44] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:12:44] [epoch 5] TLoss: 1.355 VLoss: 1.134 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:13:46] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:13:46] [epoch 6] TLoss: 1.276 VLoss: 1.065 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:14:53] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:14:53] [epoch 7] TLoss: 1.217 VLoss: 1.016 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:15:58] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:15:58] [epoch 8] TLoss: 1.167 VLoss: 0.977 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:17:05] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:17:05] [epoch 9] TLoss: 1.120 VLoss: 0.942 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:18:11] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:18:11] [epoch 10] TLoss: 1.077 VLoss: 0.910 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:19:18] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:19:18] [epoch 11] TLoss: 1.036 VLoss: 0.882 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:20:26] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:20:26] [epoch 12] TLoss: 0.998 VLoss: 0.857 TAcc: 0.511 VAcc: 0.646\n",
      "[20231216 23:21:31] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:21:31] [epoch 13] TLoss: 0.964 VLoss: 0.835 TAcc: 0.511 VAcc: 0.649\n",
      "[20231216 23:22:33] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:22:33] [epoch 14] TLoss: 0.932 VLoss: 0.816 TAcc: 0.514 VAcc: 0.648\n",
      "[20231216 23:23:33] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:23:33] [epoch 15] TLoss: 0.904 VLoss: 0.799 TAcc: 0.524 VAcc: 0.634\n",
      "[20231216 23:24:33] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:24:33] [epoch 16] TLoss: 0.878 VLoss: 0.783 TAcc: 0.537 VAcc: 0.621\n",
      "[20231216 23:25:33] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:25:33] [epoch 17] TLoss: 0.855 VLoss: 0.769 TAcc: 0.548 VAcc: 0.608\n",
      "[20231216 23:26:33] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231216 23:26:33] [epoch 18] TLoss: 0.834 VLoss: 0.754 TAcc: 0.557 VAcc: 0.600\n",
      "[20231216 23:26:33] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertMLP.1e-07.12162326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231216 23:26:39] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BertLinear\n",
    "model = BertMLP(args).to(device)\n",
    "train(model, epochs = 100, lr = 1e-7)\n",
    "evaluate(model, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd94e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[20231212 14:33:05] [Test] Precision: 0.001556 Recall: 0.000758 F1: 0.001019\n",
      "[20231212 14:33:05] [epoch 1] TLoss: 2.257 VLoss: 2.126 TAcc: 0.048 VAcc: 0.222\n",
      "[20231212 14:34:23] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:34:23] [epoch 2] TLoss: 2.054 VLoss: 1.823 TAcc: 0.305 VAcc: 0.646\n",
      "[20231212 14:35:49] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:35:49] [epoch 3] TLoss: 1.762 VLoss: 1.507 TAcc: 0.510 VAcc: 0.646\n",
      "[20231212 14:37:12] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:37:12] [epoch 4] TLoss: 1.544 VLoss: 1.311 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:38:43] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:38:43] [epoch 5] TLoss: 1.415 VLoss: 1.200 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:40:03] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:40:03] [epoch 6] TLoss: 1.329 VLoss: 1.124 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:41:20] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:41:20] [epoch 7] TLoss: 1.266 VLoss: 1.071 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:42:28] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:42:28] [epoch 8] TLoss: 1.216 VLoss: 1.031 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:43:45] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:43:45] [epoch 9] TLoss: 1.170 VLoss: 0.996 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:45:06] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:45:06] [epoch 10] TLoss: 1.126 VLoss: 0.962 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:46:31] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:46:31] [epoch 11] TLoss: 1.082 VLoss: 0.931 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:47:56] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:47:56] [epoch 12] TLoss: 1.041 VLoss: 0.901 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:49:20] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:49:20] [epoch 13] TLoss: 1.002 VLoss: 0.875 TAcc: 0.511 VAcc: 0.646\n",
      "[20231212 14:50:37] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:50:37] [epoch 14] TLoss: 0.968 VLoss: 0.853 TAcc: 0.513 VAcc: 0.646\n",
      "[20231212 14:52:04] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:52:04] [epoch 15] TLoss: 0.938 VLoss: 0.834 TAcc: 0.520 VAcc: 0.635\n",
      "[20231212 14:53:28] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:53:28] [epoch 16] TLoss: 0.910 VLoss: 0.818 TAcc: 0.534 VAcc: 0.621\n",
      "[20231212 14:54:47] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:54:47] [epoch 17] TLoss: 0.885 VLoss: 0.804 TAcc: 0.546 VAcc: 0.608\n",
      "[20231212 14:55:59] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:55:59] [epoch 18] TLoss: 0.863 VLoss: 0.791 TAcc: 0.555 VAcc: 0.597\n",
      "[20231212 14:57:12] [Test] Precision: 0.000000 Recall: 0.000000 F1: 0.000000\n",
      "[20231212 14:57:12] [epoch 19] TLoss: 0.843 VLoss: 0.780 TAcc: 0.560 VAcc: 0.588\n",
      "[20231212 14:57:12] Early stopping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m BertMLP(args)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m evaluate(model, test_loader, tag_list)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m     51\u001b[0m         logger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43msave_model\u001b[49m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, lr))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = BertMLP(args).to(device)\n",
    "train(model, epochs = 100, lr = 1e-7)\n",
    "evaluate(model, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b273b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f7c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c597466e",
   "metadata": {},
   "source": [
    "### BertCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fa4d076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T05:44:28.371018Z",
     "start_time": "2022-04-21T05:44:25.962020Z"
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'path': '/data/pretrained/bert-base-chinese/',\n",
    "    'bert_out_dim': 768,\n",
    "    'n_class': 9, \n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "class BertCRF(nn.Module):\n",
    "    def __init__(self, args, tag_dict):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.cls = nn.Linear(args['bert_out_dim'], args['n_class'])\n",
    "        self.crf = CRF({tag_dict[tag]: tag for tag in tag_dict})\n",
    "        \n",
    "    def forward(self, tokens, masks, labels):\n",
    "        bert_out = self.bert(tokens)['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        log_likelihood = self.crf(cls_out, labels, masks)\n",
    "        n_batch = labels.shape[0]\n",
    "        loss = -log_likelihood / n_batch\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, tokens, masks):\n",
    "        bert_out = self.bert(tokens)['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        predict = self.crf.viterbi_tags(cls_out, masks)\n",
    "        return predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c59372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRF model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231216 23:39:03] [Test] Precision: 0.473161 Recall: 0.387384 F1: 0.425997\n",
      "[20231216 23:39:03] [epoch 1] TLoss: 38.404 VLoss: 27.757 TAcc: 0.535 VAcc: 0.717\n",
      "[20231216 23:40:54] [Test] Precision: 0.487606 Recall: 0.555219 F1: 0.519221\n",
      "[20231216 23:40:54] [epoch 2] TLoss: 27.129 VLoss: 25.005 TAcc: 0.684 VAcc: 0.713\n",
      "[20231216 23:42:46] [Test] Precision: 0.502083 Recall: 0.593484 F1: 0.543971\n",
      "[20231216 23:42:46] [epoch 3] TLoss: 23.824 VLoss: 24.169 TAcc: 0.717 VAcc: 0.717\n",
      "[20231216 23:44:37] [Test] Precision: 0.511860 Recall: 0.613184 F1: 0.557959\n",
      "[20231216 23:44:37] [epoch 4] TLoss: 22.174 VLoss: 24.033 TAcc: 0.732 VAcc: 0.718\n",
      "[20231216 23:46:30] [Test] Precision: 0.520691 Recall: 0.605418 F1: 0.559867\n",
      "[20231216 23:46:30] [epoch 5] TLoss: 21.126 VLoss: 23.763 TAcc: 0.744 VAcc: 0.720\n",
      "[20231216 23:48:23] [Test] Precision: 0.525847 Recall: 0.626255 F1: 0.571676\n",
      "[20231216 23:48:23] [epoch 6] TLoss: 20.333 VLoss: 23.620 TAcc: 0.751 VAcc: 0.723\n",
      "[20231216 23:50:17] [Test] Precision: 0.528587 Recall: 0.635726 F1: 0.577227\n",
      "[20231216 23:50:17] [epoch 7] TLoss: 19.772 VLoss: 23.716 TAcc: 0.757 VAcc: 0.722\n",
      "[20231216 23:52:08] [Test] Precision: 0.532017 Recall: 0.648418 F1: 0.584479\n",
      "[20231216 23:52:08] [epoch 8] TLoss: 19.299 VLoss: 23.730 TAcc: 0.764 VAcc: 0.720\n",
      "[20231216 23:54:00] [Test] Precision: 0.540618 Recall: 0.610153 F1: 0.573285\n",
      "[20231216 23:54:00] [epoch 9] TLoss: 18.835 VLoss: 23.516 TAcc: 0.768 VAcc: 0.721\n",
      "[20231216 23:55:51] [Test] Precision: 0.542447 Recall: 0.614889 F1: 0.576401\n",
      "[20231216 23:55:51] [epoch 10] TLoss: 18.357 VLoss: 23.715 TAcc: 0.775 VAcc: 0.720\n",
      "[20231216 23:57:43] [Test] Precision: 0.545182 Recall: 0.619435 F1: 0.579941\n",
      "[20231216 23:57:43] [epoch 11] TLoss: 18.040 VLoss: 23.641 TAcc: 0.779 VAcc: 0.723\n",
      "[20231216 23:59:36] [Test] Precision: 0.543043 Recall: 0.616594 F1: 0.577486\n",
      "[20231216 23:59:36] [epoch 12] TLoss: 17.658 VLoss: 23.562 TAcc: 0.784 VAcc: 0.723\n",
      "[20231217 00:01:30] [Test] Precision: 0.548655 Recall: 0.625876 F1: 0.584727\n",
      "[20231217 00:01:30] [epoch 13] TLoss: 17.288 VLoss: 23.446 TAcc: 0.790 VAcc: 0.724\n",
      "[20231217 00:03:24] [Test] Precision: 0.544318 Recall: 0.635158 F1: 0.586240\n",
      "[20231217 00:03:24] [epoch 14] TLoss: 16.877 VLoss: 23.824 TAcc: 0.794 VAcc: 0.721\n",
      "[20231217 00:05:15] [Test] Precision: 0.547787 Recall: 0.630801 F1: 0.586371\n",
      "[20231217 00:05:15] [epoch 15] TLoss: 16.434 VLoss: 23.154 TAcc: 0.799 VAcc: 0.727\n",
      "[20231217 00:07:09] [Test] Precision: 0.543277 Recall: 0.644440 F1: 0.589550\n",
      "[20231217 00:07:09] [epoch 16] TLoss: 16.100 VLoss: 23.882 TAcc: 0.803 VAcc: 0.722\n",
      "[20231217 00:09:00] [Test] Precision: 0.539642 Recall: 0.645956 F1: 0.588032\n",
      "[20231217 00:09:00] [epoch 17] TLoss: 15.825 VLoss: 24.509 TAcc: 0.807 VAcc: 0.717\n",
      "[20231217 00:10:53] [Test] Precision: 0.542692 Recall: 0.656185 F1: 0.594066\n",
      "[20231217 00:10:53] [epoch 18] TLoss: 15.308 VLoss: 24.315 TAcc: 0.813 VAcc: 0.722\n",
      "[20231217 00:12:44] [Test] Precision: 0.549611 Recall: 0.655806 F1: 0.598031\n",
      "[20231217 00:12:44] [epoch 19] TLoss: 14.990 VLoss: 23.441 TAcc: 0.821 VAcc: 0.730\n",
      "[20231217 00:14:38] [Test] Precision: 0.542563 Recall: 0.659216 F1: 0.595228\n",
      "[20231217 00:14:38] [epoch 20] TLoss: 14.562 VLoss: 24.611 TAcc: 0.822 VAcc: 0.722\n",
      "[20231217 00:16:29] [Test] Precision: 0.541035 Recall: 0.655617 F1: 0.592840\n",
      "[20231217 00:16:29] [epoch 21] TLoss: 14.315 VLoss: 25.031 TAcc: 0.826 VAcc: 0.721\n"
     ]
    }
   ],
   "source": [
    "model_crf = BertCRF(args, tag_dict).to(device)\n",
    "train_crf(model_crf, epochs = 100, lr = 1e-6)\n",
    "evaluate(model_crf, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8266694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[20231217 09:01:28] [Test] Precision: 0.560790 Recall: 0.556360 F1: 0.558566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5607895173327979, 0.5563600460752746, 0.5585659998303302)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf = BertCRF(args, tag_dict).to(device)\n",
    "model_crf.load_state_dict(torch.load('./results/BertCRF.1e-06.12170022'))\n",
    "evaluate(model_crf, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37c29259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[20231217 09:01:43] [Test] Precision: 0.560538 Recall: 0.431957 F1: 0.487918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5605381165918085, 0.43195655751185913, 0.48791821512168426)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_crf2 = BertCRF(args, tag_dict).to(device)\n",
    "# train_crf(model_crf2, epochs = 100, lr = 1e-7)\n",
    "# evaluate(model_crf2, test_loader, tag_list)\n",
    "model_crf2 = BertCRF(args, tag_dict).to(device)\n",
    "model_crf2.load_state_dict(torch.load('./results/BertCRF.1e-07.12170059'))\n",
    "evaluate(model_crf2, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3cb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38fadac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRF model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 09:06:06] [Test] Precision: 0.269629 Recall: 0.187346 F1: 0.221080\n",
      "[20231217 09:06:06] [epoch 1] TLoss: 30.753 VLoss: 22.185 TAcc: 0.545 VAcc: 0.657\n",
      "[20231217 09:08:01] [Test] Precision: 0.492911 Recall: 0.592726 F1: 0.538230\n",
      "[20231217 09:08:01] [epoch 2] TLoss: 20.251 VLoss: 19.265 TAcc: 0.681 VAcc: 0.705\n",
      "[20231217 09:09:52] [Test] Precision: 0.504821 Recall: 0.634779 F1: 0.562390\n",
      "[20231217 09:09:52] [epoch 3] TLoss: 17.375 VLoss: 18.661 TAcc: 0.717 VAcc: 0.703\n",
      "[20231217 09:11:44] [Test] Precision: 0.524244 Recall: 0.653343 F1: 0.581717\n",
      "[20231217 09:11:44] [epoch 4] TLoss: 15.903 VLoss: 17.999 TAcc: 0.736 VAcc: 0.715\n",
      "[20231217 09:13:39] [Test] Precision: 0.529870 Recall: 0.656943 F1: 0.586604\n",
      "[20231217 09:13:39] [epoch 5] TLoss: 15.144 VLoss: 17.868 TAcc: 0.745 VAcc: 0.713\n",
      "[20231217 09:15:32] [Test] Precision: 0.535856 Recall: 0.665278 F1: 0.593594\n",
      "[20231217 09:15:32] [epoch 6] TLoss: 14.659 VLoss: 17.694 TAcc: 0.751 VAcc: 0.713\n",
      "[20231217 09:17:24] [Test] Precision: 0.534519 Recall: 0.665846 F1: 0.592999\n",
      "[20231217 09:17:24] [epoch 7] TLoss: 14.277 VLoss: 17.764 TAcc: 0.758 VAcc: 0.709\n",
      "[20231217 09:19:16] [Test] Precision: 0.536225 Recall: 0.660352 F1: 0.591851\n",
      "[20231217 09:19:16] [epoch 8] TLoss: 13.909 VLoss: 17.401 TAcc: 0.763 VAcc: 0.714\n",
      "[20231217 09:21:08] [Test] Precision: 0.534548 Recall: 0.666793 F1: 0.593392\n",
      "[20231217 09:21:08] [epoch 9] TLoss: 13.605 VLoss: 17.566 TAcc: 0.767 VAcc: 0.709\n",
      "[20231217 09:21:08] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRF.1e-06.12170921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 09:21:16] [Test] Precision: 0.562439 Recall: 0.564752 F1: 0.563593\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRF model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 09:23:10] [Test] Precision: 0.416093 Recall: 0.366357 F1: 0.389644\n",
      "[20231217 09:23:10] [epoch 1] TLoss: 34.723 VLoss: 25.458 TAcc: 0.555 VAcc: 0.695\n",
      "[20231217 09:25:04] [Test] Precision: 0.481137 Recall: 0.577382 F1: 0.524884\n",
      "[20231217 09:25:04] [epoch 2] TLoss: 24.535 VLoss: 22.862 TAcc: 0.682 VAcc: 0.708\n",
      "[20231217 09:26:57] [Test] Precision: 0.503340 Recall: 0.627960 F1: 0.558786\n",
      "[20231217 09:26:57] [epoch 3] TLoss: 21.594 VLoss: 21.971 TAcc: 0.714 VAcc: 0.716\n",
      "[20231217 09:28:51] [Test] Precision: 0.509851 Recall: 0.642167 F1: 0.568410\n",
      "[20231217 09:28:51] [epoch 4] TLoss: 20.117 VLoss: 21.616 TAcc: 0.731 VAcc: 0.717\n",
      "[20231217 09:30:47] [Test] Precision: 0.518418 Recall: 0.650502 F1: 0.576997\n",
      "[20231217 09:30:47] [epoch 5] TLoss: 19.194 VLoss: 21.157 TAcc: 0.743 VAcc: 0.721\n",
      "[20231217 09:32:41] [Test] Precision: 0.527511 Recall: 0.633832 F1: 0.575805\n",
      "[20231217 09:32:41] [epoch 6] TLoss: 18.534 VLoss: 21.016 TAcc: 0.751 VAcc: 0.721\n",
      "[20231217 09:34:36] [Test] Precision: 0.529847 Recall: 0.637242 F1: 0.578603\n",
      "[20231217 09:34:36] [epoch 7] TLoss: 18.028 VLoss: 21.079 TAcc: 0.759 VAcc: 0.720\n",
      "[20231217 09:36:29] [Test] Precision: 0.532352 Recall: 0.637431 F1: 0.580172\n",
      "[20231217 09:36:29] [epoch 8] TLoss: 17.576 VLoss: 20.935 TAcc: 0.765 VAcc: 0.722\n",
      "[20231217 09:38:23] [Test] Precision: 0.534515 Recall: 0.645387 F1: 0.584742\n",
      "[20231217 09:38:23] [epoch 9] TLoss: 17.166 VLoss: 20.998 TAcc: 0.767 VAcc: 0.722\n",
      "[20231217 09:40:15] [Test] Precision: 0.531813 Recall: 0.649176 F1: 0.584663\n",
      "[20231217 09:40:15] [epoch 10] TLoss: 16.783 VLoss: 21.335 TAcc: 0.774 VAcc: 0.719\n",
      "[20231217 09:42:08] [Test] Precision: 0.533125 Recall: 0.647850 F1: 0.584915\n",
      "[20231217 09:42:08] [epoch 11] TLoss: 16.443 VLoss: 21.164 TAcc: 0.780 VAcc: 0.720\n",
      "[20231217 09:43:59] [Test] Precision: 0.530957 Recall: 0.654669 F1: 0.586359\n",
      "[20231217 09:43:59] [epoch 12] TLoss: 16.049 VLoss: 21.337 TAcc: 0.784 VAcc: 0.719\n",
      "[20231217 09:45:51] [Test] Precision: 0.534530 Recall: 0.659784 F1: 0.590589\n",
      "[20231217 09:45:51] [epoch 13] TLoss: 15.805 VLoss: 20.925 TAcc: 0.789 VAcc: 0.722\n",
      "[20231217 09:47:44] [Test] Precision: 0.531795 Recall: 0.659026 F1: 0.588613\n",
      "[20231217 09:47:44] [epoch 14] TLoss: 15.449 VLoss: 21.427 TAcc: 0.794 VAcc: 0.719\n",
      "[20231217 09:49:36] [Test] Precision: 0.537323 Recall: 0.659973 F1: 0.592366\n",
      "[20231217 09:49:36] [epoch 15] TLoss: 15.079 VLoss: 20.983 TAcc: 0.799 VAcc: 0.725\n",
      "[20231217 09:51:30] [Test] Precision: 0.537196 Recall: 0.662057 F1: 0.593127\n",
      "[20231217 09:51:30] [epoch 16] TLoss: 14.824 VLoss: 21.030 TAcc: 0.803 VAcc: 0.726\n",
      "[20231217 09:53:24] [Test] Precision: 0.536108 Recall: 0.663762 F1: 0.593144\n",
      "[20231217 09:53:24] [epoch 17] TLoss: 14.579 VLoss: 21.350 TAcc: 0.808 VAcc: 0.722\n",
      "[20231217 09:55:18] [Test] Precision: 0.537467 Recall: 0.661678 F1: 0.593140\n",
      "[20231217 09:55:18] [epoch 18] TLoss: 14.170 VLoss: 21.364 TAcc: 0.811 VAcc: 0.723\n",
      "[20231217 09:57:10] [Test] Precision: 0.539219 Recall: 0.664141 F1: 0.595196\n",
      "[20231217 09:57:10] [epoch 19] TLoss: 13.889 VLoss: 21.420 TAcc: 0.814 VAcc: 0.724\n",
      "[20231217 09:59:02] [Test] Precision: 0.535228 Recall: 0.663383 F1: 0.592455\n",
      "[20231217 09:59:02] [epoch 20] TLoss: 13.539 VLoss: 22.249 TAcc: 0.821 VAcc: 0.719\n",
      "[20231217 10:00:56] [Test] Precision: 0.536362 Recall: 0.666414 F1: 0.594357\n",
      "[20231217 10:00:56] [epoch 21] TLoss: 13.253 VLoss: 22.529 TAcc: 0.826 VAcc: 0.718\n",
      "[20231217 10:00:56] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRF.1e-06.12171000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 10:01:05] [Test] Precision: 0.553979 Recall: 0.558993 F1: 0.556475\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRF model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 10:02:58] [Test] Precision: 0.487526 Recall: 0.307255 F1: 0.376946\n",
      "[20231217 10:02:58] [epoch 1] TLoss: 34.027 VLoss: 24.537 TAcc: 0.528 VAcc: 0.713\n",
      "[20231217 10:04:52] [Test] Precision: 0.492867 Recall: 0.595567 F1: 0.539372\n",
      "[20231217 10:04:52] [epoch 2] TLoss: 23.088 VLoss: 21.498 TAcc: 0.679 VAcc: 0.711\n",
      "[20231217 10:06:44] [Test] Precision: 0.507225 Recall: 0.645009 F1: 0.567879\n",
      "[20231217 10:06:44] [epoch 3] TLoss: 19.970 VLoss: 20.499 TAcc: 0.714 VAcc: 0.717\n",
      "[20231217 10:08:39] [Test] Precision: 0.521043 Recall: 0.666035 F1: 0.584684\n",
      "[20231217 10:08:39] [epoch 4] TLoss: 18.584 VLoss: 19.945 TAcc: 0.730 VAcc: 0.722\n",
      "[20231217 10:10:34] [Test] Precision: 0.526199 Recall: 0.673423 F1: 0.590777\n",
      "[20231217 10:10:34] [epoch 5] TLoss: 17.646 VLoss: 19.978 TAcc: 0.741 VAcc: 0.719\n",
      "[20231217 10:12:25] [Test] Precision: 0.530070 Recall: 0.672855 F1: 0.592988\n",
      "[20231217 10:12:25] [epoch 6] TLoss: 17.005 VLoss: 19.802 TAcc: 0.750 VAcc: 0.719\n",
      "[20231217 10:14:19] [Test] Precision: 0.530582 Recall: 0.672097 F1: 0.593014\n",
      "[20231217 10:14:19] [epoch 7] TLoss: 16.568 VLoss: 19.916 TAcc: 0.755 VAcc: 0.718\n",
      "[20231217 10:16:11] [Test] Precision: 0.531386 Recall: 0.671908 F1: 0.593442\n",
      "[20231217 10:16:11] [epoch 8] TLoss: 16.141 VLoss: 19.901 TAcc: 0.758 VAcc: 0.717\n",
      "[20231217 10:18:03] [Test] Precision: 0.535628 Recall: 0.674938 F1: 0.597268\n",
      "[20231217 10:18:03] [epoch 9] TLoss: 15.853 VLoss: 19.808 TAcc: 0.765 VAcc: 0.717\n",
      "[20231217 10:18:03] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRF.1e-06.12171018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 10:18:12] [Test] Precision: 0.565120 Recall: 0.579069 F1: 0.572009\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRF model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 10:20:05] [Test] Precision: 0.183130 Recall: 0.122561 F1: 0.146845\n",
      "[20231217 10:20:05] [epoch 1] TLoss: 36.629 VLoss: 27.967 TAcc: 0.519 VAcc: 0.641\n",
      "[20231217 10:21:59] [Test] Precision: 0.472727 Recall: 0.517143 F1: 0.493939\n",
      "[20231217 10:21:59] [epoch 2] TLoss: 25.149 VLoss: 23.408 TAcc: 0.663 VAcc: 0.697\n",
      "[20231217 10:23:53] [Test] Precision: 0.504976 Recall: 0.605607 F1: 0.550732\n",
      "[20231217 10:23:53] [epoch 3] TLoss: 21.201 VLoss: 22.166 TAcc: 0.714 VAcc: 0.712\n",
      "[20231217 10:25:46] [Test] Precision: 0.512605 Recall: 0.635537 F1: 0.567490\n",
      "[20231217 10:25:46] [epoch 4] TLoss: 19.556 VLoss: 21.771 TAcc: 0.730 VAcc: 0.712\n",
      "[20231217 10:27:39] [Test] Precision: 0.522219 Recall: 0.645577 F1: 0.577382\n",
      "[20231217 10:27:39] [epoch 5] TLoss: 18.569 VLoss: 21.292 TAcc: 0.742 VAcc: 0.719\n",
      "[20231217 10:29:34] [Test] Precision: 0.521306 Recall: 0.653533 F1: 0.579978\n",
      "[20231217 10:29:34] [epoch 6] TLoss: 17.893 VLoss: 21.318 TAcc: 0.751 VAcc: 0.715\n",
      "[20231217 10:31:27] [Test] Precision: 0.525809 Recall: 0.652207 F1: 0.582227\n",
      "[20231217 10:31:27] [epoch 7] TLoss: 17.425 VLoss: 21.115 TAcc: 0.755 VAcc: 0.714\n",
      "[20231217 10:33:19] [Test] Precision: 0.526001 Recall: 0.649555 F1: 0.581285\n",
      "[20231217 10:33:19] [epoch 8] TLoss: 16.874 VLoss: 21.193 TAcc: 0.763 VAcc: 0.712\n",
      "[20231217 10:35:13] [Test] Precision: 0.529357 Recall: 0.648987 F1: 0.583099\n",
      "[20231217 10:35:13] [epoch 9] TLoss: 16.493 VLoss: 20.993 TAcc: 0.769 VAcc: 0.716\n",
      "[20231217 10:37:05] [Test] Precision: 0.532439 Recall: 0.646713 F1: 0.584039\n",
      "[20231217 10:37:05] [epoch 10] TLoss: 16.089 VLoss: 21.053 TAcc: 0.773 VAcc: 0.716\n",
      "[20231217 10:37:05] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRF.1e-06.12171037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 10:37:13] [Test] Precision: 0.564120 Recall: 0.551588 F1: 0.557784\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRF model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 10:39:06] [Test] Precision: 0.330898 Recall: 0.240197 F1: 0.278345\n",
      "[20231217 10:39:06] [epoch 1] TLoss: 34.618 VLoss: 24.280 TAcc: 0.542 VAcc: 0.685\n",
      "[20231217 10:41:02] [Test] Precision: 0.489507 Recall: 0.569994 F1: 0.526694\n",
      "[20231217 10:41:02] [epoch 2] TLoss: 24.361 VLoss: 21.827 TAcc: 0.674 VAcc: 0.718\n",
      "[20231217 10:42:55] [Test] Precision: 0.507439 Recall: 0.613753 F1: 0.555556\n",
      "[20231217 10:42:55] [epoch 3] TLoss: 21.248 VLoss: 21.135 TAcc: 0.714 VAcc: 0.719\n",
      "[20231217 10:44:48] [Test] Precision: 0.520343 Recall: 0.644440 F1: 0.575781\n",
      "[20231217 10:44:48] [epoch 4] TLoss: 19.739 VLoss: 20.558 TAcc: 0.731 VAcc: 0.720\n",
      "[20231217 10:46:41] [Test] Precision: 0.525260 Recall: 0.659784 F1: 0.584887\n",
      "[20231217 10:46:41] [epoch 5] TLoss: 18.723 VLoss: 20.040 TAcc: 0.743 VAcc: 0.721\n",
      "[20231217 10:48:34] [Test] Precision: 0.531663 Recall: 0.663194 F1: 0.590189\n",
      "[20231217 10:48:34] [epoch 6] TLoss: 17.923 VLoss: 19.885 TAcc: 0.751 VAcc: 0.721\n",
      "[20231217 10:50:27] [Test] Precision: 0.532393 Recall: 0.664709 F1: 0.591238\n",
      "[20231217 10:50:27] [epoch 7] TLoss: 17.422 VLoss: 19.948 TAcc: 0.757 VAcc: 0.719\n",
      "[20231217 10:52:22] [Test] Precision: 0.531043 Recall: 0.667551 F1: 0.591523\n",
      "[20231217 10:52:22] [epoch 8] TLoss: 16.955 VLoss: 20.179 TAcc: 0.765 VAcc: 0.715\n",
      "[20231217 10:54:16] [Test] Precision: 0.537726 Recall: 0.660163 F1: 0.592687\n",
      "[20231217 10:54:16] [epoch 9] TLoss: 16.614 VLoss: 19.736 TAcc: 0.769 VAcc: 0.722\n",
      "[20231217 10:56:11] [Test] Precision: 0.534077 Recall: 0.662057 F1: 0.591221\n",
      "[20231217 10:56:11] [epoch 10] TLoss: 16.265 VLoss: 20.286 TAcc: 0.774 VAcc: 0.713\n",
      "[20231217 10:58:03] [Test] Precision: 0.538485 Recall: 0.663952 F1: 0.594673\n",
      "[20231217 10:58:03] [epoch 11] TLoss: 15.959 VLoss: 20.149 TAcc: 0.777 VAcc: 0.717\n",
      "[20231217 10:59:54] [Test] Precision: 0.538132 Recall: 0.665656 F1: 0.595139\n",
      "[20231217 10:59:54] [epoch 12] TLoss: 15.610 VLoss: 20.159 TAcc: 0.782 VAcc: 0.716\n",
      "[20231217 11:01:47] [Test] Precision: 0.538170 Recall: 0.671718 F1: 0.597573\n",
      "[20231217 11:01:47] [epoch 13] TLoss: 15.256 VLoss: 20.177 TAcc: 0.788 VAcc: 0.717\n",
      "[20231217 11:03:39] [Test] Precision: 0.538662 Recall: 0.666414 F1: 0.595766\n",
      "[20231217 11:03:39] [epoch 14] TLoss: 14.967 VLoss: 20.248 TAcc: 0.793 VAcc: 0.717\n",
      "[20231217 11:03:39] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRF.1e-06.12171103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 11:03:47] [Test] Precision: 0.556720 Recall: 0.559651 F1: 0.558182\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model_crf = BertCRF(args, tag_dict).to(device)\n",
    "    train_crf(model_crf, epochs = 100, lr = 1e-6)\n",
    "    evaluate(model_crf, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6731d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e181a9df",
   "metadata": {},
   "source": [
    "### Entity Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JDComment_seg.code_kg.kg_embed import getAllEntityTriplet, getEntity, getTriplet, loadBertEmbed, getTopEntityTriplet\n",
    "from_file = '/home/gene/Documents/Senti/Comment/knowledgebase/unlabel/deduplicate/data.txt'\n",
    "\n",
    "# targets, opinions\n",
    "target_ids, opinion_ids, target_list, opinion_list = getEntity(from_file)\n",
    "# target->opinions, opinion->targets\n",
    "target_triplet_dict, opinion_triplet_dict = getAllEntityTriplet()\n",
    "# triplets\n",
    "triplet_dict = getTriplet(from_file) # triplet_dict: 148600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def seg_words(text):\n",
    "    seg = list(jieba.cut(text))\n",
    "    return seg\n",
    "\n",
    "def match_entities(text, target_list):\n",
    "    matched_entities = []\n",
    "    words = seg_words(text).split()\n",
    "    matched_entities = [word for word in words if word in target_list]\n",
    "    for word in words:\n",
    "        if word in target_list:\n",
    "            matched_entities.append(word)\n",
    "    return matched_entities\n",
    "\n",
    "def match_triplets(text, target_list, opinion_list):\n",
    "    matched_triplets = []\n",
    "    words = seg_words(text).split()\n",
    "    for word in words:\n",
    "        if word in target_list:\n",
    "            matched_triplets.append(target_triplet_dict[word])\n",
    "        elif word in opinion_list:\n",
    "            matched_triplets.append(opinion_triplet_dict[word])\n",
    "    return matched_triplets\n",
    "\n",
    "text = \"I bought a new iPhone yesterday.\"\n",
    "matched_entities = match_entities(text, target_list)\n",
    "print(matched_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9cb28",
   "metadata": {},
   "source": [
    "### BertEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc9a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get entity from data_file: /home/gene/Documents/Senti/Comment/knowledgebase/unlabel/deduplicate/data.txt, target_ids: 12122, opinion_ids: 34028\n",
      "Get entity triplet from data_file: /home/gene/Documents/Senti/Comment/knowledgebase/unlabel/deduplicate/data.txt, target_dict: 12122, opinion_dict: 34028\n",
      "Get entity top triplets, target_dict: /home/gene/Documents/Senti/Comment/knowledgebase/unlabel/deduplicate/data.txt, opinion_dict: 12122\n"
     ]
    }
   ],
   "source": [
    "from JDComment_seg.code_kg.kg_embed import getAllEntityTriplet, getEntity, loadBertEmbed, getTopEntityTriplet\n",
    "from_file = '/home/gene/Documents/Senti/Comment/knowledgebase/unlabel/deduplicate/data.txt'\n",
    "\n",
    "# targets, opinions\n",
    "target_ids, opinion_ids, target_list, opinion_list = getEntity(from_file)\n",
    "# target->opinions, opinion->targets\n",
    "target_triplet_dict, opinion_triplet_dict = getAllEntityTriplet()\n",
    "# TransE embedding\n",
    "enhanced_target_embed, enhanced_opinion_embed, target_list, opinion_list = loadBertEmbed()\n",
    "target_top_triplet_dict, opinion_top_triplet_dict = getTopEntityTriplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5811738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def seg_words(text):\n",
    "    seg = list(jieba.cut(text))\n",
    "    return seg\n",
    "\n",
    "# Use Enhanced Embedding\n",
    "# 对于每个target，使用Top tail entity进行加权, [entity_emb; avg(tail_emb)]\n",
    "def embed_text(text, max_len = 40):\n",
    "    words = seg_words(text)\n",
    "    # embeds = torch.zeros((len(text), 768 * 2), dtype = torch.float32) # sen_len, emb_size\n",
    "    embeds = torch.zeros((max_len, 768 * 2), dtype = torch.float32) # sen_len, emb_size\n",
    "    pos = 1 # [CLS]\n",
    "    for word in words:\n",
    "        if word in target_ids:\n",
    "            embeds[pos: pos + len(word)] += enhanced_target_embed[target_ids[word]]\n",
    "        if word in opinion_ids:\n",
    "            embeds[pos: pos + len(word)] += enhanced_opinion_embed[opinion_ids[word]]\n",
    "        pos += len(word)\n",
    "    return embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b36fe517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 对于数据集中的所有输入文本做预处理，提前计算固定的entity embedding\n",
    "# for item in tagged_dataset:\n",
    "#     text = item['text']\n",
    "#     embed = embed_text(text)\n",
    "#     item['entity_embeds'] = embed.numpy()\n",
    "\n",
    "# def collate_ent(batch):\n",
    "#     tokens = tokenizer([item['text'] for item in batch], \n",
    "#                        padding = 'max_length', truncation = True, max_length = 40, return_tensors = 'pt')\n",
    "#     labels = torch.tensor([item['labels'] for item in batch], dtype = torch.long, device = device)\n",
    "#     entity_embeds = torch.tensor([item['entity_embeds'] for item in batch], dtype = torch.float, device = device)\n",
    "#     return {\n",
    "#         'tokens': tokens['input_ids'].to(device),\n",
    "#         'masks': tokens['attention_mask'].to(device),\n",
    "#         'labels': labels,\n",
    "#         'texts': [item['text'] for item in batch],\n",
    "#         'entity_embeds': entity_embeds,\n",
    "#     }\n",
    "\n",
    "# n_train, n_dev = int(0.6 * len(tagged_dataset)), int(0.2 * len(tagged_dataset))\n",
    "# batch_size = 8\n",
    "# train_loader_ent = DataLoader(tagged_dataset[:n_train], batch_size = batch_size, collate_fn = collate_ent)\n",
    "# valid_loader_ent = DataLoader(tagged_dataset[n_train: n_train + n_dev], batch_size = batch_size, collate_fn = collate_ent)\n",
    "# test_loader_ent = DataLoader(tagged_dataset[n_train + n_dev:], batch_size = batch_size, collate_fn = collate_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da5f9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def seg_words(text):\n",
    "    seg = list(jieba.cut(text))\n",
    "    return seg\n",
    "\n",
    "# 对于每个target，同时使用Top tail entity进行加权, [entity_emb; avg(tail_emb)]\n",
    "def embed_bert_text(text, max_len = 40):\n",
    "    words = seg_words(text)\n",
    "    embeds = torch.zeros((max_len, 768 * 2), dtype = torch.float32) # sen_len, emb_size\n",
    "    pos = 1 # [CLS]\n",
    "    for word in words:\n",
    "        if word in target_ids:\n",
    "            embeds[pos: pos + len(word)] += enhanced_target_embed[target_ids[word]]\n",
    "        if word in opinion_ids:\n",
    "            embeds[pos: pos + len(word)] += enhanced_opinion_embed[opinion_ids[word]]\n",
    "        pos += len(word)\n",
    "    return embeds\n",
    "\n",
    "class BertEntity(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.cls = nn.Linear(args['bert_out_dim'] * 3, args['n_class'])\n",
    "    \n",
    "    def ent_layer(self, texts, bert_out):\n",
    "        ent_embeds = [embed_bert_text(text, max_len = bert_out.shape[1]) for text in texts]\n",
    "        ent_embeds = torch.stack(ent_embeds)\n",
    "        return ent_embeds.cuda()\n",
    "    \n",
    "    def forward(self, **batch):\n",
    "        bert_out = self.bert(batch['tokens'], batch['masks'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        ent_embeds = self.ent_layer(batch['texts'], bert_out)\n",
    "        bert_out = torch.cat((bert_out, ent_embeds), dim = -1)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        return cls_out\n",
    "\n",
    "    def predict(self, **batch):\n",
    "        cls_out = self.forward(**batch)\n",
    "        # cls_out = self.forward({'tokens': tokens, 'masks': masks})\n",
    "        pred = torch.max(cls_out, dim = 2).indices\n",
    "        return pred\n",
    "\n",
    "# model_ent = BertEntity(args).to(device)\n",
    "# model_ent.predict(tokens = 1, masks = 2, texts = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "983dea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertEntity model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 15:08:37] [Test] Precision: 0.002572 Recall: 0.000947 F1: 0.001384\n",
      "[20231217 15:08:37] [epoch 1] TLoss: 46.004 VLoss: 30.462 TAcc: 0.402 VAcc: 0.480\n",
      "[20231217 15:09:40] [Test] Precision: 0.049020 Recall: 0.023679 F1: 0.031933\n",
      "[20231217 15:09:40] [epoch 2] TLoss: 10.692 VLoss: 4.877 TAcc: 0.467 VAcc: 0.510\n",
      "[20231217 15:10:43] [Test] Precision: 0.183801 Recall: 0.150028 F1: 0.165207\n",
      "[20231217 15:10:43] [epoch 3] TLoss: 3.415 VLoss: 3.003 TAcc: 0.540 VAcc: 0.556\n",
      "[20231217 15:11:47] [Test] Precision: 0.236123 Recall: 0.209509 F1: 0.222021\n",
      "[20231217 15:11:47] [epoch 4] TLoss: 2.311 VLoss: 2.342 TAcc: 0.594 VAcc: 0.597\n",
      "[20231217 15:12:50] [Test] Precision: 0.275250 Recall: 0.261224 F1: 0.268053\n",
      "[20231217 15:12:50] [epoch 5] TLoss: 1.840 VLoss: 1.892 TAcc: 0.622 VAcc: 0.617\n",
      "[20231217 15:13:53] [Test] Precision: 0.365740 Recall: 0.386247 F1: 0.375714\n",
      "[20231217 15:13:53] [epoch 6] TLoss: 1.520 VLoss: 1.669 TAcc: 0.645 VAcc: 0.644\n",
      "[20231217 15:14:59] [Test] Precision: 0.407740 Recall: 0.439098 F1: 0.422838\n",
      "[20231217 15:14:59] [epoch 7] TLoss: 1.358 VLoss: 1.574 TAcc: 0.664 VAcc: 0.655\n",
      "[20231217 15:16:03] [Test] Precision: 0.411321 Recall: 0.454253 F1: 0.431722\n",
      "[20231217 15:16:03] [epoch 8] TLoss: 1.261 VLoss: 1.521 TAcc: 0.675 VAcc: 0.654\n",
      "[20231217 15:17:05] [Test] Precision: 0.423057 Recall: 0.469218 F1: 0.444943\n",
      "[20231217 15:17:05] [epoch 9] TLoss: 1.180 VLoss: 1.445 TAcc: 0.682 VAcc: 0.658\n",
      "[20231217 15:18:10] [Test] Precision: 0.421512 Recall: 0.466944 F1: 0.443066\n",
      "[20231217 15:18:10] [epoch 10] TLoss: 1.103 VLoss: 1.380 TAcc: 0.691 VAcc: 0.661\n",
      "[20231217 15:19:13] [Test] Precision: 0.424258 Recall: 0.473764 F1: 0.447646\n",
      "[20231217 15:19:13] [epoch 11] TLoss: 1.039 VLoss: 1.323 TAcc: 0.697 VAcc: 0.665\n",
      "[20231217 15:20:17] [Test] Precision: 0.423908 Recall: 0.479636 F1: 0.450053\n",
      "[20231217 15:20:17] [epoch 12] TLoss: 0.980 VLoss: 1.294 TAcc: 0.706 VAcc: 0.663\n",
      "[20231217 15:21:18] [Test] Precision: 0.436450 Recall: 0.487213 F1: 0.460437\n",
      "[20231217 15:21:18] [epoch 13] TLoss: 0.925 VLoss: 1.241 TAcc: 0.713 VAcc: 0.665\n",
      "[20231217 15:22:22] [Test] Precision: 0.446149 Recall: 0.501421 F1: 0.472173\n",
      "[20231217 15:22:22] [epoch 14] TLoss: 0.876 VLoss: 1.199 TAcc: 0.721 VAcc: 0.671\n",
      "[20231217 15:23:25] [Test] Precision: 0.448212 Recall: 0.505778 F1: 0.475258\n",
      "[20231217 15:23:25] [epoch 15] TLoss: 0.836 VLoss: 1.167 TAcc: 0.726 VAcc: 0.673\n",
      "[20231217 15:24:28] [Test] Precision: 0.444958 Recall: 0.510703 F1: 0.475569\n",
      "[20231217 15:24:28] [epoch 16] TLoss: 0.798 VLoss: 1.157 TAcc: 0.731 VAcc: 0.669\n",
      "[20231217 15:25:29] [Test] Precision: 0.450876 Recall: 0.512029 F1: 0.479510\n",
      "[20231217 15:25:29] [epoch 17] TLoss: 0.763 VLoss: 1.113 TAcc: 0.738 VAcc: 0.672\n",
      "[20231217 15:26:30] [Test] Precision: 0.452521 Recall: 0.511839 F1: 0.480356\n",
      "[20231217 15:26:30] [epoch 18] TLoss: 0.734 VLoss: 1.078 TAcc: 0.743 VAcc: 0.676\n",
      "[20231217 15:27:34] [Test] Precision: 0.452567 Recall: 0.516007 F1: 0.482209\n",
      "[20231217 15:27:34] [epoch 19] TLoss: 0.705 VLoss: 1.065 TAcc: 0.749 VAcc: 0.674\n",
      "[20231217 15:28:38] [Test] Precision: 0.459153 Recall: 0.521690 F1: 0.488428\n",
      "[20231217 15:28:38] [epoch 20] TLoss: 0.680 VLoss: 1.047 TAcc: 0.755 VAcc: 0.676\n",
      "[20231217 15:29:41] [Test] Precision: 0.463293 Recall: 0.527183 F1: 0.493177\n",
      "[20231217 15:29:41] [epoch 21] TLoss: 0.655 VLoss: 1.026 TAcc: 0.759 VAcc: 0.679\n",
      "[20231217 15:30:44] [Test] Precision: 0.460283 Recall: 0.523584 F1: 0.489897\n",
      "[20231217 15:30:44] [epoch 22] TLoss: 0.632 VLoss: 1.010 TAcc: 0.764 VAcc: 0.677\n",
      "[20231217 15:31:46] [Test] Precision: 0.454247 Recall: 0.523773 F1: 0.486539\n",
      "[20231217 15:31:46] [epoch 23] TLoss: 0.609 VLoss: 1.001 TAcc: 0.768 VAcc: 0.675\n",
      "[20231217 15:32:47] [Test] Precision: 0.463130 Recall: 0.534192 F1: 0.496129\n",
      "[20231217 15:32:47] [epoch 24] TLoss: 0.589 VLoss: 0.991 TAcc: 0.775 VAcc: 0.676\n",
      "[20231217 15:33:49] [Test] Precision: 0.471636 Recall: 0.546505 F1: 0.506318\n",
      "[20231217 15:33:49] [epoch 25] TLoss: 0.569 VLoss: 0.974 TAcc: 0.779 VAcc: 0.680\n",
      "[20231217 15:34:51] [Test] Precision: 0.473573 Recall: 0.551620 F1: 0.509625\n",
      "[20231217 15:34:51] [epoch 26] TLoss: 0.553 VLoss: 0.963 TAcc: 0.784 VAcc: 0.678\n",
      "[20231217 15:35:53] [Test] Precision: 0.475926 Recall: 0.552377 F1: 0.511310\n",
      "[20231217 15:35:53] [epoch 27] TLoss: 0.536 VLoss: 0.943 TAcc: 0.789 VAcc: 0.681\n",
      "[20231217 15:36:57] [Test] Precision: 0.471452 Recall: 0.553703 F1: 0.509278\n",
      "[20231217 15:36:57] [epoch 28] TLoss: 0.517 VLoss: 0.942 TAcc: 0.795 VAcc: 0.678\n",
      "[20231217 15:37:58] [Test] Precision: 0.463446 Recall: 0.541580 F1: 0.499476\n",
      "[20231217 15:37:58] [epoch 29] TLoss: 0.504 VLoss: 0.917 TAcc: 0.798 VAcc: 0.682\n",
      "[20231217 15:39:03] [Test] Precision: 0.473104 Recall: 0.553135 F1: 0.509999\n",
      "[20231217 15:39:03] [epoch 30] TLoss: 0.489 VLoss: 0.928 TAcc: 0.803 VAcc: 0.677\n",
      "[20231217 15:40:05] [Test] Precision: 0.472072 Recall: 0.545937 F1: 0.506325\n",
      "[20231217 15:40:05] [epoch 31] TLoss: 0.477 VLoss: 0.897 TAcc: 0.807 VAcc: 0.686\n",
      "[20231217 15:41:09] [Test] Precision: 0.464698 Recall: 0.557303 F1: 0.506804\n",
      "[20231217 15:41:09] [epoch 32] TLoss: 0.463 VLoss: 0.931 TAcc: 0.812 VAcc: 0.677\n",
      "[20231217 15:42:11] [Test] Precision: 0.466350 Recall: 0.557871 F1: 0.508021\n",
      "[20231217 15:42:11] [epoch 33] TLoss: 0.449 VLoss: 0.935 TAcc: 0.816 VAcc: 0.674\n",
      "[20231217 15:43:14] [Test] Precision: 0.466656 Recall: 0.548778 F1: 0.504396\n",
      "[20231217 15:43:14] [epoch 34] TLoss: 0.438 VLoss: 0.915 TAcc: 0.820 VAcc: 0.677\n",
      "[20231217 15:44:16] [Test] Precision: 0.475500 Recall: 0.558818 F1: 0.513803\n",
      "[20231217 15:44:16] [epoch 35] TLoss: 0.426 VLoss: 0.903 TAcc: 0.824 VAcc: 0.680\n",
      "[20231217 15:45:19] [Test] Precision: 0.473541 Recall: 0.559386 F1: 0.512896\n",
      "[20231217 15:45:19] [epoch 36] TLoss: 0.416 VLoss: 0.895 TAcc: 0.828 VAcc: 0.683\n",
      "[20231217 15:45:19] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertEntity.1e-06.12171545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 15:45:25] [Test] Precision: 0.486756 Recall: 0.474741 F1: 0.480673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4867555255609099, 0.4747408260654147, 0.4806731084636767)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ent = BertEntity(args).to(device)\n",
    "train(model_ent, train_loader, valid_loader, epochs = 100, lr = 1e-6)\n",
    "evaluate(model_ent, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8c7af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertEntity model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 17:19:28] [Test] Precision: 0.067023 Recall: 0.035613 F1: 0.046512\n",
      "[20231217 17:19:28] [epoch 1] TLoss: 18.143 VLoss: 5.282 TAcc: 0.495 VAcc: 0.562\n",
      "[20231217 17:20:32] [Test] Precision: 0.218950 Recall: 0.201364 F1: 0.209789\n",
      "[20231217 17:20:32] [epoch 2] TLoss: 3.529 VLoss: 2.998 TAcc: 0.578 VAcc: 0.616\n",
      "[20231217 17:21:37] [Test] Precision: 0.297943 Recall: 0.290775 F1: 0.294315\n",
      "[20231217 17:21:37] [epoch 3] TLoss: 2.470 VLoss: 2.162 TAcc: 0.630 VAcc: 0.633\n",
      "[20231217 17:22:41] [Test] Precision: 0.388535 Recall: 0.392877 F1: 0.390694\n",
      "[20231217 17:22:41] [epoch 4] TLoss: 1.832 VLoss: 1.630 TAcc: 0.659 VAcc: 0.650\n",
      "[20231217 17:23:45] [Test] Precision: 0.405358 Recall: 0.441371 F1: 0.422599\n",
      "[20231217 17:23:45] [epoch 5] TLoss: 1.478 VLoss: 1.429 TAcc: 0.678 VAcc: 0.649\n",
      "[20231217 17:24:48] [Test] Precision: 0.435022 Recall: 0.494601 F1: 0.462902\n",
      "[20231217 17:24:48] [epoch 6] TLoss: 1.229 VLoss: 1.279 TAcc: 0.695 VAcc: 0.659\n",
      "[20231217 17:25:52] [Test] Precision: 0.450931 Recall: 0.504830 F1: 0.476361\n",
      "[20231217 17:25:52] [epoch 7] TLoss: 1.060 VLoss: 1.159 TAcc: 0.708 VAcc: 0.668\n",
      "[20231217 17:26:56] [Test] Precision: 0.435610 Recall: 0.507482 F1: 0.468807\n",
      "[20231217 17:26:56] [epoch 8] TLoss: 0.935 VLoss: 1.108 TAcc: 0.719 VAcc: 0.671\n",
      "[20231217 17:27:59] [Test] Precision: 0.440871 Recall: 0.521879 F1: 0.477967\n",
      "[20231217 17:27:59] [epoch 9] TLoss: 0.840 VLoss: 1.099 TAcc: 0.730 VAcc: 0.666\n",
      "[20231217 17:29:01] [Test] Precision: 0.439181 Recall: 0.504073 F1: 0.469395\n",
      "[20231217 17:29:01] [epoch 10] TLoss: 0.764 VLoss: 1.054 TAcc: 0.742 VAcc: 0.672\n",
      "[20231217 17:30:05] [Test] Precision: 0.443217 Recall: 0.509377 F1: 0.474000\n",
      "[20231217 17:30:05] [epoch 11] TLoss: 0.706 VLoss: 1.031 TAcc: 0.752 VAcc: 0.670\n",
      "[20231217 17:31:06] [Test] Precision: 0.451576 Recall: 0.529077 F1: 0.487264\n",
      "[20231217 17:31:06] [epoch 12] TLoss: 0.660 VLoss: 1.022 TAcc: 0.760 VAcc: 0.669\n",
      "[20231217 17:32:09] [Test] Precision: 0.463474 Recall: 0.537223 F1: 0.497631\n",
      "[20231217 17:32:09] [epoch 13] TLoss: 0.617 VLoss: 0.990 TAcc: 0.771 VAcc: 0.676\n",
      "[20231217 17:33:12] [Test] Precision: 0.465961 Recall: 0.541959 F1: 0.501095\n",
      "[20231217 17:33:12] [epoch 14] TLoss: 0.578 VLoss: 0.981 TAcc: 0.780 VAcc: 0.674\n",
      "[20231217 17:34:14] [Test] Precision: 0.459035 Recall: 0.547642 F1: 0.499439\n",
      "[20231217 17:34:14] [epoch 15] TLoss: 0.545 VLoss: 1.003 TAcc: 0.788 VAcc: 0.667\n",
      "[20231217 17:35:18] [Test] Precision: 0.471262 Recall: 0.545179 F1: 0.505533\n",
      "[20231217 17:35:18] [epoch 16] TLoss: 0.513 VLoss: 0.953 TAcc: 0.799 VAcc: 0.680\n",
      "[20231217 17:36:24] [Test] Precision: 0.467192 Recall: 0.550294 F1: 0.505349\n",
      "[20231217 17:36:24] [epoch 17] TLoss: 0.485 VLoss: 0.963 TAcc: 0.807 VAcc: 0.677\n",
      "[20231217 17:37:25] [Test] Precision: 0.475304 Recall: 0.555977 F1: 0.512485\n",
      "[20231217 17:37:25] [epoch 18] TLoss: 0.461 VLoss: 0.940 TAcc: 0.815 VAcc: 0.680\n",
      "[20231217 17:38:29] [Test] Precision: 0.474937 Recall: 0.567153 F1: 0.516965\n",
      "[20231217 17:38:29] [epoch 19] TLoss: 0.434 VLoss: 0.941 TAcc: 0.822 VAcc: 0.684\n",
      "[20231217 17:39:32] [Test] Precision: 0.474499 Recall: 0.574541 F1: 0.519750\n",
      "[20231217 17:39:32] [epoch 20] TLoss: 0.414 VLoss: 0.968 TAcc: 0.829 VAcc: 0.675\n",
      "[20231217 17:40:33] [Test] Precision: 0.468125 Recall: 0.571699 F1: 0.514754\n",
      "[20231217 17:40:33] [epoch 21] TLoss: 0.393 VLoss: 0.978 TAcc: 0.835 VAcc: 0.678\n",
      "[20231217 17:41:35] [Test] Precision: 0.473856 Recall: 0.576814 F1: 0.520290\n",
      "[20231217 17:41:35] [epoch 22] TLoss: 0.374 VLoss: 0.980 TAcc: 0.840 VAcc: 0.676\n",
      "[20231217 17:42:37] [Test] Precision: 0.472395 Recall: 0.577003 F1: 0.519485\n",
      "[20231217 17:42:37] [epoch 23] TLoss: 0.354 VLoss: 0.985 TAcc: 0.848 VAcc: 0.679\n",
      "[20231217 17:43:39] [Test] Precision: 0.483250 Recall: 0.584770 F1: 0.529185\n",
      "[20231217 17:43:39] [epoch 24] TLoss: 0.337 VLoss: 0.938 TAcc: 0.854 VAcc: 0.691\n",
      "[20231217 17:44:42] [Test] Precision: 0.474561 Recall: 0.593673 F1: 0.527476\n",
      "[20231217 17:44:42] [epoch 25] TLoss: 0.320 VLoss: 1.040 TAcc: 0.858 VAcc: 0.672\n",
      "[20231217 17:45:44] [Test] Precision: 0.471938 Recall: 0.584580 F1: 0.522254\n",
      "[20231217 17:45:44] [epoch 26] TLoss: 0.306 VLoss: 1.003 TAcc: 0.863 VAcc: 0.681\n",
      "[20231217 17:46:46] [Test] Precision: 0.480515 Recall: 0.586285 F1: 0.528157\n",
      "[20231217 17:46:46] [epoch 27] TLoss: 0.291 VLoss: 0.935 TAcc: 0.868 VAcc: 0.690\n",
      "[20231217 17:47:48] [Test] Precision: 0.477909 Recall: 0.592158 F1: 0.528934\n",
      "[20231217 17:47:48] [epoch 28] TLoss: 0.278 VLoss: 1.014 TAcc: 0.873 VAcc: 0.680\n",
      "[20231217 17:48:50] [Test] Precision: 0.475394 Recall: 0.576435 F1: 0.521062\n",
      "[20231217 17:48:50] [epoch 29] TLoss: 0.263 VLoss: 0.993 TAcc: 0.879 VAcc: 0.683\n",
      "[20231217 17:48:50] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertEntity.2e-06.12171748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 17:48:56] [Test] Precision: 0.489816 Recall: 0.498601 F1: 0.494169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4898157129000178, 0.4986012835279745, 0.4941694523255318)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ent = BertEntity(args).to(device)\n",
    "train(model_ent, train_loader, valid_loader, epochs = 100, lr = 2e-6)\n",
    "evaluate(model_ent, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c92591",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    model_ent = BertEntity(args).to(device)\n",
    "    train(model_ent, train_loader, valid_loader, epochs = 100, lr = 1e-6)\n",
    "    evaluate(model_ent, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4454a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04245e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def seg_words(text):\n",
    "    seg = list(jieba.cut(text))\n",
    "    return seg\n",
    "\n",
    "# 对于每个target，同时使用Top tail entity进行加权, [entity_emb; avg(tail_emb)]\n",
    "def embed_bert_text2(text, max_len = 40):\n",
    "    words = seg_words(text)\n",
    "    embeds = torch.zeros((max_len, 768), dtype = torch.float32) # sen_len, emb_size\n",
    "    pos = 1 # [CLS]\n",
    "    for word in words:\n",
    "        if word in target_ids:\n",
    "            embeds[pos: pos + len(word)] += enhanced_target_embed[target_ids[word]][768:]\n",
    "        if word in opinion_ids:\n",
    "            embeds[pos: pos + len(word)] += enhanced_opinion_embed[opinion_ids[word]][768:]\n",
    "        pos += len(word)\n",
    "    return embeds\n",
    "\n",
    "class BertEntity2(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.cls = nn.Linear(args['bert_out_dim'], args['n_class'])\n",
    "    \n",
    "    def ent_layer(self, texts, bert_out):\n",
    "        ent_embeds = [embed_bert_text2(text, max_len = bert_out.shape[1]) for text in texts]\n",
    "        ent_embeds = torch.stack(ent_embeds)\n",
    "        return ent_embeds.cuda()\n",
    "    \n",
    "    def forward(self, **batch):\n",
    "        bert_out = self.bert(batch['tokens'], batch['masks'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        ent_embeds = self.ent_layer(batch['texts'], bert_out)\n",
    "        bert_out = bert_out + ent_embeds\n",
    "        # bert_out = torch.cat((bert_out, ent_embeds), dim = -1)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        return cls_out\n",
    "\n",
    "    def predict(self, **batch):\n",
    "        cls_out = self.forward(**batch)\n",
    "        pred = torch.max(cls_out, dim = 2).indices\n",
    "        return pred\n",
    "\n",
    "# model_ent = BertEntity(args).to(device)\n",
    "# model_ent.predict(tokens = 1, masks = 2, texts = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ce36e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertEntity2 model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 15:50:19] [Test] Precision: 0.000357 Recall: 0.000379 F1: 0.000367\n",
      "[20231217 15:50:19] [epoch 1] TLoss: 85.017 VLoss: 56.617 TAcc: 0.381 VAcc: 0.445\n",
      "[20231217 15:51:22] [Test] Precision: 0.023900 Recall: 0.020269 F1: 0.021935\n",
      "[20231217 15:51:22] [epoch 2] TLoss: 49.183 VLoss: 29.010 TAcc: 0.431 VAcc: 0.456\n",
      "[20231217 15:52:26] [Test] Precision: 0.021310 Recall: 0.020269 F1: 0.020777\n",
      "[20231217 15:52:26] [epoch 3] TLoss: 31.542 VLoss: 19.944 TAcc: 0.466 VAcc: 0.470\n",
      "[20231217 15:53:30] [Test] Precision: 0.071947 Recall: 0.071415 F1: 0.071680\n",
      "[20231217 15:53:30] [epoch 4] TLoss: 21.849 VLoss: 14.912 TAcc: 0.488 VAcc: 0.487\n",
      "[20231217 15:54:33] [Test] Precision: 0.145590 Recall: 0.148513 F1: 0.147037\n",
      "[20231217 15:54:33] [epoch 5] TLoss: 14.966 VLoss: 11.064 TAcc: 0.518 VAcc: 0.515\n",
      "[20231217 15:55:38] [Test] Precision: 0.212606 Recall: 0.203826 F1: 0.208124\n",
      "[20231217 15:55:38] [epoch 6] TLoss: 9.268 VLoss: 8.706 TAcc: 0.546 VAcc: 0.547\n",
      "[20231217 15:56:42] [Test] Precision: 0.216598 Recall: 0.209131 F1: 0.212799\n",
      "[20231217 15:56:42] [epoch 7] TLoss: 6.989 VLoss: 7.164 TAcc: 0.580 VAcc: 0.566\n",
      "[20231217 15:57:45] [Test] Precision: 0.241413 Recall: 0.235651 F1: 0.238497\n",
      "[20231217 15:57:45] [epoch 8] TLoss: 5.798 VLoss: 5.743 TAcc: 0.600 VAcc: 0.581\n",
      "[20231217 15:58:49] [Test] Precision: 0.262453 Recall: 0.252510 F1: 0.257386\n",
      "[20231217 15:58:49] [epoch 9] TLoss: 4.746 VLoss: 4.725 TAcc: 0.619 VAcc: 0.601\n",
      "[20231217 15:59:54] [Test] Precision: 0.310619 Recall: 0.310854 F1: 0.310737\n",
      "[20231217 15:59:54] [epoch 10] TLoss: 4.164 VLoss: 4.218 TAcc: 0.632 VAcc: 0.611\n",
      "[20231217 16:00:58] [Test] Precision: 0.316822 Recall: 0.321084 F1: 0.318939\n",
      "[20231217 16:00:58] [epoch 11] TLoss: 3.737 VLoss: 3.791 TAcc: 0.642 VAcc: 0.612\n",
      "[20231217 16:02:00] [Test] Precision: 0.315789 Recall: 0.314832 F1: 0.315310\n",
      "[20231217 16:02:00] [epoch 12] TLoss: 3.382 VLoss: 3.420 TAcc: 0.647 VAcc: 0.614\n",
      "[20231217 16:03:04] [Test] Precision: 0.317366 Recall: 0.315022 F1: 0.316190\n",
      "[20231217 16:03:04] [epoch 13] TLoss: 3.090 VLoss: 3.131 TAcc: 0.655 VAcc: 0.618\n",
      "[20231217 16:04:07] [Test] Precision: 0.316625 Recall: 0.313506 F1: 0.315058\n",
      "[20231217 16:04:07] [epoch 14] TLoss: 2.832 VLoss: 2.854 TAcc: 0.661 VAcc: 0.621\n",
      "[20231217 16:05:09] [Test] Precision: 0.314705 Recall: 0.312559 F1: 0.313629\n",
      "[20231217 16:05:09] [epoch 15] TLoss: 2.589 VLoss: 2.632 TAcc: 0.667 VAcc: 0.619\n",
      "[20231217 16:06:12] [Test] Precision: 0.304704 Recall: 0.295700 F1: 0.300135\n",
      "[20231217 16:06:12] [epoch 16] TLoss: 2.365 VLoss: 2.382 TAcc: 0.672 VAcc: 0.625\n",
      "[20231217 16:07:15] [Test] Precision: 0.338200 Recall: 0.342300 F1: 0.340237\n",
      "[20231217 16:07:15] [epoch 17] TLoss: 2.174 VLoss: 2.254 TAcc: 0.678 VAcc: 0.629\n",
      "[20231217 16:08:18] [Test] Precision: 0.341362 Recall: 0.340974 F1: 0.341168\n",
      "[20231217 16:08:18] [epoch 18] TLoss: 2.026 VLoss: 2.154 TAcc: 0.684 VAcc: 0.627\n",
      "[20231217 16:09:20] [Test] Precision: 0.348175 Recall: 0.345141 F1: 0.346651\n",
      "[20231217 16:09:20] [epoch 19] TLoss: 1.901 VLoss: 2.053 TAcc: 0.687 VAcc: 0.628\n",
      "[20231217 16:10:21] [Test] Precision: 0.354833 Recall: 0.357454 F1: 0.356139\n",
      "[20231217 16:10:21] [epoch 20] TLoss: 1.785 VLoss: 1.967 TAcc: 0.692 VAcc: 0.628\n",
      "[20231217 16:11:23] [Test] Precision: 0.358815 Recall: 0.371661 F1: 0.365125\n",
      "[20231217 16:11:23] [epoch 21] TLoss: 1.675 VLoss: 1.897 TAcc: 0.698 VAcc: 0.632\n",
      "[20231217 16:12:26] [Test] Precision: 0.367096 Recall: 0.372798 F1: 0.369925\n",
      "[20231217 16:12:26] [epoch 22] TLoss: 1.572 VLoss: 1.808 TAcc: 0.702 VAcc: 0.637\n",
      "[20231217 16:13:30] [Test] Precision: 0.372656 Recall: 0.387763 F1: 0.380059\n",
      "[20231217 16:13:30] [epoch 23] TLoss: 1.474 VLoss: 1.762 TAcc: 0.708 VAcc: 0.634\n",
      "[20231217 16:14:31] [Test] Precision: 0.373186 Recall: 0.389657 F1: 0.381244\n",
      "[20231217 16:14:31] [epoch 24] TLoss: 1.383 VLoss: 1.708 TAcc: 0.714 VAcc: 0.635\n",
      "[20231217 16:15:33] [Test] Precision: 0.372022 Recall: 0.393446 F1: 0.382434\n",
      "[20231217 16:15:33] [epoch 25] TLoss: 1.298 VLoss: 1.665 TAcc: 0.719 VAcc: 0.637\n",
      "[20231217 16:16:37] [Test] Precision: 0.366094 Recall: 0.379617 F1: 0.372733\n",
      "[20231217 16:16:37] [epoch 26] TLoss: 1.218 VLoss: 1.604 TAcc: 0.724 VAcc: 0.640\n",
      "[20231217 16:17:39] [Test] Precision: 0.366276 Recall: 0.384732 F1: 0.375277\n",
      "[20231217 16:17:39] [epoch 27] TLoss: 1.140 VLoss: 1.559 TAcc: 0.730 VAcc: 0.640\n",
      "[20231217 16:18:42] [Test] Precision: 0.372284 Recall: 0.386247 F1: 0.379137\n",
      "[20231217 16:18:42] [epoch 28] TLoss: 1.075 VLoss: 1.515 TAcc: 0.737 VAcc: 0.643\n",
      "[20231217 16:19:45] [Test] Precision: 0.382256 Recall: 0.396666 F1: 0.389328\n",
      "[20231217 16:19:45] [epoch 29] TLoss: 1.021 VLoss: 1.478 TAcc: 0.741 VAcc: 0.643\n",
      "[20231217 16:20:47] [Test] Precision: 0.382379 Recall: 0.403675 F1: 0.392739\n",
      "[20231217 16:20:47] [epoch 30] TLoss: 0.975 VLoss: 1.458 TAcc: 0.746 VAcc: 0.642\n",
      "[20231217 16:21:48] [Test] Precision: 0.392086 Recall: 0.409168 F1: 0.400445\n",
      "[20231217 16:21:48] [epoch 31] TLoss: 0.932 VLoss: 1.403 TAcc: 0.751 VAcc: 0.649\n",
      "[20231217 16:22:51] [Test] Precision: 0.396090 Recall: 0.414472 F1: 0.405073\n",
      "[20231217 16:22:51] [epoch 32] TLoss: 0.891 VLoss: 1.367 TAcc: 0.756 VAcc: 0.654\n",
      "[20231217 16:23:55] [Test] Precision: 0.425599 Recall: 0.457284 F1: 0.440873\n",
      "[20231217 16:23:55] [epoch 33] TLoss: 0.852 VLoss: 1.351 TAcc: 0.761 VAcc: 0.655\n",
      "[20231217 16:24:58] [Test] Precision: 0.422053 Recall: 0.462588 F1: 0.441392\n",
      "[20231217 16:24:58] [epoch 34] TLoss: 0.814 VLoss: 1.322 TAcc: 0.766 VAcc: 0.659\n",
      "[20231217 16:26:02] [Test] Precision: 0.421117 Recall: 0.460125 F1: 0.439757\n",
      "[20231217 16:26:02] [epoch 35] TLoss: 0.778 VLoss: 1.308 TAcc: 0.772 VAcc: 0.656\n",
      "[20231217 16:27:04] [Test] Precision: 0.426020 Recall: 0.460883 F1: 0.442766\n",
      "[20231217 16:27:04] [epoch 36] TLoss: 0.749 VLoss: 1.257 TAcc: 0.775 VAcc: 0.669\n",
      "[20231217 16:28:08] [Test] Precision: 0.431040 Recall: 0.481909 F1: 0.455058\n",
      "[20231217 16:28:08] [epoch 37] TLoss: 0.717 VLoss: 1.267 TAcc: 0.782 VAcc: 0.661\n",
      "[20231217 16:29:09] [Test] Precision: 0.431742 Recall: 0.486456 F1: 0.457469\n",
      "[20231217 16:29:09] [epoch 38] TLoss: 0.689 VLoss: 1.255 TAcc: 0.789 VAcc: 0.666\n",
      "[20231217 16:30:11] [Test] Precision: 0.431420 Recall: 0.475469 F1: 0.452375\n",
      "[20231217 16:30:11] [epoch 39] TLoss: 0.670 VLoss: 1.221 TAcc: 0.793 VAcc: 0.670\n",
      "[20231217 16:31:16] [Test] Precision: 0.421035 Recall: 0.470165 F1: 0.444246\n",
      "[20231217 16:31:16] [epoch 40] TLoss: 0.653 VLoss: 1.247 TAcc: 0.795 VAcc: 0.662\n",
      "[20231217 16:32:19] [Test] Precision: 0.419898 Recall: 0.467702 F1: 0.442513\n",
      "[20231217 16:32:19] [epoch 41] TLoss: 0.632 VLoss: 1.252 TAcc: 0.800 VAcc: 0.661\n",
      "[20231217 16:33:22] [Test] Precision: 0.424944 Recall: 0.469218 F1: 0.445985\n",
      "[20231217 16:33:22] [epoch 42] TLoss: 0.616 VLoss: 1.199 TAcc: 0.803 VAcc: 0.672\n",
      "[20231217 16:34:24] [Test] Precision: 0.427819 Recall: 0.477174 F1: 0.451151\n",
      "[20231217 16:34:24] [epoch 43] TLoss: 0.601 VLoss: 1.209 TAcc: 0.807 VAcc: 0.670\n",
      "[20231217 16:35:26] [Test] Precision: 0.428547 Recall: 0.477742 F1: 0.451809\n",
      "[20231217 16:35:26] [epoch 44] TLoss: 0.582 VLoss: 1.228 TAcc: 0.810 VAcc: 0.666\n",
      "[20231217 16:36:28] [Test] Precision: 0.433305 Recall: 0.478121 F1: 0.454611\n",
      "[20231217 16:36:28] [epoch 45] TLoss: 0.569 VLoss: 1.194 TAcc: 0.814 VAcc: 0.674\n",
      "[20231217 16:37:31] [Test] Precision: 0.427598 Recall: 0.475469 F1: 0.450265\n",
      "[20231217 16:37:31] [epoch 46] TLoss: 0.554 VLoss: 1.219 TAcc: 0.818 VAcc: 0.667\n",
      "[20231217 16:38:34] [Test] Precision: 0.432956 Recall: 0.478310 F1: 0.454505\n",
      "[20231217 16:38:34] [epoch 47] TLoss: 0.540 VLoss: 1.193 TAcc: 0.820 VAcc: 0.670\n",
      "[20231217 16:39:36] [Test] Precision: 0.431049 Recall: 0.480205 F1: 0.454301\n",
      "[20231217 16:39:36] [epoch 48] TLoss: 0.526 VLoss: 1.226 TAcc: 0.823 VAcc: 0.665\n",
      "[20231217 16:40:37] [Test] Precision: 0.428354 Recall: 0.480205 F1: 0.452800\n",
      "[20231217 16:40:37] [epoch 49] TLoss: 0.512 VLoss: 1.201 TAcc: 0.827 VAcc: 0.670\n",
      "[20231217 16:41:39] [Test] Precision: 0.433953 Recall: 0.486645 F1: 0.458791\n",
      "[20231217 16:41:39] [epoch 50] TLoss: 0.503 VLoss: 1.191 TAcc: 0.830 VAcc: 0.675\n",
      "[20231217 16:42:43] [Test] Precision: 0.439321 Recall: 0.485509 F1: 0.461262\n",
      "[20231217 16:42:43] [epoch 51] TLoss: 0.494 VLoss: 1.166 TAcc: 0.832 VAcc: 0.680\n",
      "[20231217 16:43:46] [Test] Precision: 0.433773 Recall: 0.491949 F1: 0.461033\n",
      "[20231217 16:43:46] [epoch 52] TLoss: 0.484 VLoss: 1.203 TAcc: 0.835 VAcc: 0.671\n",
      "[20231217 16:44:49] [Test] Precision: 0.439540 Recall: 0.492328 F1: 0.464439\n",
      "[20231217 16:44:49] [epoch 53] TLoss: 0.473 VLoss: 1.192 TAcc: 0.839 VAcc: 0.675\n",
      "[20231217 16:45:51] [Test] Precision: 0.439061 Recall: 0.496117 F1: 0.465848\n",
      "[20231217 16:45:51] [epoch 54] TLoss: 0.466 VLoss: 1.196 TAcc: 0.839 VAcc: 0.675\n",
      "[20231217 16:46:53] [Test] Precision: 0.434164 Recall: 0.491570 F1: 0.461087\n",
      "[20231217 16:46:53] [epoch 55] TLoss: 0.455 VLoss: 1.203 TAcc: 0.844 VAcc: 0.673\n",
      "[20231217 16:47:54] [Test] Precision: 0.438955 Recall: 0.499905 F1: 0.467452\n",
      "[20231217 16:47:54] [epoch 56] TLoss: 0.450 VLoss: 1.213 TAcc: 0.844 VAcc: 0.672\n",
      "[20231217 16:47:54] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertEntity2.1e-06.12171647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 16:48:00] [Test] Precision: 0.456582 Recall: 0.432615 F1: 0.444275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4565821465785938, 0.43261477702806767, 0.44427545366167387)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ent2 = BertEntity2(args).to(device)\n",
    "train(model_ent2, train_loader, valid_loader, epochs = 100, lr = 1e-6)\n",
    "evaluate(model_ent2, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20423b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376c668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6204d6b1",
   "metadata": {},
   "source": [
    "### BertEntityCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ccceb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def seg_words(text):\n",
    "    seg = list(jieba.cut(text))\n",
    "    return seg\n",
    "\n",
    "# 对于每个target，同时使用Top tail entity进行加权, [entity_emb; avg(tail_emb)]\n",
    "def embed_bert_text(text, max_len = 40):\n",
    "    words = seg_words(text)\n",
    "    embeds = torch.zeros((max_len, 768 * 2), dtype = torch.float32) # sen_len, emb_size\n",
    "    pos = 1 # [CLS]\n",
    "    for word in words:\n",
    "        if word in target_ids:\n",
    "            embeds[pos: pos + len(word)] += enhanced_target_embed[target_ids[word]]\n",
    "        if word in opinion_ids:\n",
    "            embeds[pos: pos + len(word)] += enhanced_opinion_embed[opinion_ids[word]]\n",
    "        pos += len(word)\n",
    "    return embeds\n",
    "\n",
    "class BertCRFEntity(nn.Module):\n",
    "    def __init__(self, args, tag_dict):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.cls = nn.Linear(args['bert_out_dim'] * 3, args['n_class'])\n",
    "        self.crf = CRF({tag_dict[tag]: tag for tag in tag_dict})\n",
    "        \n",
    "    def ent_layer(self, texts, bert_out):\n",
    "        ent_embeds = [embed_bert_text(text, max_len = bert_out.shape[1]) for text in texts]\n",
    "        ent_embeds = torch.stack(ent_embeds)\n",
    "        return ent_embeds.cuda()\n",
    "    \n",
    "    # def forward(self, **batch):\n",
    "    #     bert_out = self.bert(tokens)['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "    #     bert_out = self.dropout(bert_out)\n",
    "    #     cls_out = self.cls(bert_out)\n",
    "    #     log_likelihood = self.crf(cls_out, labels, masks)\n",
    "    #     n_batch = labels.shape[0]\n",
    "    #     loss = -log_likelihood / n_batch\n",
    "    #     return loss\n",
    "    \n",
    "    def forward(self, **batch):\n",
    "        bert_out = self.bert(batch['tokens'], batch['masks'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        ent_embeds = self.ent_layer(batch['texts'], bert_out)\n",
    "        bert_out = torch.cat((bert_out, ent_embeds), dim = -1)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        log_likelihood = self.crf(cls_out, batch['labels'], batch['masks'])\n",
    "        n_batch = batch['labels'].shape[0]\n",
    "        loss = -log_likelihood / n_batch\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, **batch):\n",
    "        bert_out = self.bert(batch['tokens'], batch['masks'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        # bert_out = self.bert(batch['tokens'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        ent_embeds = self.ent_layer(batch['texts'], bert_out)\n",
    "        bert_out = torch.cat((bert_out, ent_embeds), dim = -1)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        predict = self.crf.viterbi_tags(cls_out, batch['masks'])\n",
    "        return predict\n",
    "\n",
    "# model_ent = BertEntity(args).to(device)\n",
    "# model_ent.predict(tokens = 1, masks = 2, texts = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67e2de7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Building prefix dict from the default dictionary ...\n",
      "[20231217 23:54:32] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "[20231217 23:54:32] Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.580 seconds.\n",
      "[20231217 23:54:33] Loading model cost 0.580 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "[20231217 23:54:33] Prefix dict has been built successfully.\n",
      "[20231217 23:54:41] [Test] Precision: 0.514052 Recall: 0.577917 F1: 0.544117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5140515222481683, 0.5779167352311044, 0.5441165073643575)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf_ent = BertCRFEntity(args, tag_dict).to(device)\n",
    "# train_crf(model_crf_ent, epochs = 100, lr = 1e-6)\n",
    "# evaluate(model_crf_ent, test_loader, tag_list)\n",
    "model_crf_ent.load_state_dict(torch.load('./results/BertCRFEntity.1e-06.12171934'))\n",
    "evaluate(model_crf_ent, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e588addc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntity model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231217 23:57:29] [Test] Precision: 0.199636 Recall: 0.249100 F1: 0.221642\n",
      "[20231217 23:57:29] [epoch 1] TLoss: 452.807 VLoss: 150.459 TAcc: 0.500 VAcc: 0.597\n",
      "[20231217 23:59:29] [Test] Precision: 0.331000 Recall: 0.420913 F1: 0.370580\n",
      "[20231217 23:59:29] [epoch 2] TLoss: 103.061 VLoss: 95.830 TAcc: 0.618 VAcc: 0.649\n",
      "[20231218 00:01:31] [Test] Precision: 0.376172 Recall: 0.494033 F1: 0.427121\n",
      "[20231218 00:01:31] [epoch 3] TLoss: 69.118 VLoss: 66.628 TAcc: 0.653 VAcc: 0.669\n",
      "[20231218 00:03:34] [Test] Precision: 0.399972 Recall: 0.535518 F1: 0.457925\n",
      "[20231218 00:03:34] [epoch 4] TLoss: 53.109 VLoss: 56.985 TAcc: 0.679 VAcc: 0.677\n",
      "[20231218 00:05:40] [Test] Precision: 0.416327 Recall: 0.560333 F1: 0.477713\n",
      "[20231218 00:05:40] [epoch 5] TLoss: 45.422 VLoss: 51.789 TAcc: 0.702 VAcc: 0.683\n",
      "[20231218 00:07:42] [Test] Precision: 0.425849 Recall: 0.584202 F1: 0.492612\n",
      "[20231218 00:07:42] [epoch 6] TLoss: 39.783 VLoss: 49.016 TAcc: 0.719 VAcc: 0.680\n",
      "[20231218 00:09:42] [Test] Precision: 0.447788 Recall: 0.617352 F1: 0.519073\n",
      "[20231218 00:09:42] [epoch 7] TLoss: 35.179 VLoss: 46.816 TAcc: 0.734 VAcc: 0.683\n",
      "[20231218 00:11:48] [Test] Precision: 0.455497 Recall: 0.626255 F1: 0.527399\n",
      "[20231218 00:11:48] [epoch 8] TLoss: 31.666 VLoss: 45.115 TAcc: 0.751 VAcc: 0.683\n",
      "[20231218 00:13:55] [Test] Precision: 0.454091 Recall: 0.636105 F1: 0.529904\n",
      "[20231218 00:13:55] [epoch 9] TLoss: 29.107 VLoss: 43.646 TAcc: 0.763 VAcc: 0.684\n",
      "[20231218 00:16:03] [Test] Precision: 0.461289 Recall: 0.645577 F1: 0.538091\n",
      "[20231218 00:16:03] [epoch 10] TLoss: 26.682 VLoss: 41.781 TAcc: 0.777 VAcc: 0.684\n",
      "[20231218 00:18:05] [Test] Precision: 0.472843 Recall: 0.661299 F1: 0.551414\n",
      "[20231218 00:18:05] [epoch 11] TLoss: 24.745 VLoss: 40.958 TAcc: 0.791 VAcc: 0.687\n",
      "[20231218 00:20:07] [Test] Precision: 0.477648 Recall: 0.667930 F1: 0.556986\n",
      "[20231218 00:20:07] [epoch 12] TLoss: 22.779 VLoss: 39.648 TAcc: 0.804 VAcc: 0.689\n",
      "[20231218 00:22:10] [Test] Precision: 0.478569 Recall: 0.678916 F1: 0.561404\n",
      "[20231218 00:22:10] [epoch 13] TLoss: 21.093 VLoss: 40.065 TAcc: 0.812 VAcc: 0.684\n",
      "[20231218 00:24:13] [Test] Precision: 0.487848 Recall: 0.673044 F1: 0.565674\n",
      "[20231218 00:24:13] [epoch 14] TLoss: 19.766 VLoss: 38.686 TAcc: 0.822 VAcc: 0.695\n",
      "[20231218 00:26:17] [Test] Precision: 0.484211 Recall: 0.679674 F1: 0.565529\n",
      "[20231218 00:26:17] [epoch 15] TLoss: 18.518 VLoss: 39.795 TAcc: 0.833 VAcc: 0.684\n",
      "[20231218 00:28:18] [Test] Precision: 0.490747 Recall: 0.678159 F1: 0.569429\n",
      "[20231218 00:28:18] [epoch 16] TLoss: 17.422 VLoss: 38.171 TAcc: 0.839 VAcc: 0.696\n",
      "[20231218 00:30:23] [Test] Precision: 0.493831 Recall: 0.682326 F1: 0.572974\n",
      "[20231218 00:30:23] [epoch 17] TLoss: 16.420 VLoss: 38.297 TAcc: 0.848 VAcc: 0.696\n",
      "[20231218 00:32:24] [Test] Precision: 0.500558 Recall: 0.679674 F1: 0.576524\n",
      "[20231218 00:32:24] [epoch 18] TLoss: 15.427 VLoss: 37.431 TAcc: 0.855 VAcc: 0.700\n",
      "[20231218 00:34:27] [Test] Precision: 0.500695 Recall: 0.682516 F1: 0.577635\n",
      "[20231218 00:34:27] [epoch 19] TLoss: 14.423 VLoss: 37.019 TAcc: 0.863 VAcc: 0.704\n",
      "[20231218 00:36:37] [Test] Precision: 0.495079 Recall: 0.686115 F1: 0.575149\n",
      "[20231218 00:36:37] [epoch 20] TLoss: 13.772 VLoss: 40.139 TAcc: 0.869 VAcc: 0.687\n",
      "[20231218 00:38:42] [Test] Precision: 0.502502 Recall: 0.684789 F1: 0.579652\n",
      "[20231218 00:38:42] [epoch 21] TLoss: 13.138 VLoss: 38.313 TAcc: 0.876 VAcc: 0.701\n",
      "[20231218 00:40:41] [Test] Precision: 0.494466 Recall: 0.685547 F1: 0.574536\n",
      "[20231218 00:40:41] [epoch 22] TLoss: 12.592 VLoss: 42.138 TAcc: 0.882 VAcc: 0.686\n",
      "[20231218 00:42:43] [Test] Precision: 0.495163 Recall: 0.688388 F1: 0.576003\n",
      "[20231218 00:42:43] [epoch 23] TLoss: 12.000 VLoss: 42.172 TAcc: 0.885 VAcc: 0.688\n",
      "[20231218 00:44:46] [Test] Precision: 0.500138 Recall: 0.687441 F1: 0.579019\n",
      "[20231218 00:44:46] [epoch 24] TLoss: 11.611 VLoss: 41.107 TAcc: 0.890 VAcc: 0.694\n",
      "[20231218 00:44:46] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntity.2e-06.12180044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 00:44:55] [Test] Precision: 0.537805 Recall: 0.612144 F1: 0.572572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5378054069682611, 0.612144150073949, 0.5725719557895187)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf_ent = BertCRFEntity(args, tag_dict).to(device)\n",
    "train_crf(model_crf_ent, epochs = 100, lr = 2e-6)\n",
    "evaluate(model_crf_ent, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0564403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntity model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 00:46:57] [Test] Precision: 0.330181 Recall: 0.324114 F1: 0.327120\n",
      "[20231218 00:46:57] [epoch 1] TLoss: 263.273 VLoss: 113.801 TAcc: 0.498 VAcc: 0.655\n",
      "[20231218 00:48:58] [Test] Precision: 0.388771 Recall: 0.452548 F1: 0.418242\n",
      "[20231218 00:48:58] [epoch 2] TLoss: 103.249 VLoss: 73.488 TAcc: 0.634 VAcc: 0.676\n",
      "[20231218 00:51:01] [Test] Precision: 0.433328 Recall: 0.557113 F1: 0.487485\n",
      "[20231218 00:51:01] [epoch 3] TLoss: 73.065 VLoss: 59.314 TAcc: 0.669 VAcc: 0.689\n",
      "[20231218 00:53:05] [Test] Precision: 0.429884 Recall: 0.549346 F1: 0.482328\n",
      "[20231218 00:53:05] [epoch 4] TLoss: 58.212 VLoss: 56.045 TAcc: 0.695 VAcc: 0.675\n",
      "[20231218 00:55:06] [Test] Precision: 0.440857 Recall: 0.576814 F1: 0.499754\n",
      "[20231218 00:55:06] [epoch 5] TLoss: 49.910 VLoss: 51.372 TAcc: 0.708 VAcc: 0.685\n",
      "[20231218 00:57:08] [Test] Precision: 0.453767 Recall: 0.602387 F1: 0.517620\n",
      "[20231218 00:57:08] [epoch 6] TLoss: 43.932 VLoss: 46.247 TAcc: 0.725 VAcc: 0.695\n",
      "[20231218 00:59:12] [Test] Precision: 0.446114 Recall: 0.613184 F1: 0.516474\n",
      "[20231218 00:59:12] [epoch 7] TLoss: 39.263 VLoss: 45.245 TAcc: 0.737 VAcc: 0.690\n",
      "[20231218 01:01:12] [Test] Precision: 0.454246 Recall: 0.599924 F1: 0.517019\n",
      "[20231218 01:01:12] [epoch 8] TLoss: 35.531 VLoss: 42.820 TAcc: 0.748 VAcc: 0.693\n",
      "[20231218 01:03:12] [Test] Precision: 0.457598 Recall: 0.609206 F1: 0.522629\n",
      "[20231218 01:03:12] [epoch 9] TLoss: 32.275 VLoss: 41.746 TAcc: 0.760 VAcc: 0.693\n",
      "[20231218 01:05:11] [Test] Precision: 0.472733 Recall: 0.619057 F1: 0.536089\n",
      "[20231218 01:05:11] [epoch 10] TLoss: 29.327 VLoss: 40.206 TAcc: 0.769 VAcc: 0.697\n",
      "[20231218 01:07:11] [Test] Precision: 0.476427 Recall: 0.635537 F1: 0.544599\n",
      "[20231218 01:07:11] [epoch 11] TLoss: 26.699 VLoss: 40.585 TAcc: 0.783 VAcc: 0.693\n",
      "[20231218 01:09:09] [Test] Precision: 0.477677 Recall: 0.640462 F1: 0.547220\n",
      "[20231218 01:09:09] [epoch 12] TLoss: 24.689 VLoss: 38.889 TAcc: 0.795 VAcc: 0.696\n",
      "[20231218 01:11:08] [Test] Precision: 0.481223 Recall: 0.640841 F1: 0.549679\n",
      "[20231218 01:11:08] [epoch 13] TLoss: 22.979 VLoss: 39.125 TAcc: 0.803 VAcc: 0.695\n",
      "[20231218 01:13:06] [Test] Precision: 0.483071 Recall: 0.656753 F1: 0.556680\n",
      "[20231218 01:13:06] [epoch 14] TLoss: 21.463 VLoss: 39.052 TAcc: 0.815 VAcc: 0.694\n",
      "[20231218 01:15:06] [Test] Precision: 0.489799 Recall: 0.654859 F1: 0.560428\n",
      "[20231218 01:15:06] [epoch 15] TLoss: 20.235 VLoss: 38.705 TAcc: 0.825 VAcc: 0.697\n",
      "[20231218 01:15:06] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntity.2e-06.12180115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 01:15:15] [Test] Precision: 0.532356 Recall: 0.579398 F1: 0.554881\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntity model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 01:17:17] [Test] Precision: 0.243431 Recall: 0.212351 F1: 0.226831\n",
      "[20231218 01:17:17] [epoch 1] TLoss: 357.431 VLoss: 129.137 TAcc: 0.470 VAcc: 0.608\n",
      "[20231218 01:19:21] [Test] Precision: 0.316594 Recall: 0.341542 F1: 0.328595\n",
      "[20231218 01:19:21] [epoch 2] TLoss: 101.463 VLoss: 97.806 TAcc: 0.603 VAcc: 0.646\n",
      "[20231218 01:21:24] [Test] Precision: 0.346241 Recall: 0.404812 F1: 0.373243\n",
      "[20231218 01:21:24] [epoch 3] TLoss: 81.478 VLoss: 81.614 TAcc: 0.645 VAcc: 0.667\n",
      "[20231218 01:23:25] [Test] Precision: 0.359768 Recall: 0.433984 F1: 0.393406\n",
      "[20231218 01:23:25] [epoch 4] TLoss: 68.025 VLoss: 68.578 TAcc: 0.669 VAcc: 0.675\n",
      "[20231218 01:25:29] [Test] Precision: 0.389852 Recall: 0.468649 F1: 0.425634\n",
      "[20231218 01:25:29] [epoch 5] TLoss: 57.157 VLoss: 57.254 TAcc: 0.686 VAcc: 0.680\n",
      "[20231218 01:27:33] [Test] Precision: 0.434113 Recall: 0.523584 F1: 0.474669\n",
      "[20231218 01:27:33] [epoch 6] TLoss: 48.273 VLoss: 50.126 TAcc: 0.706 VAcc: 0.698\n",
      "[20231218 01:29:38] [Test] Precision: 0.440773 Recall: 0.548399 F1: 0.488731\n",
      "[20231218 01:29:38] [epoch 7] TLoss: 42.071 VLoss: 45.858 TAcc: 0.719 VAcc: 0.700\n",
      "[20231218 01:31:42] [Test] Precision: 0.438854 Recall: 0.560144 F1: 0.492136\n",
      "[20231218 01:31:42] [epoch 8] TLoss: 37.239 VLoss: 42.744 TAcc: 0.734 VAcc: 0.694\n",
      "[20231218 01:33:43] [Test] Precision: 0.441138 Recall: 0.581360 F1: 0.501635\n",
      "[20231218 01:33:43] [epoch 9] TLoss: 33.008 VLoss: 40.118 TAcc: 0.747 VAcc: 0.692\n",
      "[20231218 01:35:45] [Test] Precision: 0.438917 Recall: 0.586664 F1: 0.502148\n",
      "[20231218 01:35:45] [epoch 10] TLoss: 29.256 VLoss: 37.228 TAcc: 0.753 VAcc: 0.689\n",
      "[20231218 01:37:47] [Test] Precision: 0.445915 Recall: 0.599735 F1: 0.511511\n",
      "[20231218 01:37:47] [epoch 11] TLoss: 26.061 VLoss: 34.481 TAcc: 0.764 VAcc: 0.692\n",
      "[20231218 01:39:49] [Test] Precision: 0.459751 Recall: 0.622088 F1: 0.528739\n",
      "[20231218 01:39:49] [epoch 12] TLoss: 23.177 VLoss: 32.578 TAcc: 0.774 VAcc: 0.694\n",
      "[20231218 01:39:49] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntity.2e-06.12180139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 01:39:57] [Test] Precision: 0.486514 Recall: 0.543196 F1: 0.513295\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntity model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 01:41:59] [Test] Precision: 0.125931 Recall: 0.189051 F1: 0.151166\n",
      "[20231218 01:41:59] [epoch 1] TLoss: 1213.103 VLoss: 406.590 TAcc: 0.367 VAcc: 0.515\n",
      "[20231218 01:44:02] [Test] Precision: 0.339678 Recall: 0.419208 F1: 0.375276\n",
      "[20231218 01:44:02] [epoch 2] TLoss: 156.816 VLoss: 111.449 TAcc: 0.592 VAcc: 0.656\n",
      "[20231218 01:46:04] [Test] Precision: 0.362917 Recall: 0.469407 F1: 0.409350\n",
      "[20231218 01:46:04] [epoch 3] TLoss: 100.522 VLoss: 85.416 TAcc: 0.657 VAcc: 0.664\n",
      "[20231218 01:48:10] [Test] Precision: 0.389868 Recall: 0.502936 F1: 0.439242\n",
      "[20231218 01:48:10] [epoch 4] TLoss: 78.272 VLoss: 68.745 TAcc: 0.684 VAcc: 0.679\n",
      "[20231218 01:50:14] [Test] Precision: 0.398902 Recall: 0.550483 F1: 0.462592\n",
      "[20231218 01:50:14] [epoch 5] TLoss: 63.439 VLoss: 60.034 TAcc: 0.702 VAcc: 0.681\n",
      "[20231218 01:52:19] [Test] Precision: 0.404952 Recall: 0.557681 F1: 0.469201\n",
      "[20231218 01:52:19] [epoch 6] TLoss: 52.064 VLoss: 52.854 TAcc: 0.720 VAcc: 0.681\n",
      "[20231218 01:54:22] [Test] Precision: 0.410763 Recall: 0.571131 F1: 0.477851\n",
      "[20231218 01:54:22] [epoch 7] TLoss: 43.320 VLoss: 48.267 TAcc: 0.731 VAcc: 0.681\n",
      "[20231218 01:56:23] [Test] Precision: 0.432558 Recall: 0.602008 F1: 0.503406\n",
      "[20231218 01:56:23] [epoch 8] TLoss: 36.422 VLoss: 45.091 TAcc: 0.746 VAcc: 0.683\n",
      "[20231218 01:58:24] [Test] Precision: 0.430427 Recall: 0.615268 F1: 0.506511\n",
      "[20231218 01:58:24] [epoch 9] TLoss: 31.393 VLoss: 42.682 TAcc: 0.761 VAcc: 0.684\n",
      "[20231218 02:00:26] [Test] Precision: 0.447319 Recall: 0.635348 F1: 0.525006\n",
      "[20231218 02:00:26] [epoch 10] TLoss: 28.313 VLoss: 39.986 TAcc: 0.770 VAcc: 0.692\n",
      "[20231218 02:02:34] [Test] Precision: 0.450764 Recall: 0.648608 F1: 0.531883\n",
      "[20231218 02:02:34] [epoch 11] TLoss: 25.965 VLoss: 40.955 TAcc: 0.782 VAcc: 0.688\n",
      "[20231218 02:04:36] [Test] Precision: 0.463210 Recall: 0.670203 F1: 0.547805\n",
      "[20231218 02:04:36] [epoch 12] TLoss: 23.964 VLoss: 40.718 TAcc: 0.794 VAcc: 0.688\n",
      "[20231218 02:06:37] [Test] Precision: 0.480374 Recall: 0.672286 F1: 0.560354\n",
      "[20231218 02:06:37] [epoch 13] TLoss: 22.159 VLoss: 38.305 TAcc: 0.806 VAcc: 0.696\n",
      "[20231218 02:08:40] [Test] Precision: 0.485862 Recall: 0.677022 F1: 0.565730\n",
      "[20231218 02:08:40] [epoch 14] TLoss: 20.865 VLoss: 37.911 TAcc: 0.816 VAcc: 0.699\n",
      "[20231218 02:10:40] [Test] Precision: 0.485286 Recall: 0.681000 F1: 0.566722\n",
      "[20231218 02:10:40] [epoch 15] TLoss: 19.816 VLoss: 38.650 TAcc: 0.825 VAcc: 0.695\n",
      "[20231218 02:12:39] [Test] Precision: 0.500965 Recall: 0.688577 F1: 0.579976\n",
      "[20231218 02:12:39] [epoch 16] TLoss: 18.810 VLoss: 37.138 TAcc: 0.833 VAcc: 0.701\n",
      "[20231218 02:14:39] [Test] Precision: 0.500966 Recall: 0.687441 F1: 0.579574\n",
      "[20231218 02:14:39] [epoch 17] TLoss: 17.840 VLoss: 36.883 TAcc: 0.843 VAcc: 0.704\n",
      "[20231218 02:16:39] [Test] Precision: 0.499525 Recall: 0.697291 F1: 0.582068\n",
      "[20231218 02:16:39] [epoch 18] TLoss: 17.140 VLoss: 39.070 TAcc: 0.848 VAcc: 0.701\n",
      "[20231218 02:18:39] [Test] Precision: 0.497708 Recall: 0.699185 F1: 0.581489\n",
      "[20231218 02:18:39] [epoch 19] TLoss: 16.250 VLoss: 38.679 TAcc: 0.854 VAcc: 0.699\n",
      "[20231218 02:20:42] [Test] Precision: 0.502591 Recall: 0.698238 F1: 0.584476\n",
      "[20231218 02:20:42] [epoch 20] TLoss: 15.568 VLoss: 38.595 TAcc: 0.861 VAcc: 0.700\n",
      "[20231218 02:22:42] [Test] Precision: 0.501493 Recall: 0.699943 F1: 0.584328\n",
      "[20231218 02:22:42] [epoch 21] TLoss: 14.767 VLoss: 39.678 TAcc: 0.868 VAcc: 0.695\n",
      "[20231218 02:24:43] [Test] Precision: 0.507452 Recall: 0.702974 F1: 0.589422\n",
      "[20231218 02:24:43] [epoch 22] TLoss: 14.068 VLoss: 37.712 TAcc: 0.873 VAcc: 0.702\n",
      "[20231218 02:24:43] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntity.2e-06.12180224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 02:24:52] [Test] Precision: 0.537024 Recall: 0.609840 F1: 0.571120\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntity model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 02:26:57] [Test] Precision: 0.132809 Recall: 0.163099 F1: 0.146404\n",
      "[20231218 02:26:57] [epoch 1] TLoss: 809.783 VLoss: 271.051 TAcc: 0.413 VAcc: 0.523\n",
      "[20231218 02:29:00] [Test] Precision: 0.290849 Recall: 0.384732 F1: 0.331267\n",
      "[20231218 02:29:00] [epoch 2] TLoss: 143.062 VLoss: 91.430 TAcc: 0.569 VAcc: 0.616\n",
      "[20231218 02:31:02] [Test] Precision: 0.342654 Recall: 0.448002 F1: 0.388310\n",
      "[20231218 02:31:02] [epoch 3] TLoss: 79.179 VLoss: 69.672 TAcc: 0.648 VAcc: 0.644\n",
      "[20231218 02:33:06] [Test] Precision: 0.396208 Recall: 0.538360 F1: 0.456473\n",
      "[20231218 02:33:06] [epoch 4] TLoss: 63.164 VLoss: 58.412 TAcc: 0.678 VAcc: 0.670\n",
      "[20231218 02:35:09] [Test] Precision: 0.414706 Recall: 0.560902 F1: 0.476850\n",
      "[20231218 02:35:09] [epoch 5] TLoss: 52.839 VLoss: 50.038 TAcc: 0.699 VAcc: 0.683\n",
      "[20231218 02:37:15] [Test] Precision: 0.393213 Recall: 0.537791 F1: 0.454276\n",
      "[20231218 02:37:15] [epoch 6] TLoss: 45.065 VLoss: 46.402 TAcc: 0.716 VAcc: 0.672\n",
      "[20231218 02:39:16] [Test] Precision: 0.401391 Recall: 0.546694 F1: 0.462908\n",
      "[20231218 02:39:16] [epoch 7] TLoss: 39.859 VLoss: 43.429 TAcc: 0.730 VAcc: 0.680\n",
      "[20231218 02:41:16] [Test] Precision: 0.429811 Recall: 0.581739 F1: 0.494366\n",
      "[20231218 02:41:16] [epoch 8] TLoss: 36.658 VLoss: 41.801 TAcc: 0.738 VAcc: 0.677\n",
      "[20231218 02:43:16] [Test] Precision: 0.442186 Recall: 0.593294 F1: 0.506714\n",
      "[20231218 02:43:16] [epoch 9] TLoss: 33.859 VLoss: 39.379 TAcc: 0.751 VAcc: 0.682\n",
      "[20231218 02:45:18] [Test] Precision: 0.452681 Recall: 0.618867 F1: 0.522887\n",
      "[20231218 02:45:18] [epoch 10] TLoss: 31.267 VLoss: 37.740 TAcc: 0.762 VAcc: 0.690\n",
      "[20231218 02:47:22] [Test] Precision: 0.474188 Recall: 0.647282 F1: 0.547377\n",
      "[20231218 02:47:22] [epoch 11] TLoss: 28.915 VLoss: 35.644 TAcc: 0.774 VAcc: 0.696\n",
      "[20231218 02:49:24] [Test] Precision: 0.469537 Recall: 0.656943 F1: 0.547651\n",
      "[20231218 02:49:24] [epoch 12] TLoss: 26.721 VLoss: 35.429 TAcc: 0.784 VAcc: 0.693\n",
      "[20231218 02:51:25] [Test] Precision: 0.472791 Recall: 0.659973 F1: 0.550917\n",
      "[20231218 02:51:25] [epoch 13] TLoss: 24.678 VLoss: 34.193 TAcc: 0.796 VAcc: 0.695\n",
      "[20231218 02:53:27] [Test] Precision: 0.484707 Recall: 0.669445 F1: 0.562291\n",
      "[20231218 02:53:27] [epoch 14] TLoss: 22.955 VLoss: 32.508 TAcc: 0.804 VAcc: 0.702\n",
      "[20231218 02:55:31] [Test] Precision: 0.483358 Recall: 0.673991 F1: 0.562975\n",
      "[20231218 02:55:31] [epoch 15] TLoss: 21.248 VLoss: 33.495 TAcc: 0.814 VAcc: 0.692\n",
      "[20231218 02:57:30] [Test] Precision: 0.502358 Recall: 0.686115 F1: 0.580030\n",
      "[20231218 02:57:30] [epoch 16] TLoss: 19.761 VLoss: 32.333 TAcc: 0.825 VAcc: 0.701\n",
      "[20231218 02:59:31] [Test] Precision: 0.498228 Recall: 0.692555 F1: 0.579536\n",
      "[20231218 02:59:31] [epoch 17] TLoss: 18.453 VLoss: 32.873 TAcc: 0.837 VAcc: 0.697\n",
      "[20231218 03:01:32] [Test] Precision: 0.496943 Recall: 0.692745 F1: 0.578731\n",
      "[20231218 03:01:32] [epoch 18] TLoss: 17.464 VLoss: 33.704 TAcc: 0.844 VAcc: 0.692\n",
      "[20231218 03:03:35] [Test] Precision: 0.500272 Recall: 0.696155 F1: 0.582178\n",
      "[20231218 03:03:35] [epoch 19] TLoss: 16.390 VLoss: 34.048 TAcc: 0.851 VAcc: 0.696\n",
      "[20231218 03:03:35] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntity.2e-06.12180303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 03:03:44] [Test] Precision: 0.518741 Recall: 0.596676 F1: 0.554986\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntity model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 03:05:46] [Test] Precision: 0.154576 Recall: 0.194544 F1: 0.172272\n",
      "[20231218 03:05:46] [epoch 1] TLoss: 790.098 VLoss: 205.415 TAcc: 0.371 VAcc: 0.542\n",
      "[20231218 03:07:51] [Test] Precision: 0.354638 Recall: 0.430953 F1: 0.389088\n",
      "[20231218 03:07:51] [epoch 2] TLoss: 122.032 VLoss: 95.042 TAcc: 0.592 VAcc: 0.662\n",
      "[20231218 03:09:51] [Test] Precision: 0.398464 Recall: 0.530782 F1: 0.455203\n",
      "[20231218 03:09:51] [epoch 3] TLoss: 74.773 VLoss: 71.568 TAcc: 0.662 VAcc: 0.684\n",
      "[20231218 03:11:51] [Test] Precision: 0.444115 Recall: 0.595378 F1: 0.508741\n",
      "[20231218 03:11:51] [epoch 4] TLoss: 55.098 VLoss: 56.263 TAcc: 0.691 VAcc: 0.697\n",
      "[20231218 03:13:53] [Test] Precision: 0.461617 Recall: 0.595757 F1: 0.520179\n",
      "[20231218 03:13:53] [epoch 5] TLoss: 43.656 VLoss: 48.436 TAcc: 0.715 VAcc: 0.700\n",
      "[20231218 03:15:54] [Test] Precision: 0.458397 Recall: 0.620951 F1: 0.527434\n",
      "[20231218 03:15:54] [epoch 6] TLoss: 37.298 VLoss: 46.501 TAcc: 0.731 VAcc: 0.695\n",
      "[20231218 03:17:56] [Test] Precision: 0.482155 Recall: 0.647471 F1: 0.552717\n",
      "[20231218 03:17:56] [epoch 7] TLoss: 32.888 VLoss: 42.058 TAcc: 0.744 VAcc: 0.703\n",
      "[20231218 03:20:01] [Test] Precision: 0.488590 Recall: 0.652965 F1: 0.558943\n",
      "[20231218 03:20:01] [epoch 8] TLoss: 29.790 VLoss: 39.076 TAcc: 0.758 VAcc: 0.703\n",
      "[20231218 03:22:05] [Test] Precision: 0.490579 Recall: 0.655995 F1: 0.561355\n",
      "[20231218 03:22:05] [epoch 9] TLoss: 27.640 VLoss: 38.135 TAcc: 0.768 VAcc: 0.702\n",
      "[20231218 03:24:04] [Test] Precision: 0.493527 Recall: 0.657132 F1: 0.563698\n",
      "[20231218 03:24:04] [epoch 10] TLoss: 25.870 VLoss: 37.099 TAcc: 0.778 VAcc: 0.702\n",
      "[20231218 03:26:09] [Test] Precision: 0.494131 Recall: 0.661868 F1: 0.565830\n",
      "[20231218 03:26:09] [epoch 11] TLoss: 24.286 VLoss: 36.050 TAcc: 0.786 VAcc: 0.698\n",
      "[20231218 03:28:12] [Test] Precision: 0.487805 Recall: 0.663004 F1: 0.562068\n",
      "[20231218 03:28:12] [epoch 12] TLoss: 22.782 VLoss: 36.228 TAcc: 0.794 VAcc: 0.693\n",
      "[20231218 03:30:13] [Test] Precision: 0.495184 Recall: 0.662247 F1: 0.566659\n",
      "[20231218 03:30:13] [epoch 13] TLoss: 21.403 VLoss: 34.757 TAcc: 0.804 VAcc: 0.704\n",
      "[20231218 03:32:16] [Test] Precision: 0.499434 Recall: 0.668498 F1: 0.571729\n",
      "[20231218 03:32:16] [epoch 14] TLoss: 20.211 VLoss: 34.028 TAcc: 0.814 VAcc: 0.702\n",
      "[20231218 03:34:19] [Test] Precision: 0.500633 Recall: 0.674181 F1: 0.574588\n",
      "[20231218 03:34:19] [epoch 15] TLoss: 19.014 VLoss: 34.557 TAcc: 0.822 VAcc: 0.700\n",
      "[20231218 03:36:24] [Test] Precision: 0.500413 Recall: 0.688388 F1: 0.579539\n",
      "[20231218 03:36:24] [epoch 16] TLoss: 18.063 VLoss: 35.334 TAcc: 0.829 VAcc: 0.698\n",
      "[20231218 03:38:25] [Test] Precision: 0.496774 Recall: 0.685547 F1: 0.576090\n",
      "[20231218 03:38:25] [epoch 17] TLoss: 16.938 VLoss: 34.784 TAcc: 0.840 VAcc: 0.701\n",
      "[20231218 03:40:28] [Test] Precision: 0.514096 Recall: 0.690851 F1: 0.589509\n",
      "[20231218 03:40:28] [epoch 18] TLoss: 15.994 VLoss: 33.747 TAcc: 0.847 VAcc: 0.709\n",
      "[20231218 03:42:31] [Test] Precision: 0.511108 Recall: 0.692934 F1: 0.588292\n",
      "[20231218 03:42:31] [epoch 19] TLoss: 15.114 VLoss: 34.039 TAcc: 0.853 VAcc: 0.703\n",
      "[20231218 03:44:31] [Test] Precision: 0.501634 Recall: 0.698049 F1: 0.583762\n",
      "[20231218 03:44:31] [epoch 20] TLoss: 14.394 VLoss: 36.444 TAcc: 0.859 VAcc: 0.694\n",
      "[20231218 03:46:35] [Test] Precision: 0.499730 Recall: 0.702216 F1: 0.583917\n",
      "[20231218 03:46:35] [epoch 21] TLoss: 13.695 VLoss: 37.314 TAcc: 0.867 VAcc: 0.683\n",
      "[20231218 03:48:35] [Test] Precision: 0.512205 Recall: 0.699564 F1: 0.591400\n",
      "[20231218 03:48:35] [epoch 22] TLoss: 13.087 VLoss: 34.735 TAcc: 0.872 VAcc: 0.706\n",
      "[20231218 03:50:39] [Test] Precision: 0.499932 Recall: 0.698238 F1: 0.582675\n",
      "[20231218 03:50:39] [epoch 23] TLoss: 12.561 VLoss: 37.409 TAcc: 0.878 VAcc: 0.685\n",
      "[20231218 03:50:39] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntity.2e-06.12180350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 03:50:47] [Test] Precision: 0.525154 Recall: 0.602929 F1: 0.561361\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model_crf_ent = BertCRFEntity(args, tag_dict).to(device)\n",
    "    train_crf(model_crf_ent, epochs = 100, lr = 2e-6)\n",
    "    evaluate(model_crf_ent, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c151cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6abbc0ed",
   "metadata": {},
   "source": [
    "## TransE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed09b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransE Embedding\n",
    "path = '/home/gene/Documents/Senti/Comment/CommentTransE/result/'\n",
    "def load_transe_embeds(model_time):\n",
    "    import pickle\n",
    "    ent_dict, ent_emb = pickle.load(open(path + 'transe_ent_emb.{}.pkl'.format(model_time), 'rb'))\n",
    "    ent_emb_dict = {ent: ent_emb[ent_dict[ent]] for ent in ent_dict}\n",
    "    print('Load TransE embedding: transe_ent_emb.{}.pkl ent_emb: {}'.format(model_time, ent_emb.shape))\n",
    "    return ent_dict, ent_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65504ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load TransE embedding: transe_ent_emb.12101726.pkl ent_emb: torch.Size([24383, 768])\n"
     ]
    }
   ],
   "source": [
    "# TransE Embedding\n",
    "model_time = '12101726'\n",
    "ent_dict, ent_emb = load_transe_embeds(model_time)\n",
    "ent_emb.requires_grad = False\n",
    "# # Bert TransE Embedding\n",
    "# model_time = '12111305'\n",
    "# ent_dict, ent_emb = load_transe_embeds(model_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfb4ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def seg_words(text):\n",
    "    seg = list(jieba.cut(text))\n",
    "    return seg\n",
    "\n",
    "# 对于每个target，同时使用Top tail entity进行加权, [entity_emb; avg(tail_emb)]\n",
    "def embedByTransE(text, max_len = 40):\n",
    "    words = seg_words(text)\n",
    "    embeds = torch.zeros((max_len, 768), dtype = torch.float32) # sen_len, emb_size\n",
    "    pos = 1 # [CLS]\n",
    "    for word in words:\n",
    "        if word in ent_dict:\n",
    "            embeds[pos: pos + len(word)] += ent_emb[ent_dict[word]]\n",
    "        if word in ent_dict:\n",
    "            embeds[pos: pos + len(word)] += ent_emb[ent_dict[word]]\n",
    "        pos += len(word)\n",
    "    return embeds\n",
    "\n",
    "class BertCRFTransE(nn.Module):\n",
    "    def __init__(self, args, tag_dict):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.cls = nn.Linear(args['bert_out_dim'] * 2, args['n_class'])\n",
    "        self.crf = CRF({tag_dict[tag]: tag for tag in tag_dict})\n",
    "        \n",
    "    # 比起BertCRFEntity只改了ent_layer\n",
    "    def ent_layer(self, texts, bert_out):\n",
    "        ent_embeds = [embedByTransE(text, max_len = bert_out.shape[1]) for text in texts]\n",
    "        ent_embeds = torch.stack(ent_embeds)\n",
    "        return ent_embeds.cuda()\n",
    "    \n",
    "    def forward(self, **batch):\n",
    "        bert_out = self.bert(batch['tokens'], batch['masks'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        ent_embeds = self.ent_layer(batch['texts'], bert_out)\n",
    "        bert_out = torch.cat((bert_out, ent_embeds), dim = -1)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        log_likelihood = self.crf(cls_out, batch['labels'], batch['masks'])\n",
    "        n_batch = batch['labels'].shape[0]\n",
    "        loss = -log_likelihood / n_batch\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, **batch):\n",
    "        bert_out = self.bert(batch['tokens'], batch['masks'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        # bert_out = self.bert(batch['tokens'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        ent_embeds = self.ent_layer(batch['texts'], bert_out)\n",
    "        bert_out = torch.cat((bert_out, ent_embeds), dim = -1)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        predict = self.crf.viterbi_tags(cls_out, batch['masks'])\n",
    "        return predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59f59456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 11:46:41] [Test] Precision: 0.394724 Recall: 0.297594 F1: 0.339346\n",
      "[20231218 11:46:41] [epoch 1] TLoss: 32.154 VLoss: 22.659 TAcc: 0.517 VAcc: 0.692\n",
      "[20231218 11:48:36] [Test] Precision: 0.480583 Recall: 0.593105 F1: 0.530948\n",
      "[20231218 11:48:36] [epoch 2] TLoss: 20.928 VLoss: 19.671 TAcc: 0.688 VAcc: 0.707\n",
      "[20231218 11:50:33] [Test] Precision: 0.505611 Recall: 0.648608 F1: 0.568252\n",
      "[20231218 11:50:33] [epoch 3] TLoss: 18.027 VLoss: 18.860 TAcc: 0.722 VAcc: 0.713\n",
      "[20231218 11:52:29] [Test] Precision: 0.524081 Recall: 0.680242 F1: 0.592037\n",
      "[20231218 11:52:29] [epoch 4] TLoss: 16.686 VLoss: 18.181 TAcc: 0.740 VAcc: 0.719\n",
      "[20231218 11:54:25] [Test] Precision: 0.531937 Recall: 0.692555 F1: 0.601712\n",
      "[20231218 11:54:25] [epoch 5] TLoss: 15.823 VLoss: 18.074 TAcc: 0.748 VAcc: 0.720\n",
      "[20231218 11:56:22] [Test] Precision: 0.537971 Recall: 0.703163 F1: 0.609574\n",
      "[20231218 11:56:22] [epoch 6] TLoss: 15.283 VLoss: 18.176 TAcc: 0.756 VAcc: 0.718\n",
      "[20231218 11:58:18] [Test] Precision: 0.541606 Recall: 0.705247 F1: 0.612688\n",
      "[20231218 11:58:18] [epoch 7] TLoss: 14.812 VLoss: 18.011 TAcc: 0.764 VAcc: 0.719\n",
      "[20231218 12:00:12] [Test] Precision: 0.546311 Recall: 0.702785 F1: 0.614747\n",
      "[20231218 12:00:12] [epoch 8] TLoss: 14.403 VLoss: 17.796 TAcc: 0.767 VAcc: 0.722\n",
      "[20231218 12:02:09] [Test] Precision: 0.547633 Recall: 0.705626 F1: 0.616671\n",
      "[20231218 12:02:09] [epoch 9] TLoss: 14.156 VLoss: 17.916 TAcc: 0.772 VAcc: 0.718\n",
      "[20231218 12:04:04] [Test] Precision: 0.547345 Recall: 0.702974 F1: 0.615474\n",
      "[20231218 12:04:04] [epoch 10] TLoss: 13.856 VLoss: 17.944 TAcc: 0.778 VAcc: 0.718\n",
      "[20231218 12:05:59] [Test] Precision: 0.545963 Recall: 0.702027 F1: 0.614237\n",
      "[20231218 12:05:59] [epoch 11] TLoss: 13.553 VLoss: 18.238 TAcc: 0.784 VAcc: 0.713\n",
      "[20231218 12:07:53] [Test] Precision: 0.545294 Recall: 0.701269 F1: 0.613523\n",
      "[20231218 12:07:53] [epoch 12] TLoss: 13.257 VLoss: 18.295 TAcc: 0.789 VAcc: 0.712\n",
      "[20231218 12:09:49] [Test] Precision: 0.545266 Recall: 0.697102 F1: 0.611906\n",
      "[20231218 12:09:49] [epoch 13] TLoss: 13.013 VLoss: 18.086 TAcc: 0.794 VAcc: 0.715\n",
      "[20231218 12:09:49] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.1e-06.12181209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 12:09:57] [Test] Precision: 0.561087 Recall: 0.587955 F1: 0.574207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5610866834169973, 0.5879545828532848, 0.5742065081381883)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf_transe = BertCRFTransE(args, tag_dict).to(device)\n",
    "train_crf(model_crf_transe, epochs = 100, lr = 1e-6)\n",
    "evaluate(model_crf_transe, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77fcfc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 12:56:39] [Test] Precision: 0.494443 Recall: 0.623603 F1: 0.551562\n",
      "[20231218 12:56:39] [epoch 1] TLoss: 28.866 VLoss: 19.814 TAcc: 0.601 VAcc: 0.734\n",
      "[20231218 12:58:34] [Test] Precision: 0.519938 Recall: 0.701459 F1: 0.597210\n",
      "[20231218 12:58:34] [epoch 2] TLoss: 18.966 VLoss: 18.536 TAcc: 0.726 VAcc: 0.740\n",
      "[20231218 13:00:30] [Test] Precision: 0.523581 Recall: 0.723432 F1: 0.607492\n",
      "[20231218 13:00:30] [epoch 3] TLoss: 16.855 VLoss: 18.734 TAcc: 0.751 VAcc: 0.734\n",
      "[20231218 13:02:24] [Test] Precision: 0.524496 Recall: 0.724001 F1: 0.608308\n",
      "[20231218 13:02:24] [epoch 4] TLoss: 15.846 VLoss: 19.023 TAcc: 0.766 VAcc: 0.728\n",
      "[20231218 13:04:19] [Test] Precision: 0.522498 Recall: 0.725895 F1: 0.607627\n",
      "[20231218 13:04:19] [epoch 5] TLoss: 15.123 VLoss: 19.199 TAcc: 0.777 VAcc: 0.722\n",
      "[20231218 13:06:15] [Test] Precision: 0.530036 Recall: 0.728737 F1: 0.613703\n",
      "[20231218 13:06:15] [epoch 6] TLoss: 14.602 VLoss: 19.004 TAcc: 0.786 VAcc: 0.724\n",
      "[20231218 13:08:11] [Test] Precision: 0.529904 Recall: 0.731767 F1: 0.614687\n",
      "[20231218 13:08:11] [epoch 7] TLoss: 13.984 VLoss: 19.531 TAcc: 0.793 VAcc: 0.719\n",
      "[20231218 13:08:11] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 13:08:19] [Test] Precision: 0.563610 Recall: 0.637156 F1: 0.598131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5636098981076326, 0.6371564916898738, 0.5981308406232786)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf_transe = BertCRFTransE(args, tag_dict).to(device)\n",
    "train_crf(model_crf_transe, epochs = 100, lr = 2e-6)\n",
    "evaluate(model_crf_transe, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0f0d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 13:27:17] [Test] Precision: 0.500373 Recall: 0.635537 F1: 0.559913\n",
      "[20231218 13:27:17] [epoch 1] TLoss: 28.868 VLoss: 19.401 TAcc: 0.600 VAcc: 0.738\n",
      "[20231218 13:29:11] [Test] Precision: 0.516208 Recall: 0.720970 F1: 0.601644\n",
      "[20231218 13:29:11] [epoch 2] TLoss: 18.754 VLoss: 18.761 TAcc: 0.732 VAcc: 0.736\n",
      "[20231218 13:31:06] [Test] Precision: 0.529931 Recall: 0.727789 F1: 0.613297\n",
      "[20231218 13:31:06] [epoch 3] TLoss: 16.740 VLoss: 18.392 TAcc: 0.756 VAcc: 0.738\n",
      "[20231218 13:33:03] [Test] Precision: 0.520701 Recall: 0.731389 F1: 0.608319\n",
      "[20231218 13:33:03] [epoch 4] TLoss: 15.730 VLoss: 19.307 TAcc: 0.768 VAcc: 0.724\n",
      "[20231218 13:34:58] [Test] Precision: 0.523938 Recall: 0.735935 F1: 0.612100\n",
      "[20231218 13:34:58] [epoch 5] TLoss: 15.065 VLoss: 19.166 TAcc: 0.778 VAcc: 0.724\n",
      "[20231218 13:36:52] [Test] Precision: 0.529749 Recall: 0.735367 F1: 0.615848\n",
      "[20231218 13:36:52] [epoch 6] TLoss: 14.531 VLoss: 19.202 TAcc: 0.788 VAcc: 0.723\n",
      "[20231218 13:38:47] [Test] Precision: 0.537359 Recall: 0.738397 F1: 0.622038\n",
      "[20231218 13:38:47] [epoch 7] TLoss: 13.987 VLoss: 19.080 TAcc: 0.797 VAcc: 0.726\n",
      "[20231218 13:40:42] [Test] Precision: 0.538409 Recall: 0.740860 F1: 0.623615\n",
      "[20231218 13:40:42] [epoch 8] TLoss: 13.464 VLoss: 19.511 TAcc: 0.806 VAcc: 0.723\n",
      "[20231218 13:40:42] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 13:40:50] [Test] Precision: 0.564774 Recall: 0.638473 F1: 0.599367\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 13:42:47] [Test] Precision: 0.487865 Recall: 0.632127 F1: 0.550706\n",
      "[20231218 13:42:47] [epoch 1] TLoss: 28.337 VLoss: 20.071 TAcc: 0.605 VAcc: 0.731\n",
      "[20231218 13:44:43] [Test] Precision: 0.510057 Recall: 0.710930 F1: 0.593970\n",
      "[20231218 13:44:43] [epoch 2] TLoss: 18.812 VLoss: 19.320 TAcc: 0.732 VAcc: 0.732\n",
      "[20231218 13:46:40] [Test] Precision: 0.520808 Recall: 0.723054 F1: 0.605489\n",
      "[20231218 13:46:40] [epoch 3] TLoss: 16.888 VLoss: 19.251 TAcc: 0.755 VAcc: 0.729\n",
      "[20231218 13:48:37] [Test] Precision: 0.515510 Recall: 0.727221 F1: 0.603332\n",
      "[20231218 13:48:37] [epoch 4] TLoss: 15.888 VLoss: 19.996 TAcc: 0.768 VAcc: 0.718\n",
      "[20231218 13:50:33] [Test] Precision: 0.523240 Recall: 0.729305 F1: 0.609322\n",
      "[20231218 13:50:33] [epoch 5] TLoss: 15.154 VLoss: 20.025 TAcc: 0.777 VAcc: 0.718\n",
      "[20231218 13:52:28] [Test] Precision: 0.526259 Recall: 0.732715 F1: 0.612558\n",
      "[20231218 13:52:28] [epoch 6] TLoss: 14.527 VLoss: 20.199 TAcc: 0.786 VAcc: 0.718\n",
      "[20231218 13:54:23] [Test] Precision: 0.526465 Recall: 0.736693 F1: 0.614085\n",
      "[20231218 13:54:23] [epoch 7] TLoss: 13.955 VLoss: 20.589 TAcc: 0.796 VAcc: 0.714\n",
      "[20231218 13:54:23] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 13:54:31] [Test] Precision: 0.559527 Recall: 0.638802 F1: 0.596542\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 13:56:28] [Test] Precision: 0.507300 Recall: 0.651639 F1: 0.570481\n",
      "[20231218 13:56:28] [epoch 1] TLoss: 25.718 VLoss: 18.757 TAcc: 0.612 VAcc: 0.734\n",
      "[20231218 13:58:24] [Test] Precision: 0.521799 Recall: 0.718697 F1: 0.604622\n",
      "[20231218 13:58:24] [epoch 2] TLoss: 16.670 VLoss: 17.392 TAcc: 0.736 VAcc: 0.739\n",
      "[20231218 14:00:21] [Test] Precision: 0.524441 Recall: 0.733662 F1: 0.611655\n",
      "[20231218 14:00:21] [epoch 3] TLoss: 14.842 VLoss: 17.555 TAcc: 0.756 VAcc: 0.732\n",
      "[20231218 14:02:16] [Test] Precision: 0.526487 Recall: 0.734230 F1: 0.613243\n",
      "[20231218 14:02:16] [epoch 4] TLoss: 13.970 VLoss: 17.553 TAcc: 0.769 VAcc: 0.728\n",
      "[20231218 14:04:10] [Test] Precision: 0.528585 Recall: 0.733851 F1: 0.614530\n",
      "[20231218 14:04:10] [epoch 5] TLoss: 13.369 VLoss: 17.630 TAcc: 0.777 VAcc: 0.725\n",
      "[20231218 14:06:05] [Test] Precision: 0.527411 Recall: 0.734419 F1: 0.613935\n",
      "[20231218 14:06:05] [epoch 6] TLoss: 12.878 VLoss: 17.941 TAcc: 0.785 VAcc: 0.718\n",
      "[20231218 14:08:01] [Test] Precision: 0.526677 Recall: 0.740481 F1: 0.615542\n",
      "[20231218 14:08:01] [epoch 7] TLoss: 12.399 VLoss: 18.398 TAcc: 0.795 VAcc: 0.713\n",
      "[20231218 14:08:01] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 14:08:09] [Test] Precision: 0.562401 Recall: 0.645878 F1: 0.601256\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 14:10:06] [Test] Precision: 0.488432 Recall: 0.611858 F1: 0.543222\n",
      "[20231218 14:10:06] [epoch 1] TLoss: 27.049 VLoss: 19.034 TAcc: 0.591 VAcc: 0.721\n",
      "[20231218 14:12:02] [Test] Precision: 0.517414 Recall: 0.706384 F1: 0.597309\n",
      "[20231218 14:12:02] [epoch 2] TLoss: 16.650 VLoss: 17.520 TAcc: 0.727 VAcc: 0.734\n",
      "[20231218 14:13:58] [Test] Precision: 0.520022 Recall: 0.728168 F1: 0.606740\n",
      "[20231218 14:13:58] [epoch 3] TLoss: 14.664 VLoss: 17.522 TAcc: 0.754 VAcc: 0.728\n",
      "[20231218 14:15:53] [Test] Precision: 0.525776 Recall: 0.728358 F1: 0.610705\n",
      "[20231218 14:15:53] [epoch 4] TLoss: 13.699 VLoss: 17.213 TAcc: 0.766 VAcc: 0.730\n",
      "[20231218 14:17:49] [Test] Precision: 0.523264 Recall: 0.734988 F1: 0.611312\n",
      "[20231218 14:17:49] [epoch 5] TLoss: 13.085 VLoss: 17.608 TAcc: 0.775 VAcc: 0.723\n",
      "[20231218 14:19:45] [Test] Precision: 0.529871 Recall: 0.732525 F1: 0.614932\n",
      "[20231218 14:19:45] [epoch 6] TLoss: 12.585 VLoss: 17.461 TAcc: 0.785 VAcc: 0.725\n",
      "[20231218 14:21:43] [Test] Precision: 0.532869 Recall: 0.738587 F1: 0.619085\n",
      "[20231218 14:21:43] [epoch 7] TLoss: 12.160 VLoss: 17.680 TAcc: 0.790 VAcc: 0.720\n",
      "[20231218 14:21:43] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 14:21:51] [Test] Precision: 0.561832 Recall: 0.639954 F1: 0.598354\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 14:23:48] [Test] Precision: 0.500451 Recall: 0.630422 F1: 0.557968\n",
      "[20231218 14:23:48] [epoch 1] TLoss: 27.122 VLoss: 19.482 TAcc: 0.615 VAcc: 0.733\n",
      "[20231218 14:25:44] [Test] Precision: 0.516664 Recall: 0.713582 F1: 0.599364\n",
      "[20231218 14:25:44] [epoch 2] TLoss: 17.352 VLoss: 18.405 TAcc: 0.733 VAcc: 0.734\n",
      "[20231218 14:27:40] [Test] Precision: 0.514554 Recall: 0.726653 F1: 0.602482\n",
      "[20231218 14:27:40] [epoch 3] TLoss: 15.449 VLoss: 18.555 TAcc: 0.756 VAcc: 0.726\n",
      "[20231218 14:29:35] [Test] Precision: 0.520604 Recall: 0.725137 F1: 0.606080\n",
      "[20231218 14:29:35] [epoch 4] TLoss: 14.575 VLoss: 18.292 TAcc: 0.769 VAcc: 0.724\n",
      "[20231218 14:31:31] [Test] Precision: 0.519886 Recall: 0.725516 F1: 0.605725\n",
      "[20231218 14:31:31] [epoch 5] TLoss: 13.956 VLoss: 18.588 TAcc: 0.777 VAcc: 0.715\n",
      "[20231218 14:33:26] [Test] Precision: 0.526388 Recall: 0.727410 F1: 0.610784\n",
      "[20231218 14:33:26] [epoch 6] TLoss: 13.420 VLoss: 18.452 TAcc: 0.784 VAcc: 0.715\n",
      "[20231218 14:35:22] [Test] Precision: 0.530059 Recall: 0.729873 F1: 0.614122\n",
      "[20231218 14:35:22] [epoch 7] TLoss: 12.979 VLoss: 18.624 TAcc: 0.794 VAcc: 0.715\n",
      "[20231218 14:35:22] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 14:35:31] [Test] Precision: 0.562382 Recall: 0.637156 F1: 0.597439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56242114 0.63956996 0.59851474]\n"
     ]
    }
   ],
   "source": [
    "metrics = [[0.5636098981076326, 0.6371564916898738, 0.5981308406232786]]\n",
    "for i in range(5):\n",
    "    model_crf_transe = BertCRFTransE(args, tag_dict).to(device)\n",
    "    train_crf(model_crf_transe, epochs = 100, lr = 2e-6)\n",
    "    precision, recall, f1 = evaluate(model_crf_transe, test_loader, tag_list)\n",
    "    metrics.append([precision, recall, f1])\n",
    "metrics = np.array(metrics)\n",
    "print(np.mean(metrics, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630eb60e",
   "metadata": {},
   "source": [
    "### Bert-TransE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d6afd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [3], 'b': [4]}\n"
     ]
    }
   ],
   "source": [
    "a = {'a': [1], 'b': [2]}\n",
    "b = {'a': [3], 'b': [4]}\n",
    "c = a.copy()\n",
    "c.update(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load TransE embedding: transe_ent_emb.12111305.pkl ent_emb: torch.Size([24383, 768])\n"
     ]
    }
   ],
   "source": [
    "# Bert-TransE Embedding\n",
    "model_time = '12111305'\n",
    "ent_dict, ent_emb = load_transe_embeds(model_time)\n",
    "ent_emb.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ca47180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 15:39:19] [Test] Precision: 0.212111 Recall: 0.124077 F1: 0.156567\n",
      "[20231218 15:39:19] [epoch 1] TLoss: 34.044 VLoss: 24.240 TAcc: 0.495 VAcc: 0.659\n",
      "[20231218 15:41:15] [Test] Precision: 0.468386 Recall: 0.564122 F1: 0.511816\n",
      "[20231218 15:41:15] [epoch 2] TLoss: 21.869 VLoss: 20.354 TAcc: 0.669 VAcc: 0.699\n",
      "[20231218 15:43:12] [Test] Precision: 0.493771 Recall: 0.638189 F1: 0.556767\n",
      "[20231218 15:43:12] [epoch 3] TLoss: 18.320 VLoss: 19.480 TAcc: 0.716 VAcc: 0.711\n",
      "[20231218 15:45:09] [Test] Precision: 0.514571 Recall: 0.682326 F1: 0.586693\n",
      "[20231218 15:45:09] [epoch 4] TLoss: 16.837 VLoss: 18.863 TAcc: 0.734 VAcc: 0.719\n",
      "[20231218 15:47:05] [Test] Precision: 0.516827 Recall: 0.692366 F1: 0.591855\n",
      "[20231218 15:47:05] [epoch 5] TLoss: 15.819 VLoss: 18.695 TAcc: 0.744 VAcc: 0.717\n",
      "[20231218 15:49:00] [Test] Precision: 0.519525 Recall: 0.695586 F1: 0.594800\n",
      "[20231218 15:49:00] [epoch 6] TLoss: 15.222 VLoss: 18.458 TAcc: 0.753 VAcc: 0.716\n",
      "[20231218 15:50:55] [Test] Precision: 0.521801 Recall: 0.695965 F1: 0.596429\n",
      "[20231218 15:50:55] [epoch 7] TLoss: 14.702 VLoss: 18.559 TAcc: 0.759 VAcc: 0.713\n",
      "[20231218 15:52:50] [Test] Precision: 0.526074 Recall: 0.695586 F1: 0.599070\n",
      "[20231218 15:52:50] [epoch 8] TLoss: 14.348 VLoss: 18.431 TAcc: 0.764 VAcc: 0.713\n",
      "[20231218 15:54:46] [Test] Precision: 0.528041 Recall: 0.695586 F1: 0.600343\n",
      "[20231218 15:54:46] [epoch 9] TLoss: 13.998 VLoss: 18.244 TAcc: 0.769 VAcc: 0.715\n",
      "[20231218 15:54:46] Early stopping\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:274] . unexpected pos 326401280 vs 326401168",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py:372\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 372\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py:493\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    492\u001b[0m buf_value \u001b[38;5;241m=\u001b[39m buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m--> 493\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_crf_transe2 \u001b[38;5;241m=\u001b[39m BertCRFTransE(args, tag_dict)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_crf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_crf_transe2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m evaluate(model_crf_transe2, test_loader, tag_list)\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mtrain_crf\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m     53\u001b[0m         logger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model\u001b[39m(model, path):\n\u001b[1;32m      3\u001b[0m     ts \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mlocaltime())\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSave model to\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path, ts))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py:373\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    372\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    374\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py:259\u001b[0m, in \u001b[0;36m_open_zipfile_writer_buffer.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:274] . unexpected pos 326401280 vs 326401168"
     ]
    }
   ],
   "source": [
    "model_crf_transe2 = BertCRFTransE(args, tag_dict).to(device)\n",
    "train_crf(model_crf_transe2, epochs = 100, lr = 1e-6)\n",
    "evaluate(model_crf_transe2, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c30f41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.1e-06.12181609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 16:09:55] [Test] Precision: 0.550581 Recall: 0.592891 F1: 0.570953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5505806845964929, 0.5928912292248489, 0.5709531727832182)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model_crf_transe2, './results/BertCRFTransE.{}'.format(1e-6))\n",
    "evaluate(model_crf_transe2, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcc57276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 16:12:05] [Test] Precision: 0.497160 Recall: 0.630044 F1: 0.555769\n",
      "[20231218 16:12:05] [epoch 1] TLoss: 30.590 VLoss: 20.815 TAcc: 0.590 VAcc: 0.738\n",
      "[20231218 16:14:01] [Test] Precision: 0.519537 Recall: 0.705247 F1: 0.598313\n",
      "[20231218 16:14:01] [epoch 2] TLoss: 19.679 VLoss: 20.004 TAcc: 0.728 VAcc: 0.736\n",
      "[20231218 16:15:57] [Test] Precision: 0.521420 Recall: 0.726274 F1: 0.607030\n",
      "[20231218 16:15:57] [epoch 3] TLoss: 17.617 VLoss: 20.280 TAcc: 0.753 VAcc: 0.732\n",
      "[20231218 16:17:52] [Test] Precision: 0.520909 Recall: 0.729115 F1: 0.607673\n",
      "[20231218 16:17:52] [epoch 4] TLoss: 16.549 VLoss: 20.501 TAcc: 0.764 VAcc: 0.725\n",
      "[20231218 16:19:46] [Test] Precision: 0.520297 Recall: 0.730820 F1: 0.607846\n",
      "[20231218 16:19:46] [epoch 5] TLoss: 15.769 VLoss: 20.689 TAcc: 0.776 VAcc: 0.722\n",
      "[20231218 16:21:43] [Test] Precision: 0.523649 Recall: 0.734041 F1: 0.611247\n",
      "[20231218 16:21:43] [epoch 6] TLoss: 15.115 VLoss: 21.053 TAcc: 0.784 VAcc: 0.718\n",
      "[20231218 16:21:43] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 16:21:54] [Test] Precision: 0.556486 Recall: 0.645220 F1: 0.597577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5564859494748003, 0.6452196807634284, 0.5975767731062317)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf_transe2 = BertCRFTransE(args, tag_dict).to(device)\n",
    "train_crf(model_crf_transe2, epochs = 100, lr = 2e-6)\n",
    "evaluate(model_crf_transe2, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ec4537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:03:19] [Test] Precision: 0.478596 Recall: 0.612048 F1: 0.537157\n",
      "[20231218 17:03:19] [epoch 1] TLoss: 29.930 VLoss: 21.592 TAcc: 0.602 VAcc: 0.725\n",
      "[20231218 17:05:14] [Test] Precision: 0.516929 Recall: 0.702785 F1: 0.595697\n",
      "[20231218 17:05:14] [epoch 2] TLoss: 20.043 VLoss: 20.532 TAcc: 0.729 VAcc: 0.734\n",
      "[20231218 17:07:10] [Test] Precision: 0.521443 Recall: 0.725516 F1: 0.606781\n",
      "[20231218 17:07:10] [epoch 3] TLoss: 17.872 VLoss: 20.551 TAcc: 0.753 VAcc: 0.729\n",
      "[20231218 17:09:07] [Test] Precision: 0.521380 Recall: 0.729873 F1: 0.608256\n",
      "[20231218 17:09:07] [epoch 4] TLoss: 16.752 VLoss: 21.089 TAcc: 0.768 VAcc: 0.721\n",
      "[20231218 17:11:03] [Test] Precision: 0.523972 Recall: 0.728737 F1: 0.609619\n",
      "[20231218 17:11:03] [epoch 5] TLoss: 15.915 VLoss: 21.127 TAcc: 0.776 VAcc: 0.719\n",
      "[20231218 17:12:58] [Test] Precision: 0.527698 Recall: 0.734419 F1: 0.614130\n",
      "[20231218 17:12:58] [epoch 6] TLoss: 15.211 VLoss: 21.780 TAcc: 0.789 VAcc: 0.718\n",
      "[20231218 17:14:54] [Test] Precision: 0.529348 Recall: 0.734609 F1: 0.615311\n",
      "[20231218 17:14:54] [epoch 7] TLoss: 14.572 VLoss: 22.050 TAcc: 0.798 VAcc: 0.714\n",
      "[20231218 17:14:54] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:15:06] [Test] Precision: 0.554046 Recall: 0.630903 F1: 0.589982\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:17:03] [Test] Precision: 0.484187 Recall: 0.609017 F1: 0.539475\n",
      "[20231218 17:17:03] [epoch 1] TLoss: 32.794 VLoss: 23.190 TAcc: 0.591 VAcc: 0.735\n",
      "[20231218 17:18:59] [Test] Precision: 0.512625 Recall: 0.699943 F1: 0.591815\n",
      "[20231218 17:18:59] [epoch 2] TLoss: 21.950 VLoss: 22.175 TAcc: 0.728 VAcc: 0.736\n",
      "[20231218 17:20:56] [Test] Precision: 0.522780 Recall: 0.723811 F1: 0.607086\n",
      "[20231218 17:20:56] [epoch 3] TLoss: 19.548 VLoss: 22.272 TAcc: 0.753 VAcc: 0.734\n",
      "[20231218 17:22:52] [Test] Precision: 0.521674 Recall: 0.729494 F1: 0.608325\n",
      "[20231218 17:22:52] [epoch 4] TLoss: 18.219 VLoss: 22.798 TAcc: 0.767 VAcc: 0.728\n",
      "[20231218 17:24:46] [Test] Precision: 0.523139 Recall: 0.732336 F1: 0.610309\n",
      "[20231218 17:24:46] [epoch 5] TLoss: 17.348 VLoss: 22.932 TAcc: 0.778 VAcc: 0.724\n",
      "[20231218 17:26:41] [Test] Precision: 0.525674 Recall: 0.734988 F1: 0.612954\n",
      "[20231218 17:26:41] [epoch 6] TLoss: 16.596 VLoss: 23.293 TAcc: 0.787 VAcc: 0.722\n",
      "[20231218 17:28:36] [Test] Precision: 0.528046 Recall: 0.736503 F1: 0.615093\n",
      "[20231218 17:28:36] [epoch 7] TLoss: 15.938 VLoss: 23.503 TAcc: 0.798 VAcc: 0.723\n",
      "[20231218 17:28:36] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:28:47] [Test] Precision: 0.563297 Recall: 0.639954 F1: 0.599183\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:30:44] [Test] Precision: 0.496396 Recall: 0.626255 F1: 0.553815\n",
      "[20231218 17:30:44] [epoch 1] TLoss: 26.765 VLoss: 18.520 TAcc: 0.608 VAcc: 0.731\n",
      "[20231218 17:32:43] [Test] Precision: 0.511536 Recall: 0.709794 F1: 0.594573\n",
      "[20231218 17:32:43] [epoch 2] TLoss: 17.025 VLoss: 17.816 TAcc: 0.728 VAcc: 0.730\n",
      "[20231218 17:34:39] [Test] Precision: 0.519121 Recall: 0.725137 F1: 0.605074\n",
      "[20231218 17:34:39] [epoch 3] TLoss: 15.107 VLoss: 17.630 TAcc: 0.753 VAcc: 0.730\n",
      "[20231218 17:36:35] [Test] Precision: 0.517149 Recall: 0.731199 F1: 0.605823\n",
      "[20231218 17:36:35] [epoch 4] TLoss: 14.085 VLoss: 17.916 TAcc: 0.767 VAcc: 0.721\n",
      "[20231218 17:38:30] [Test] Precision: 0.520156 Recall: 0.733283 F1: 0.608600\n",
      "[20231218 17:38:30] [epoch 5] TLoss: 13.499 VLoss: 18.026 TAcc: 0.775 VAcc: 0.718\n",
      "[20231218 17:40:25] [Test] Precision: 0.521920 Recall: 0.735177 F1: 0.610460\n",
      "[20231218 17:40:25] [epoch 6] TLoss: 12.978 VLoss: 18.215 TAcc: 0.785 VAcc: 0.717\n",
      "[20231218 17:40:25] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:40:36] [Test] Precision: 0.557391 Recall: 0.638473 F1: 0.595183\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:42:32] [Test] Precision: 0.460446 Recall: 0.626255 F1: 0.530701\n",
      "[20231218 17:42:32] [epoch 1] TLoss: 33.904 VLoss: 23.403 TAcc: 0.593 VAcc: 0.726\n",
      "[20231218 17:44:28] [Test] Precision: 0.497238 Recall: 0.699185 F1: 0.581168\n",
      "[20231218 17:44:28] [epoch 2] TLoss: 23.272 VLoss: 22.632 TAcc: 0.722 VAcc: 0.734\n",
      "[20231218 17:46:24] [Test] Precision: 0.509550 Recall: 0.722675 F1: 0.597681\n",
      "[20231218 17:46:24] [epoch 3] TLoss: 20.820 VLoss: 22.849 TAcc: 0.749 VAcc: 0.731\n",
      "[20231218 17:48:19] [Test] Precision: 0.511448 Recall: 0.727789 F1: 0.600735\n",
      "[20231218 17:48:19] [epoch 4] TLoss: 19.557 VLoss: 23.623 TAcc: 0.764 VAcc: 0.722\n",
      "[20231218 17:50:14] [Test] Precision: 0.516011 Recall: 0.723432 F1: 0.602366\n",
      "[20231218 17:50:14] [epoch 5] TLoss: 18.597 VLoss: 23.655 TAcc: 0.776 VAcc: 0.720\n",
      "[20231218 17:52:08] [Test] Precision: 0.514613 Recall: 0.723811 F1: 0.601543\n",
      "[20231218 17:52:08] [epoch 6] TLoss: 17.778 VLoss: 24.390 TAcc: 0.789 VAcc: 0.714\n",
      "[20231218 17:54:03] [Test] Precision: 0.522886 Recall: 0.724948 F1: 0.607557\n",
      "[20231218 17:54:03] [epoch 7] TLoss: 17.032 VLoss: 24.468 TAcc: 0.796 VAcc: 0.715\n",
      "[20231218 17:54:03] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFTransE.2e-06.12181754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:54:14] [Test] Precision: 0.553685 Recall: 0.633043 F1: 0.590710\n",
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 17:56:10] [Test] Precision: 0.507725 Recall: 0.628718 F1: 0.561781\n",
      "[20231218 17:56:10] [epoch 1] TLoss: 26.552 VLoss: 18.419 TAcc: 0.599 VAcc: 0.739\n",
      "[20231218 17:58:07] [Test] Precision: 0.525695 Recall: 0.709225 F1: 0.603822\n",
      "[20231218 17:58:07] [epoch 2] TLoss: 16.985 VLoss: 17.429 TAcc: 0.726 VAcc: 0.741\n",
      "[20231218 18:00:04] [Test] Precision: 0.526446 Recall: 0.724001 F1: 0.609618\n",
      "[20231218 18:00:04] [epoch 3] TLoss: 15.049 VLoss: 17.448 TAcc: 0.750 VAcc: 0.734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     model_crf_transe2 \u001b[38;5;241m=\u001b[39m BertCRFTransE(args, tag_dict)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_crf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_crf_transe2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     evaluate(model_crf_transe2, test_loader, tag_list)\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mtrain_crf\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch) \u001b[38;5;66;03m# (n_batch, n_token, n_class)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch) \u001b[38;5;66;03m# (n_batch, n_tokens)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Tensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model_crf_transe2 = BertCRFTransE(args, tag_dict).to(device)\n",
    "    train_crf(model_crf_transe2, epochs = 100, lr = 2e-6)\n",
    "    evaluate(model_crf_transe2, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3257c471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFTransE model with lr = 1e-06\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      3\u001b[0m     model_crf_transe2 \u001b[38;5;241m=\u001b[39m BertCRFTransE(args, tag_dict)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain_crf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_crf_transe2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     precision, recall, f1 \u001b[38;5;241m=\u001b[39m evaluate(model_crf_transe2, test_loader, tag_list)\n\u001b[1;32m      6\u001b[0m     metrics\u001b[38;5;241m.\u001b[39mappend([precision, recall, f1])\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_crf\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m     12\u001b[0m valid_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# tokens, masks, labels, texts = batch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# loss = model(tokens, masks, labels) # (n_batch, n_token, n_class)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "for i in range(5):\n",
    "    model_crf_transe2 = BertCRFTransE(args, tag_dict).to(device)\n",
    "    train_crf(model_crf_transe2, epochs = 100, lr = 1e-6)\n",
    "    precision, recall, f1 = evaluate(model_crf_transe2, test_loader, tag_list)\n",
    "    metrics.append([precision, recall, f1])\n",
    "metrics = np.array(metrics)\n",
    "print(np.mean(metrics, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccddae5",
   "metadata": {},
   "source": [
    "### Bert Entity + TransE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "152ce86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load TransE embedding: transe_ent_emb.12111305.pkl ent_emb: torch.Size([24383, 768])\n"
     ]
    }
   ],
   "source": [
    "# Bert-TransE Embedding\n",
    "model_time = '12111305'\n",
    "ent_dict, ent_emb = load_transe_embeds(model_time)\n",
    "ent_emb.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "784cfebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get entity triplet from data_file: /home/gene/Documents/Senti/Comment/knowledgebase/unlabel/deduplicate/data.txt, target_dict: 12122, opinion_dict: 34028\n"
     ]
    }
   ],
   "source": [
    "# emhanced_emb: [entity_emb; avg(tail_emb)]\n",
    "# def enhanceTripEmbed(ent_dict, ent_emb, entity_triplet_dict):\n",
    "#     emb_size = ent_emb.size(-1)\n",
    "#     enhanced_entity_embeds = torch.zeros(len(ent_dict), 2 * emb_size)\n",
    "#     for entity in ent_dict:\n",
    "#         enhanced_entity_embeds[ent_dict[entity], :emb_size] = ent_emb[ent_dict[entity]]\n",
    "#         for tail in entity_triplet_dict[entity]:\n",
    "#             if tail in ent_dict:\n",
    "#                 enhanced_entity_embeds[ent_dict[entity], emb_size:] += ent_emb[ent_dict[tail]]\n",
    "#     return enhanced_entity_embeds\n",
    "\n",
    "def enhanceTripEmbed2(ent_dict, ent_emb, target_triplet_dict, opinion_triplet_dict):\n",
    "    emb_size = ent_emb.size(-1)\n",
    "    enhanced_entity_embeds = torch.zeros(len(ent_dict), 2 * emb_size)\n",
    "    for entity in ent_dict:\n",
    "        enhanced_entity_embeds[ent_dict[entity], :emb_size] = ent_emb[ent_dict[entity]]\n",
    "        if entity in target_triplet_dict:\n",
    "            for tail in target_triplet_dict[entity]:\n",
    "                if tail in ent_dict:\n",
    "                    enhanced_entity_embeds[ent_dict[entity], emb_size:] += ent_emb[ent_dict[tail]]\n",
    "        if entity in opinion_triplet_dict:\n",
    "            for tail in opinion_triplet_dict[entity]:\n",
    "                if tail in ent_dict:\n",
    "                    enhanced_entity_embeds[ent_dict[entity], emb_size:] += ent_emb[ent_dict[tail]]\n",
    "    return enhanced_entity_embeds\n",
    "\n",
    "from JDComment_seg.code_kg.kg_embed import getAllEntityTriplet\n",
    "from_file = '/home/gene/Documents/Senti/Comment/knowledgebase/unlabel/deduplicate/data.txt'\n",
    "target_triplet_dict, opinion_triplet_dict = getAllEntityTriplet(from_file)\n",
    "enhanced_entity_embeds = enhanceTripEmbed2(ent_dict, ent_emb, target_triplet_dict, opinion_triplet_dict)\n",
    "enhanced_entity_embeds.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a0e085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def seg_words(text):\n",
    "    seg = list(jieba.cut(text))\n",
    "    return seg\n",
    "\n",
    "# 对于每个target，同时使用Top tail entity进行加权, [entity_emb; avg(tail_emb)]\n",
    "def embedByEntityTransE(text, max_len = 40):\n",
    "    words = seg_words(text)\n",
    "    embeds = torch.zeros((max_len, 768 * 2), dtype = torch.float32) # sen_len, emb_size\n",
    "    pos = 1 # [CLS]\n",
    "    for word in words:\n",
    "        if word in ent_dict:\n",
    "            # embeds[pos: pos + len(word)] += ent_emb[ent_dict[word]]\n",
    "            embeds[pos: pos + len(word)] += enhanced_entity_embeds[ent_dict[word]]\n",
    "        pos += len(word)\n",
    "    return embeds\n",
    "\n",
    "class BertCRFEntityTransE(nn.Module):\n",
    "    def __init__(self, args, tag_dict):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('/data/pretrained/bert-base-chinese/')\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.cls = nn.Linear(args['bert_out_dim'] * 3, args['n_class'])\n",
    "        self.crf = CRF({tag_dict[tag]: tag for tag in tag_dict})\n",
    "        \n",
    "    # 比起BertCRFEntity只改了ent_layer\n",
    "    def ent_layer(self, texts, bert_out):\n",
    "        ent_embeds = [embedByEntityTransE(text, max_len = bert_out.shape[1]) for text in texts]\n",
    "        ent_embeds = torch.stack(ent_embeds)\n",
    "        return ent_embeds.cuda()\n",
    "    \n",
    "    def forward(self, **batch):\n",
    "        bert_out = self.bert(batch['tokens'], batch['masks'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        ent_embeds = self.ent_layer(batch['texts'], bert_out)\n",
    "        bert_out = torch.cat((bert_out, ent_embeds), dim = -1)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        log_likelihood = self.crf(cls_out, batch['labels'], batch['masks'])\n",
    "        n_batch = batch['labels'].shape[0]\n",
    "        loss = -log_likelihood / n_batch\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, **batch):\n",
    "        bert_out = self.bert(batch['tokens'], batch['masks'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        # bert_out = self.bert(batch['tokens'])['last_hidden_state'] # (n_batch, n_tokens, n_emb)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        ent_embeds = self.ent_layer(batch['texts'], bert_out)\n",
    "        bert_out = torch.cat((bert_out, ent_embeds), dim = -1)\n",
    "        cls_out = self.cls(bert_out)\n",
    "        predict = self.crf.viterbi_tags(cls_out, batch['masks'])\n",
    "        return predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f741163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntityTransE model with lr = 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 21:31:52] [Test] Precision: 0.020844 Recall: 0.038075 F1: 0.026940\n",
      "[20231218 21:31:52] [epoch 1] TLoss: 1329.824 VLoss: 874.195 TAcc: 0.314 VAcc: 0.443\n",
      "[20231218 21:33:53] [Test] Precision: 0.128600 Recall: 0.207236 F1: 0.158712\n",
      "[20231218 21:33:53] [epoch 2] TLoss: 562.352 VLoss: 426.927 TAcc: 0.426 VAcc: 0.512\n",
      "[20231218 21:35:55] [Test] Precision: 0.207278 Recall: 0.308581 F1: 0.247983\n",
      "[20231218 21:35:55] [epoch 3] TLoss: 281.861 VLoss: 199.742 TAcc: 0.511 VAcc: 0.559\n",
      "[20231218 21:37:58] [Test] Precision: 0.261586 Recall: 0.354991 F1: 0.301214\n",
      "[20231218 21:37:58] [epoch 4] TLoss: 150.947 VLoss: 136.761 TAcc: 0.582 VAcc: 0.606\n",
      "[20231218 21:40:02] [Test] Precision: 0.309471 Recall: 0.425838 F1: 0.358447\n",
      "[20231218 21:40:02] [epoch 5] TLoss: 117.041 VLoss: 119.105 TAcc: 0.612 VAcc: 0.624\n",
      "[20231218 21:42:03] [Test] Precision: 0.339384 Recall: 0.456905 F1: 0.389472\n",
      "[20231218 21:42:03] [epoch 6] TLoss: 99.294 VLoss: 105.450 TAcc: 0.631 VAcc: 0.639\n",
      "[20231218 21:44:07] [Test] Precision: 0.371348 Recall: 0.500852 F1: 0.426486\n",
      "[20231218 21:44:07] [epoch 7] TLoss: 85.449 VLoss: 93.765 TAcc: 0.648 VAcc: 0.649\n",
      "[20231218 21:46:09] [Test] Precision: 0.384419 Recall: 0.512218 F1: 0.439211\n",
      "[20231218 21:46:09] [epoch 8] TLoss: 75.773 VLoss: 85.856 TAcc: 0.664 VAcc: 0.656\n",
      "[20231218 21:48:10] [Test] Precision: 0.391850 Recall: 0.520932 F1: 0.447264\n",
      "[20231218 21:48:10] [epoch 9] TLoss: 68.642 VLoss: 79.145 TAcc: 0.678 VAcc: 0.658\n",
      "[20231218 21:50:11] [Test] Precision: 0.400829 Recall: 0.531351 F1: 0.456952\n",
      "[20231218 21:50:11] [epoch 10] TLoss: 63.266 VLoss: 73.910 TAcc: 0.687 VAcc: 0.663\n",
      "[20231218 21:52:13] [Test] Precision: 0.408818 Recall: 0.541012 F1: 0.465715\n",
      "[20231218 21:52:13] [epoch 11] TLoss: 58.977 VLoss: 69.954 TAcc: 0.695 VAcc: 0.663\n",
      "[20231218 21:54:20] [Test] Precision: 0.417301 Recall: 0.550104 F1: 0.474587\n",
      "[20231218 21:54:20] [epoch 12] TLoss: 55.203 VLoss: 66.189 TAcc: 0.707 VAcc: 0.667\n",
      "[20231218 21:56:23] [Test] Precision: 0.422962 Recall: 0.561091 F1: 0.482332\n",
      "[20231218 21:56:23] [epoch 13] TLoss: 51.710 VLoss: 63.344 TAcc: 0.712 VAcc: 0.670\n",
      "[20231218 21:58:28] [Test] Precision: 0.427287 Recall: 0.567153 F1: 0.487384\n",
      "[20231218 21:58:28] [epoch 14] TLoss: 48.523 VLoss: 60.281 TAcc: 0.721 VAcc: 0.673\n",
      "[20231218 22:00:32] [Test] Precision: 0.429668 Recall: 0.572836 F1: 0.491029\n",
      "[20231218 22:00:32] [epoch 15] TLoss: 45.721 VLoss: 58.412 TAcc: 0.730 VAcc: 0.673\n",
      "[20231218 22:02:35] [Test] Precision: 0.435122 Recall: 0.580602 F1: 0.497444\n",
      "[20231218 22:02:35] [epoch 16] TLoss: 43.241 VLoss: 56.565 TAcc: 0.738 VAcc: 0.674\n",
      "[20231218 22:04:38] [Test] Precision: 0.439459 Recall: 0.584391 F1: 0.501667\n",
      "[20231218 22:04:38] [epoch 17] TLoss: 41.090 VLoss: 53.800 TAcc: 0.742 VAcc: 0.677\n",
      "[20231218 22:06:42] [Test] Precision: 0.442341 Recall: 0.588558 F1: 0.505080\n",
      "[20231218 22:06:42] [epoch 18] TLoss: 38.917 VLoss: 52.201 TAcc: 0.749 VAcc: 0.678\n",
      "[20231218 22:08:44] [Test] Precision: 0.446601 Recall: 0.597272 F1: 0.511062\n",
      "[20231218 22:08:44] [epoch 19] TLoss: 37.099 VLoss: 50.749 TAcc: 0.751 VAcc: 0.678\n",
      "[20231218 22:10:48] [Test] Precision: 0.447673 Recall: 0.601250 F1: 0.513219\n",
      "[20231218 22:10:48] [epoch 20] TLoss: 35.351 VLoss: 49.729 TAcc: 0.761 VAcc: 0.677\n",
      "[20231218 22:12:51] [Test] Precision: 0.450192 Recall: 0.600114 F1: 0.514453\n",
      "[20231218 22:12:51] [epoch 21] TLoss: 33.769 VLoss: 48.147 TAcc: 0.764 VAcc: 0.678\n",
      "[20231218 22:14:55] [Test] Precision: 0.452266 Recall: 0.604849 F1: 0.517546\n",
      "[20231218 22:14:55] [epoch 22] TLoss: 32.147 VLoss: 47.038 TAcc: 0.771 VAcc: 0.678\n",
      "[20231218 22:17:00] [Test] Precision: 0.455752 Recall: 0.604849 F1: 0.519821\n",
      "[20231218 22:17:00] [epoch 23] TLoss: 30.583 VLoss: 46.021 TAcc: 0.777 VAcc: 0.677\n",
      "[20231218 22:19:06] [Test] Precision: 0.456407 Recall: 0.607880 F1: 0.521365\n",
      "[20231218 22:19:06] [epoch 24] TLoss: 29.350 VLoss: 45.295 TAcc: 0.782 VAcc: 0.677\n",
      "[20231218 22:19:06] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntityTransE.1e-06.12182219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 22:19:18] [Test] Precision: 0.494712 Recall: 0.554221 F1: 0.522778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4947121034076829, 0.5542208326475968, 0.5227784240261691)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf_ent_transe = BertCRFEntityTransE(args, tag_dict).to(device)\n",
    "train_crf(model_crf_ent_transe, epochs = 100, lr = 1e-6)\n",
    "evaluate(model_crf_ent_transe, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntityTransE model with lr = 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 22:57:15] [Test] Precision: 0.062915 Recall: 0.089790 F1: 0.073987\n",
      "[20231218 22:57:15] [epoch 1] TLoss: 1050.945 VLoss: 570.942 TAcc: 0.461 VAcc: 0.556\n",
      "[20231218 22:59:19] [Test] Precision: 0.238647 Recall: 0.300625 F1: 0.266074\n",
      "[20231218 22:59:19] [epoch 2] TLoss: 357.027 VLoss: 298.722 TAcc: 0.567 VAcc: 0.622\n",
      "[20231218 23:01:24] [Test] Precision: 0.313330 Recall: 0.397613 F1: 0.350476\n",
      "[20231218 23:01:24] [epoch 3] TLoss: 207.902 VLoss: 198.023 TAcc: 0.621 VAcc: 0.641\n",
      "[20231218 23:03:30] [Test] Precision: 0.355718 Recall: 0.442508 F1: 0.394395\n",
      "[20231218 23:03:30] [epoch 4] TLoss: 156.739 VLoss: 141.468 TAcc: 0.654 VAcc: 0.660\n",
      "[20231218 23:05:35] [Test] Precision: 0.396728 Recall: 0.500663 F1: 0.442676\n",
      "[20231218 23:05:35] [epoch 5] TLoss: 118.917 VLoss: 101.449 TAcc: 0.677 VAcc: 0.675\n",
      "[20231218 23:07:39] [Test] Precision: 0.399143 Recall: 0.529267 F1: 0.455086\n",
      "[20231218 23:07:39] [epoch 6] TLoss: 94.359 VLoss: 87.631 TAcc: 0.694 VAcc: 0.678\n",
      "[20231218 23:09:41] [Test] Precision: 0.407782 Recall: 0.549915 F1: 0.468301\n",
      "[20231218 23:09:41] [epoch 7] TLoss: 76.132 VLoss: 74.648 TAcc: 0.709 VAcc: 0.674\n",
      "[20231218 23:11:44] [Test] Precision: 0.420314 Recall: 0.573025 F1: 0.484931\n",
      "[20231218 23:11:44] [epoch 8] TLoss: 60.029 VLoss: 64.679 TAcc: 0.722 VAcc: 0.677\n",
      "[20231218 23:13:48] [Test] Precision: 0.423040 Recall: 0.577382 F1: 0.488305\n",
      "[20231218 23:13:48] [epoch 9] TLoss: 49.431 VLoss: 57.069 TAcc: 0.735 VAcc: 0.681\n",
      "[20231218 23:15:50] [Test] Precision: 0.446147 Recall: 0.606554 F1: 0.514130\n",
      "[20231218 23:15:50] [epoch 10] TLoss: 41.450 VLoss: 52.004 TAcc: 0.751 VAcc: 0.686\n",
      "[20231218 23:17:55] [Test] Precision: 0.453328 Recall: 0.609017 F1: 0.519764\n",
      "[20231218 23:17:55] [epoch 11] TLoss: 36.301 VLoss: 49.529 TAcc: 0.766 VAcc: 0.690\n",
      "[20231218 23:19:57] [Test] Precision: 0.460954 Recall: 0.620572 F1: 0.528984\n",
      "[20231218 23:19:57] [epoch 12] TLoss: 32.951 VLoss: 47.486 TAcc: 0.778 VAcc: 0.692\n",
      "[20231218 23:21:59] [Test] Precision: 0.453423 Recall: 0.626066 F1: 0.525939\n",
      "[20231218 23:21:59] [epoch 13] TLoss: 30.096 VLoss: 47.654 TAcc: 0.789 VAcc: 0.683\n",
      "[20231218 23:24:02] [Test] Precision: 0.456002 Recall: 0.625308 F1: 0.527401\n",
      "[20231218 23:24:02] [epoch 14] TLoss: 27.583 VLoss: 46.529 TAcc: 0.802 VAcc: 0.683\n",
      "[20231218 23:26:02] [Test] Precision: 0.469336 Recall: 0.630612 F1: 0.538151\n",
      "[20231218 23:26:02] [epoch 15] TLoss: 25.521 VLoss: 44.208 TAcc: 0.807 VAcc: 0.689\n",
      "[20231218 23:28:08] [Test] Precision: 0.471167 Recall: 0.643872 F1: 0.544145\n",
      "[20231218 23:28:08] [epoch 16] TLoss: 23.491 VLoss: 45.688 TAcc: 0.819 VAcc: 0.686\n",
      "[20231218 23:30:12] [Test] Precision: 0.478004 Recall: 0.644251 F1: 0.548814\n",
      "[20231218 23:30:12] [epoch 17] TLoss: 21.818 VLoss: 42.693 TAcc: 0.826 VAcc: 0.696\n",
      "[20231218 23:32:14] [Test] Precision: 0.483019 Recall: 0.654669 F1: 0.555895\n",
      "[20231218 23:32:14] [epoch 18] TLoss: 20.240 VLoss: 43.509 TAcc: 0.837 VAcc: 0.691\n",
      "[20231218 23:34:13] [Test] Precision: 0.486888 Recall: 0.664709 F1: 0.562070\n",
      "[20231218 23:34:13] [epoch 19] TLoss: 18.893 VLoss: 43.177 TAcc: 0.845 VAcc: 0.695\n",
      "[20231218 23:36:14] [Test] Precision: 0.484698 Recall: 0.666035 F1: 0.561079\n",
      "[20231218 23:36:14] [epoch 20] TLoss: 17.657 VLoss: 42.957 TAcc: 0.852 VAcc: 0.689\n",
      "[20231218 23:38:16] [Test] Precision: 0.487332 Recall: 0.666793 F1: 0.563110\n",
      "[20231218 23:38:16] [epoch 21] TLoss: 16.495 VLoss: 43.126 TAcc: 0.860 VAcc: 0.695\n",
      "[20231218 23:40:20] [Test] Precision: 0.480966 Recall: 0.667740 F1: 0.559169\n",
      "[20231218 23:40:20] [epoch 22] TLoss: 15.620 VLoss: 43.595 TAcc: 0.866 VAcc: 0.691\n",
      "[20231218 23:40:20] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntityTransE.2e-06.12182340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 23:40:32] [Test] Precision: 0.514278 Recall: 0.595689 F1: 0.551998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5142775962493942, 0.5956886621687353, 0.5519975597343835)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf_ent_transe = BertCRFEntityTransE(args, tag_dict).to(device)\n",
    "train_crf(model_crf_ent_transe, epochs = 100, lr = 2e-6)\n",
    "evaluate(model_crf_ent_transe, test_loader, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "822a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/pretrained/bert-base-chinese/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BertCRFEntityTransE model with lr = 3e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231218 23:48:41] [Test] Precision: 0.203280 Recall: 0.248911 F1: 0.223793\n",
      "[20231218 23:48:41] [epoch 1] TLoss: 636.874 VLoss: 208.643 TAcc: 0.487 VAcc: 0.598\n",
      "[20231218 23:50:43] [Test] Precision: 0.321116 Recall: 0.409926 F1: 0.360126\n",
      "[20231218 23:50:43] [epoch 2] TLoss: 145.043 VLoss: 116.003 TAcc: 0.620 VAcc: 0.643\n",
      "[20231218 23:52:42] [Test] Precision: 0.349111 Recall: 0.449896 F1: 0.393147\n",
      "[20231218 23:52:42] [epoch 3] TLoss: 101.405 VLoss: 94.005 TAcc: 0.657 VAcc: 0.656\n",
      "[20231218 23:54:40] [Test] Precision: 0.390754 Recall: 0.509187 F1: 0.442178\n",
      "[20231218 23:54:40] [epoch 4] TLoss: 76.729 VLoss: 76.834 TAcc: 0.682 VAcc: 0.673\n",
      "[20231218 23:56:41] [Test] Precision: 0.401896 Recall: 0.538170 F1: 0.460155\n",
      "[20231218 23:56:41] [epoch 5] TLoss: 60.527 VLoss: 64.413 TAcc: 0.709 VAcc: 0.675\n",
      "[20231218 23:58:46] [Test] Precision: 0.416842 Recall: 0.561659 F1: 0.478535\n",
      "[20231218 23:58:46] [epoch 6] TLoss: 49.061 VLoss: 55.097 TAcc: 0.728 VAcc: 0.682\n",
      "[20231219 00:00:49] [Test] Precision: 0.438500 Recall: 0.607123 F1: 0.509215\n",
      "[20231219 00:00:49] [epoch 7] TLoss: 39.817 VLoss: 48.435 TAcc: 0.749 VAcc: 0.686\n",
      "[20231219 00:02:49] [Test] Precision: 0.469050 Recall: 0.628718 F1: 0.537272\n",
      "[20231219 00:02:49] [epoch 8] TLoss: 32.862 VLoss: 42.900 TAcc: 0.766 VAcc: 0.697\n",
      "[20231219 00:04:53] [Test] Precision: 0.453933 Recall: 0.650502 F1: 0.534724\n",
      "[20231219 00:04:53] [epoch 9] TLoss: 27.589 VLoss: 43.595 TAcc: 0.788 VAcc: 0.682\n",
      "[20231219 00:06:52] [Test] Precision: 0.485456 Recall: 0.692366 F1: 0.570737\n",
      "[20231219 00:06:52] [epoch 10] TLoss: 23.825 VLoss: 39.774 TAcc: 0.804 VAcc: 0.699\n",
      "[20231219 00:08:50] [Test] Precision: 0.497923 Recall: 0.703921 F1: 0.583268\n",
      "[20231219 00:08:50] [epoch 11] TLoss: 20.918 VLoss: 37.565 TAcc: 0.822 VAcc: 0.708\n",
      "[20231219 00:10:50] [Test] Precision: 0.497994 Recall: 0.705247 F1: 0.583771\n",
      "[20231219 00:10:50] [epoch 12] TLoss: 18.819 VLoss: 37.999 TAcc: 0.837 VAcc: 0.705\n",
      "[20231219 00:12:53] [Test] Precision: 0.499316 Recall: 0.691798 F1: 0.580005\n",
      "[20231219 00:12:53] [epoch 13] TLoss: 17.263 VLoss: 37.407 TAcc: 0.848 VAcc: 0.703\n",
      "[20231219 00:14:58] [Test] Precision: 0.504518 Recall: 0.698049 F1: 0.585711\n",
      "[20231219 00:14:58] [epoch 14] TLoss: 15.964 VLoss: 37.740 TAcc: 0.859 VAcc: 0.705\n",
      "[20231219 00:16:59] [Test] Precision: 0.488004 Recall: 0.712824 F1: 0.579369\n",
      "[20231219 00:16:59] [epoch 15] TLoss: 14.942 VLoss: 41.840 TAcc: 0.871 VAcc: 0.691\n",
      "[20231219 00:18:59] [Test] Precision: 0.490459 Recall: 0.706005 F1: 0.578817\n",
      "[20231219 00:18:59] [epoch 16] TLoss: 13.938 VLoss: 41.616 TAcc: 0.877 VAcc: 0.689\n",
      "[20231219 00:18:59] Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to ./results/BertCRFEntityTransE.3e-06.12190018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20231219 00:19:11] [Test] Precision: 0.523144 Recall: 0.628600 F1: 0.571044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5231443440152665, 0.6285996379791626, 0.5710441732091665)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_crf_ent_transe = BertCRFEntityTransE(args, tag_dict).to(device)\n",
    "train_crf(model_crf_ent_transe, epochs = 100, lr = 3e-6)\n",
    "evaluate(model_crf_ent_transe, test_loader, tag_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
